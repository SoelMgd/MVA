{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are BNNs posteriors really like???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST_CSV(Dataset): # Dataset pytorch\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path).values  # Charger le CSV en NumPy\n",
    "        self.labels = self.data[:, 0]  # Première colonne = labels\n",
    "        self.images = self.data[:, 1:].reshape(-1, 1, 28, 28).astype(np.float32)  # Reshape en (N, 1, 28, 28)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def get_fashion_mnist_loaders_from_csv(batch_size=128):\n",
    "    #train-set 60000 images, test-set 10000\n",
    "    train_dataset = FashionMNIST_CSV(\"data/fashion-mnist_train.csv\", transform=lambda x: (x / 255.0))  # Normalisation [0,1]\n",
    "    test_dataset = FashionMNIST_CSV(\"data/fashion-mnist_test.csv\", transform=lambda x: (x / 255.0))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_fashion_mnist_loaders_from_csv(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Neural Network (ResNet quoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_bias=True, activation=nn.ReLU):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=use_bias),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return self.activation(out)\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self, num_classes=10, width=8, activation=nn.ReLU): # width 16 initialement\n",
    "        super(ResNet20, self).__init__()\n",
    "        self.num_blocks = 3  # ResNet-20 has 3 blocks per stage\n",
    "        self.in_channels = width\n",
    "        self.activation = activation()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1, bias=True) #1 canal\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.stage1 = self._make_layer(width, self.num_blocks, stride=1)\n",
    "        self.stage2 = self._make_layer(width * 2, self.num_blocks, stride=2)\n",
    "        self.stage3 = self._make_layer(width * 4, self.num_blocks, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(width * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.activation(self.bn1(self.conv1(x)))\n",
    "        out = self.stage1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_weights(model, new_weights):\n",
    "    \"\"\"Remplace les poids d'un modèle PyTorch par ceux issus de HMC\"\"\"\n",
    "    state_dict = model.state_dict()\n",
    "    param_keys = list(state_dict.keys()) # liste des paramètres du modèle\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for i, key in enumerate(param_keys):\n",
    "        state_dict[key] = new_weights[i]\n",
    "    model.load_state_dict(state_dict) # charge les nouveaux paramètres\n",
    "\n",
    "def model_predictions(model, dataloader):\n",
    "    \"\"\"Fait des prédictions d'un modèle donné sur tout le dataset et stocke les probabilités\"\"\"\n",
    "    \n",
    "    probabilities = []\n",
    "    model.eval() \n",
    "    with torch.no_grad():  # Désactive le calcul des gradients pour la prédiction\n",
    "        for step, (image, label) in enumerate(dataloader):  # itère sur le dataset\n",
    "            image = image.device()  \n",
    "            logits = model(image)\n",
    "            probs = F.softmax(logits, dim=1)  \n",
    "            probabilities.append(probs)\n",
    "    return torch.cat(probabilities, dim=0)  # (dataset_size, num_classes)\n",
    "\n",
    "def BMA_predictions(probabilities):\n",
    "    \"\"\"Fait une prédiction moyenne Bayesian Model Average p(y|x, D) = 1/M * sum_i( p(y|x, wi))\n",
    "    Args:\n",
    "        probabilities: Tensor  (n_models, dataset_size, num_classes)\"\"\"\n",
    "    \n",
    "    n_models = probabilities.size(0)  # Nombre de modèles\n",
    "    # Moyenne des prédictions sur la première dimension (celles des modèles)\n",
    "    average_predictions = probabilities.mean(dim=0)  # (dataset_size, num_classes)\n",
    "    \n",
    "    # Prédiction finale : la classe qui a la probabilité la plus élevée\n",
    "    class_predict = average_predictions.argmax(dim=1)  # (dataset_size)\n",
    "    return class_predict\n",
    "\n",
    "    \n",
    "def calculate_accuracy(predictions, labels):\n",
    "    \"\"\"Calcule l'accuracy en comparant les prédictions aux labels.\"\"\"\n",
    "    correct_predictions = (predictions == labels).sum().item()  # Nombre de prédictions correctes\n",
    "    total_predictions = labels.size(0)  # Nombre total d'exemples\n",
    "    accuracy = correct_predictions / total_predictions  # Accuracy\n",
    "    return accuracy   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor(np.array([[1, 2], [3, 4]]))\n",
    "tensor.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de densité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def posterior_log_density_func(model, data_loader, weight_decay, device):\n",
    "    \"\"\" \n",
    "    Approximation stochastique du posterior: log p(w|D) = log p(D|w) + log p(w) - log p(D)\n",
    "    p(D) est une constante, on ne la calcule pas\n",
    "    \"\"\"\n",
    "    # Échantillonne un mini-batch\n",
    "    data, target = next(iter(data_loader))  \n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Calcule la log-vraisemblance (log p(D | w))\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
    "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
    "\n",
    "    # Calcule le log-prior (log p(w))\n",
    "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay # + constante\n",
    "\n",
    "    # f(w) = log p(D | w) + log p(w)\n",
    "    f_w = log_p_data + log_p_w \n",
    "    return f_w\n",
    "\n",
    "\n",
    "def stochastic_grad_f(model, data_loader, weight_decay, device):\n",
    "    \"\"\"Approximation stochastique du gradient ∇f(w) avec un mini-batch\"\"\"\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Échantillonne un mini-batch\n",
    "    data, target = next(iter(data_loader))  \n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Calcule la log-vraisemblance (log p(D | w))\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(output, target, reduction=\"sum\")  # NLL\n",
    "    log_p_data = -loss  # On prend le négatif car HMC maximise log-likelihood\n",
    "\n",
    "    # Calcule le log-prior (log p(w))\n",
    "    log_p_w = -0.5 * sum(torch.sum(p**2) for p in model.parameters()) * weight_decay\n",
    "\n",
    "    # f(w) = log p(D | w) + log p(w)\n",
    "    f_w = log_p_data + log_p_w  \n",
    "\n",
    "    # Gradient ∇f(w)\n",
    "    (-f_w).backward()  \n",
    "    gradients = [p.grad for p in model.parameters()]\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme MHC minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device):\n",
    "    \"\"\"\n",
    "    Version Stochastic Gradient Leapfrog conforme à l'algorithme du papier.\n",
    "    \"\"\"\n",
    "    for _ in range(n_leapfrog):\n",
    "        # Étape 1 : Mise à jour du momentum à mi-chemin\n",
    "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            noise = torch.normal(mean=0, std=np.sqrt(2 * eta * delta), size=p.shape, device=device)\n",
    "            m[i] = m[i] + (delta / 2) * grad_w[i] + noise  # Ajout du bruit SG-HMC\n",
    "\n",
    "        # Étape 2 : Mise à jour des poids\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                p += delta * m[i]\n",
    "\n",
    "        # Étape 3 : Dernière mise à jour du momentum\n",
    "        grad_w = stochastic_grad_f(model, data_loader, weight_decay, device)\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            m[i] = m[i] + (delta / 2) * grad_w[i]  # Pas de bruit ici, car déjà ajouté avant\n",
    "    \n",
    "    return w, [-mi for mi in m]  # Inversion du momentum pour réversibilité\n",
    "\n",
    "\n",
    "def SG_HMC(trajectory_length, n_burnin, model, data_loader, delta, n_samples, weight_decay, eta, device):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Hamiltonian Monte Carlo (SG-HMC)\n",
    "    \n",
    "    Args:\n",
    "        trajectory_length : Longueur de trajectoire\n",
    "        n_burnin : Nombre d'itérations de burn-in\n",
    "        model : Réseau de neurones (torch.nn.Module)\n",
    "        data_loader : DataLoader pour mini-batchs\n",
    "        delta : Pas d'intégration pour Leapfrog\n",
    "        n_samples : Nombre d'échantillons à générer\n",
    "        weight_decay : Coefficient pour le log-prior gaussien\n",
    "        eta : Coefficient de bruit correctif pour SG-HMC\n",
    "        device : CPU ou GPU\n",
    "    \n",
    "    Returns:\n",
    "        Liste des échantillons de poids\n",
    "    \"\"\"\n",
    "    n_leapfrog = int(trajectory_length / delta)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialisation des poids et des moments\n",
    "    w = [p for p in model.parameters()]\n",
    "    m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
    "    \n",
    "    # Burn-in phase\n",
    "    for _ in range(n_burnin):\n",
    "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
    "        w, m = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
    "    \n",
    "    # Échantillonnage\n",
    "    w_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        m = [torch.normal(mean=torch.zeros_like(p), std=torch.ones_like(p)) for p in model.parameters()]\n",
    "        w_proposed, m_proposed = sg_leapfrog(w, m, delta, n_leapfrog, model, data_loader, weight_decay, eta, device)\n",
    "        \n",
    "        # Metropolis-Hastings correction\n",
    "        log_acceptance_ratio = (\n",
    "        posterior_log_density_func(w_proposed) - posterior_log_density_func(w) \n",
    "        + 0.5 * (torch.norm(m) ** 2 - torch.norm(m_proposed) ** 2)    )\n",
    "\n",
    "        if torch.rand(1) < torch.exp(log_acceptance_ratio):\n",
    "            w = w_proposed\n",
    "        \n",
    "        w_samples.append([p.clone().detach() for p in w])\n",
    "    \n",
    "    return w_samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet20(num_classes=10)  # Création du modèle\n",
    "hmc_weights = [torch.randn_like(p) for p in resnet.parameters()]  # Simulation de poids HMC\n",
    "set_weights(resnet, hmc_weights)  # Injection des nouveaux poids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper-paramètres ###\n",
    "\n",
    "# Paramètres du prior gaussien :\n",
    "prior_variance = 1/5\n",
    "std = np.srqrt(prior_variance)\n",
    "weight_decay = 1/(std**2) # definition\n",
    "\n",
    "# Paramètres HMC :\n",
    "trajectory_length = (np.pi*std)/2 # formule du papier\n",
    "n_burnin = 10 # 50 dans le papier\n",
    "delta = 1e-5 # 1e-5, 5e-5, 1e-4 dans le papier\n",
    "n_samples = 240\n",
    "\n",
    "# Choix du modèle et des fonctions\n",
    "model = ResNet20(num_classes=10)\n",
    "f, grad_f = f, grad_f # fonction de densité et son gradient\n",
    "\n",
    "\n",
    "### HMC et BMA predictions ###\n",
    "\n",
    "# Initialise les poids du modèle suivant le prior\n",
    "w_init = [torch.normal(mean=0, std=std, size=p.shape) for p in model.parameters()] \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
