{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80517dbc",
      "metadata": {
        "id": "80517dbc"
      },
      "source": [
        "# GRPO Training project: teach an LLM to do additions, again"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DAEbh3jBadc7",
      "metadata": {
        "id": "DAEbh3jBadc7"
      },
      "source": [
        "In this notebook, you'll find:\n",
        "* A basic Transformer with basic tokenizer\n",
        "* A basic dataset for additions\n",
        "* A classical pre-trainer, minimizing cross entropy loss\n",
        "* A Vanilla GRPO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MB8lj855LgHl",
      "metadata": {
        "id": "MB8lj855LgHl"
      },
      "source": [
        "You're not supposed to edit the existing code (you can if you want to...).\n",
        "You should implement one (or more) of the following:\n",
        "* GRPO with PPO (the `usual` one)\n",
        "* RLOO\n",
        "* ReMax\n",
        "* DPO\n",
        "* RAFT\n",
        "* your own RLHF method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ae993bb9",
      "metadata": {
        "id": "ae993bb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OzGh9ahKF17h",
      "metadata": {
        "id": "OzGh9ahKF17h"
      },
      "outputs": [],
      "source": [
        "num_digits = 3\n",
        "\n",
        "dataset_size = 64_000\n",
        "train_proportion = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fabd151a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabd151a",
        "outputId": "e7b36965-8f38-4c49-cf03-0109ab723eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c054bed",
      "metadata": {
        "id": "6c054bed"
      },
      "source": [
        "## Step 1: Construct a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "g2QiF-otFur3",
      "metadata": {
        "id": "g2QiF-otFur3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad_token=\"[PAD]\"\n",
        "eos_token=\"[EOS]\"\n",
        "\n",
        "class character_level_tokenizer:\n",
        "    \"\"\"\n",
        "    character-level\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "\n",
        "    def clean(self, text):\n",
        "        \"\"\"\n",
        "        removes all characters not in the vocabulary\n",
        "        \"\"\"\n",
        "        out = re.sub(self.pattern, \"\", text)\n",
        "        return out\n",
        "\n",
        "    def pre_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        character-level\n",
        "        \"\"\"\n",
        "        return [c for c in text]\n",
        "\n",
        "    def encode(self, text):\n",
        "        text_list = self.pre_tokenization(self.clean(text))\n",
        "        return [self.token_to_id[c] for c in text_list]\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        return \"\".join([self.id_to_token[x] for x in token_list])\n",
        "    \n",
        "tokenizer = character_level_tokenizer()\n",
        "ntokens = tokenizer.ntokens\n",
        "ntokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8FXW2K-1Jd-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXW2K-1Jd-P",
        "outputId": "fff6c129-8820-4b3c-8caa-b43710370098"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1, 2, 10, 4, 2, 11], '12+42=')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"12 + 42 =\"\n",
        "inputs = tokenizer.encode(prompt)\n",
        "inputs, tokenizer.decode(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491af297",
      "metadata": {
        "id": "491af297"
      },
      "source": [
        "## Step 2: Create a dataset for arithmetic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "daa90f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa90f31",
        "outputId": "c23f0b74-6948-43a9-9e6f-5846f4b325b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('399+425=', '824')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sample_datapoint(num_digits = 3):\n",
        "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    a_str = \"\".join([str(x) for x in a_list])\n",
        "    b_str = \"\".join([str(x) for x in b_list])\n",
        "    sum_int = a_int + b_int\n",
        "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
        "\n",
        "sample_datapoint(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b6e861d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e861d2",
        "outputId": "7b639165-7809-4f2d-a067-f21547c293b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('487+575=', '1062'),\n",
              " ('276+106=', '382'),\n",
              " ('189+310=', '499'),\n",
              " ('368+547=', '915')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(num_digits))\n",
        "data[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fee85050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee85050",
        "outputId": "69def39c-084c-4074-f296-908925038748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(57600, 6400)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37200598",
      "metadata": {
        "id": "37200598"
      },
      "source": [
        "## Step 3: Construct a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "91674239",
      "metadata": {
        "id": "91674239"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4eb278ab",
      "metadata": {
        "id": "4eb278ab"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp,\n",
        "                                               nhead=nhead,\n",
        "                                               dim_feedforward=nhid,\n",
        "                                               num_encoder_layers=nlayers)\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.decoder = nn.Linear(ninp, ntoken) # projection sur le vocabulaire\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz))) # masque de l'attention\n",
        "\n",
        "    def forward(self, src):\n",
        "        # S = sequence lenght = len(src), B = batch size, E = embedding dimension = ninp, V = vocab size = ntoken\n",
        "        # src: (S, B)\n",
        "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "        self.src_mask = mask # (S, S)\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp) # (S, B, E)\n",
        "        src = self.pos_encoder(src)  # (S, B, E)\n",
        "        output_enc = self.encoder(src, mask=self.src_mask) # (S, B, E) couches MultiHeadSelfAttention() pytorch\n",
        "        output_dec = self.decoder(output_enc) # (S, B, V)\n",
        "        return F.log_softmax(output_dec, dim=-1), output_enc # (S, B, V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1d568cc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d568cc4",
        "outputId": "7677a7ea-d06a-46a8-85f3-44274cc008a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
              "  (input_emb): Embedding(14, 128)\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TransformerModel(ntoken = ntokens,\n",
        "                         ninp = 128,\n",
        "                         nhead = 16,\n",
        "                         nhid = 64,\n",
        "                         nlayers = 8)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a6PmJSo95N4C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6PmJSo95N4C",
        "outputId": "f0177342-43b6-46c6-b079-aab40db326d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 668942\n"
          ]
        }
      ],
      "source": [
        "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35d113",
      "metadata": {
        "id": "2e35d113"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f2f06e0",
      "metadata": {
        "id": "8f2f06e0"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
        "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device) # (S, B*G) G= num generations per prompt\n",
        "    # (prompt_length, batch_size * num_samples)\n",
        "    for _ in range(new_tokens):\n",
        "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
        "        logits = output[-1,:,:] # dernier token (batch_size * num_samples, ntokens)\n",
        "        if mode == \"greedy\":\n",
        "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        else: # mode == \"sampling\"\n",
        "            logits /= temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d76d1b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76d1b19",
        "outputId": "7c084e50-8269-4f24-859a-4f95f8bd143d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2, 10,  3, 11, 13, 13, 13, 13, 13]]),\n",
              " '2+3=[EOS][EOS][EOS][EOS][EOS]')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"2+3=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "output = generate(model, prompt_tensor).view((1,-1))\n",
        "output, tokenizer.decode(output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f61a9bec",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0': 0,\n",
              " '1': 1,\n",
              " '2': 2,\n",
              " '3': 3,\n",
              " '4': 4,\n",
              " '5': 5,\n",
              " '6': 6,\n",
              " '7': 7,\n",
              " '8': 8,\n",
              " '9': 9,\n",
              " '+': 10,\n",
              " '=': 11,\n",
              " '[PAD]': 12,\n",
              " '[EOS]': 13}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.token_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "00954ddc",
      "metadata": {
        "id": "00954ddc"
      },
      "outputs": [],
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list]) # taille commune des sequences\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x) # padding à gauche\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x))) # padding à droite\n",
        "    return out, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2c84beab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c84beab",
        "outputId": "3e53a680-31e1-4cd6-8ce0-d96009def972"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "padded_prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "264f9227",
      "metadata": {
        "id": "264f9227"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, i, batch_size):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "\n",
        "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
        "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
        "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
        "\n",
        "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
        "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
        "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
        "\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X, Y, prompt_length, answers_length, prompts, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "91e281ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e281ad",
        "outputId": "2a97779d-08e9-485a-9588-07e699dc6b77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 16]), torch.Size([5, 16]), 8, 4, '491+750=', '1241')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
        "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113e1fd1",
      "metadata": {
        "id": "113e1fd1"
      },
      "source": [
        "## Step 4: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "KfmcSdPwp3K6",
      "metadata": {
        "id": "KfmcSdPwp3K6"
      },
      "outputs": [],
      "source": [
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1cfcd10a",
      "metadata": {
        "id": "1cfcd10a"
      },
      "outputs": [],
      "source": [
        "def evaluate(batch_size = batch_size):\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size) (P, B)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size) (A, B)\n",
        "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size) (P+A, B)\n",
        "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens (A, B)\n",
        "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=0).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ac335b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac335b05",
        "outputId": "1355d497-45f0-440a-90b8-30a7f2818091"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c54061a",
      "metadata": {
        "id": "4c54061a"
      },
      "source": [
        "## Step 5: Train the model, classical approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b827e567",
      "metadata": {
        "id": "b827e567"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5b140ba3",
      "metadata": {
        "id": "5b140ba3"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 8e-4\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3638a75d",
      "metadata": {
        "id": "3638a75d"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size) (P, B)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size) (A, B)\n",
        "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size) (P+A, B)\n",
        "            model.zero_grad()\n",
        "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens) # (P+A, B, V) probabilité sur le vocabulaire pour chaque token\n",
        "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens) # (A*B, V) prob sur le vocab de la réponse\n",
        "            target_answers = target_answers.view(-1) # (A*B)\n",
        "            loss = F.cross_entropy(output_answers, target_answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4e2a8490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2a8490",
        "outputId": "492562cc-d243-4314-ab56-d17d41c070ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.41 | loss  0.09 | perplexity     1.09\n",
            "|  1200/ 3600 batches | ms/batch  2.40 | loss  0.07 | perplexity     1.07\n",
            "|  1800/ 3600 batches | ms/batch  2.40 | loss  0.07 | perplexity     1.07\n",
            "|  2400/ 3600 batches | ms/batch  2.41 | loss  0.07 | perplexity     1.07\n",
            "|  3000/ 3600 batches | ms/batch  2.41 | loss  0.07 | perplexity     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 154.66s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.45 | loss  0.06 | perplexity     1.07\n",
            "|  1200/ 3600 batches | ms/batch  2.39 | loss  0.06 | perplexity     1.06\n",
            "|  1800/ 3600 batches | ms/batch  2.43 | loss  0.07 | perplexity     1.07\n",
            "|  2400/ 3600 batches | ms/batch  2.41 | loss  0.06 | perplexity     1.06\n",
            "|  3000/ 3600 batches | ms/batch  2.44 | loss  0.05 | perplexity     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 155.70s | test accuracy  0.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.41 | loss  0.04 | perplexity     1.04\n",
            "|  1200/ 3600 batches | ms/batch  2.44 | loss  0.04 | perplexity     1.04\n",
            "|  1800/ 3600 batches | ms/batch  2.41 | loss  0.04 | perplexity     1.04\n",
            "|  2400/ 3600 batches | ms/batch  2.42 | loss  0.04 | perplexity     1.04\n",
            "|  3000/ 3600 batches | ms/batch  2.41 | loss  0.04 | perplexity     1.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 155.26s | test accuracy  0.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.40 | loss  0.03 | perplexity     1.03\n",
            "|  1200/ 3600 batches | ms/batch  2.42 | loss  0.03 | perplexity     1.03\n",
            "|  1800/ 3600 batches | ms/batch 28.39 | loss  0.03 | perplexity     1.04\n",
            "|  2400/ 3600 batches | ms/batch  2.35 | loss  0.03 | perplexity     1.03\n",
            "|  3000/ 3600 batches | ms/batch  2.45 | loss  0.04 | perplexity     1.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 405.03s | test accuracy  0.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.44 | loss  0.03 | perplexity     1.03\n",
            "|  1200/ 3600 batches | ms/batch  2.55 | loss  0.03 | perplexity     1.03\n",
            "|  1800/ 3600 batches | ms/batch  2.46 | loss  0.02 | perplexity     1.02\n",
            "|  2400/ 3600 batches | ms/batch  2.50 | loss  0.03 | perplexity     1.03\n",
            "|  3000/ 3600 batches | ms/batch  2.62 | loss  0.03 | perplexity     1.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 164.27s | test accuracy  0.33\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "56d9d440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9d440",
        "outputId": "b876390d-4396-4a95-e600-e2230317dbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "103+569=673[EOS]\t actual result: 672\n",
            "936+720=171[EOS][EOS]\t actual result: 1656\n",
            "236+737=973[EOS]\t actual result: 973\n",
            "273+702=975[EOS]\t actual result: 975\n",
            "427+818=1235[EOS]\t actual result: 1245\n",
            "712+297=910[EOS][PAD]\t actual result: 1009\n",
            "882+786=1700[EOS]\t actual result: 1668\n",
            "293+935=1221[EOS]\t actual result: 1228\n",
            "085+664=740[EOS]\t actual result: 749\n",
            "280+634=91[EOS][PAD]\t actual result: 914\n",
            "327+455=782[EOS]\t actual result: 782\n",
            "394+426=812[EOS]\t actual result: 820\n",
            "163+969=1132[EOS]\t actual result: 1132\n",
            "226+418=644[EOS]\t actual result: 644\n",
            "501+680=1181[EOS]\t actual result: 1181\n",
            "600+206=801[EOS]\t actual result: 806\n",
            "419+797=1216[EOS]\t actual result: 1216\n",
            "124+965=1099[EOS]\t actual result: 1089\n",
            "227+422=649[EOS]\t actual result: 649\n",
            "302+153=456[EOS]\t actual result: 455\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa4c591",
      "metadata": {
        "id": "cfa4c591"
      },
      "source": [
        "## Step 4 bis: Coding DeepSeek GRPO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff83f72",
      "metadata": {
        "id": "aff83f72"
      },
      "source": [
        "### Custom reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3c548bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c548bf7",
        "outputId": "b5d45345-22eb-445a-b888-6ad22e96d7b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.0)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def accuracy_reward(output, answer):\n",
        "    # 1 si output=answer\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return 1. if output == answer else 0.\n",
        "\n",
        "accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), accuracy_reward(\"123\", \"124\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e1f02762",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f02762",
        "outputId": "7271bdf4-4ecf-44ee-ec86-8c815ffaa9a5",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 0.008064516129032258)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def distance_accuracy_reward(output, answer):\n",
        "    # ecart relatif entre output et answer\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    int_output = int(output)\n",
        "    int_answer = int(answer)\n",
        "    return abs(int_output - int_answer) / max(int_output, int_answer)\n",
        "\n",
        "distance_accuracy_reward(\"123[EOS]\", \"123\"), distance_accuracy_reward(\"123[PAD]\", \"124\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b42a0d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42a0d70",
        "outputId": "a79ca9d6-a79b-4640-c172-4db78e433256"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def digit_accuracy_reward(output, answer):\n",
        "    # ecart relatif entre les digits\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return sum(c1 == c2 for (c1,c2) in zip(output, answer)) / max(len(output), len(answer))\n",
        "\n",
        "digit_accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), digit_accuracy_reward(\"123[EOS]\", \"123\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a41603b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41603b2",
        "outputId": "33dbf431-4177-4ae3-d36c-08ff8a2261fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0, 0.0)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def reward_format(output):\n",
        "    # 1 si format avec EOS\n",
        "    pattern = r\"\\d+\\[EOS\\](\\[PAD\\])*$\"\n",
        "    return 1. if bool(re.match(pattern, output)) else 0.\n",
        "\n",
        "reward_format(\"123[EOS][PAD][PAD]\"), reward_format(\"123[EOS]\"), reward_format(\"123\"),"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4482e411",
      "metadata": {
        "id": "4482e411"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "cf764cb0",
      "metadata": {
        "id": "cf764cb0"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 16 # B\n",
        "learning_rate = 1e-4\n",
        "num_samples = 16 # G\n",
        "temperature = .8\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)\n",
        "\n",
        "reward_fun = digit_accuracy_reward\n",
        "reward_format = reward_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f15cdced",
      "metadata": {
        "id": "f15cdced"
      },
      "outputs": [],
      "source": [
        "def compute_rewards(text_outputs, answers):\n",
        "    # reward pondérée\n",
        "    repeated_answers = [answer for answer in answers for _ in range(num_samples)]\n",
        "    rewards = torch.tensor(\n",
        "        [0.2 * reward_format(output) + 0.8 * reward_fun(output, answer)\n",
        "         for output, answer in zip(text_outputs, repeated_answers)],\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )\n",
        "    return rewards # (B*G)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b5ce8be",
      "metadata": {},
      "source": [
        "### Coding our actual GRPO trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3c40a38a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRPOTrainer:\n",
        "    \"\"\"\n",
        "    Entraîne un modèle en utilisant la méthode GRPO introduite dans l'article : DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
        "    \"\"\"\n",
        "    def __init__(self, model, \n",
        "                 ref_model=None, \n",
        "                 learning_rate=1e-3, \n",
        "                 epochs = 3,\n",
        "                 batch_size = 16,\n",
        "                 num_samples = 5,\n",
        "                 temperature = 0.8,\n",
        "                 num_iterations = 2,\n",
        "                 eps=0.2,\n",
        "                 beta=0.4):\n",
        "        \"\"\" \n",
        "        Initialise les hyper-paramètres d'entraînement.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.ref_model = ref_model\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size # B\n",
        "        self.num_samples = num_samples # G\n",
        "        self.temperature = temperature\n",
        "        self.num_iterations = num_iterations\n",
        "        self.eps = eps # clipping parameter\n",
        "        self.beta = beta# KL parameter\n",
        "\n",
        "    def config(self):\n",
        "        print(\"-\"*89)\n",
        "        print(\"| training hyper-parameters \")\n",
        "        print(\"-\"*89)\n",
        "        print(f\"num_epochs: {self.epochs}, batch_size: {self.batch_size}, batch_per_epochs: {(len(data_train) - 1)//self.batch_size}\")\n",
        "        print(f\"num_samples: {self.num_samples}, num_iterations: {self.num_iterations}\")\n",
        "        print(f\"eps: {self.eps}, beta: {self.beta}\")\n",
        "        print(\"-\"*89)\n",
        "\n",
        "    def _calculate_grpo_advantages(self, rewards):\n",
        "        \"\"\" \n",
        "        Calcule les avantages de chaque réponse générée grâce à leur rewards.\n",
        "        Args:\n",
        "            rewards: rewards pré-calculées(B*G) \n",
        "        Returns: \n",
        "            advantages (B*G)\n",
        "        \"\"\"\n",
        "        mean_rewards = rewards.view(-1, self.num_samples).mean(dim=1) # (B,)\n",
        "        std_rewards = rewards.view(-1, self.num_samples).std(dim=1) # (B,)\n",
        "        mean_rewards = mean_rewards.repeat_interleave(self.num_samples, dim=0) # (B*G,)\n",
        "        std_rewards = std_rewards.repeat_interleave(self.num_samples, dim=0) # (B*G,)\n",
        "        advantages = (rewards - mean_rewards) / (std_rewards + 1e-5) # (B*G,)\n",
        "        return advantages\n",
        "    \n",
        "    def _compute_log_probs(self, model, outputs, prompt_length):\n",
        "        \"\"\" \n",
        "        Calcule les log-probabilités (sur tout le vocabulaire) des tokens des réponses.\n",
        "        Args:\n",
        "            model: modèle à utiliser pour le calcul des probabilités des tokens\n",
        "            outputs: (P+A, B*G) séquence générée\n",
        "            promt_length: int longeur paddée des prompts du batch\n",
        "        \"\"\"\n",
        "        logits, _ = model(outputs) #(P+A, B*G, V)\n",
        "        logits = logits[prompt_length-1:-1, :, :] #(A, B*G, V)\n",
        "\n",
        "        log_probs = F.log_softmax(logits, dim=-1) #(A, B*G, V) convert raw logits into log probabilities along the vocabulary axis\n",
        "        return log_probs #(A, B*G, V)\n",
        "\n",
        "    def _compute_loss_deepseek(self, advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref):\n",
        "        \"\"\" \n",
        "        Calcule la loss GRPO (DeepSeek) d'un batch, en utilisant les politiques du modèle, de model_ref et model_old.\n",
        "        Args:\n",
        "            advantages: (B*G) avantage de chaque séquence générée\n",
        "            responses: (A, B*G) token générés par le modèle old pour chaque séquence générée\n",
        "            answers_length: int longueur des réponses générées (complétée par du padding)\n",
        "            log_probs: (A, B*G, V) log-probabilités des tokens du vocabulaire, pour chaque étape de la génération par le modèle (mis à jour plusieurs fois par batch)\n",
        "            log_probs_old: (A, B*G, V) même chose pour le modèle old (mis à jour une fois par batch)\n",
        "            log_probs_ref: (A, B*G, V) même chose pour le modèle de réference (mis à jour une fois par époque)\n",
        "\n",
        "        \"\"\"\n",
        "        A, B, G = answers_length+1, self.batch_size, self.num_samples # dimensions\n",
        "\n",
        "        responses = responses.unsqueeze(-1) # (A, B*G) -> (A, B*G, 1) pour gathering\n",
        "\n",
        "        # Récupère et reformatte les log-probs des tokens générés par le modèle (A, B*G, V) -> (B, G, A)\n",
        "        selected_log_probs = log_probs.gather(dim=-1, index=responses) # selectionne uniquement les probabilités des tokens générés\n",
        "        selected_log_probs = selected_log_probs.squeeze(-1) # (A, B*G)\n",
        "        selected_log_probs = selected_log_probs.view(A, B, G).permute(1, 2, 0) # (A, B*G) -> (B, G, A)\n",
        "\n",
        "        # Récupère et reformatte les log-probs des tokens générés par le model_old (A, B*G, V) -> (B, G, A)\n",
        "        selected_log_probs_old = log_probs_old.gather(dim=-1, index=responses)\n",
        "        selected_log_probs_old = selected_log_probs_old.squeeze(-1) # (A, B*G)\n",
        "        selected_log_probs_old = selected_log_probs_old.view(A, B, G).permute(1, 2, 0) # (B, G, A) \n",
        "\n",
        "        # Récupère et reformatte les log-probs des tokens générés par le model_ref (A, B*G, V) -> (B, G, A)\n",
        "        selected_log_probs_ref = log_probs_ref.gather(dim=-1, index=responses)\n",
        "        selected_log_probs_ref = selected_log_probs_ref.squeeze(-1) # (A, B*G)\n",
        "        selected_log_probs_ref = selected_log_probs_ref.view(A, B, G).permute(1, 2, 0) # (B, G, A) \n",
        "\n",
        "        # Expension des avantages pour avoir le même format que les politiques\n",
        "        advantages = advantages.view(B, G) # (B*G)  -> (B, G)\n",
        "        advantages = advantages.unsqueeze(-1).expand(-1, -1, A) # (B, G) -> (B, G, A) \n",
        "\n",
        "        # Calcul des ratios de politiques entre model et model_old sur les token générés\n",
        "        ratios_old = torch.exp(selected_log_probs - selected_log_probs_old) # (B, G, A) rapport des probabilités sur les tokens générés entre model et model_old\n",
        "        clipped_ratios = torch.clamp(ratios_old, 1 - self.eps, 1 + self.eps) # (B, G, A) clipped ratio\n",
        "\n",
        "        weighted_advantages_1 = ratios_old * advantages # (B, G, A)\n",
        "        weighted_advantages_2 = clipped_ratios * advantages # (B, G, A)\n",
        "\n",
        "        # Calcule la KL divergence entre la politique du modèle et celle de model_ref\n",
        "        KL = torch.exp(selected_log_probs_ref - selected_log_probs) - (selected_log_probs_ref - selected_log_probs) -1 # (B, G, A) Estimateur différentiable\n",
        "        \n",
        "        loss_tensor = torch.minimum(weighted_advantages_1, weighted_advantages_2) - self.beta*KL # (B, G, A)\n",
        "        \n",
        "        # Masque les probs des tokens à partir de EOS pour ne pas fausser le calcul de la loss\n",
        "        responses = responses.squeeze(-1).view(A, B, G).permute(1, 2, 0) # (A, B*G, 1) -> (B, G, A)\n",
        "        eos_positions = (responses == 13).int().argmax(dim=2) # (B, G) identifie l'index de EOS dans chaque génération\n",
        "        position_indices = torch.arange(A).expand_as(responses) # (B, G, A)        \n",
        "        mask_padding = position_indices < eos_positions.unsqueeze(-1)  # (B, G, A) masque les tokens à partir de EOS\n",
        "\n",
        "        masked_loss = loss_tensor * mask_padding # (B, G, A) \n",
        "        valid_lengths = mask_padding.sum(dim=2) # (B, G) longeur de chaque génération\n",
        "\n",
        "        loss_per_generation = masked_loss.sum(dim=2) / (valid_lengths + 1e-5) # (B, G)\n",
        "        loss_per_group = loss_per_generation.mean(dim=1) # (B)\n",
        "        loss = - loss_per_group.sum() \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _train_step(self, prompts, prompt_length, answers_length, answers):\n",
        "        \"\"\"\n",
        "            Effectue plusieurs pas de gradient sur un même batch.\n",
        "            Args:\n",
        "                prompts: prompts du  batch (B)\n",
        "                prompt_length: int, longueur paddée des prompts du batch P\n",
        "                answer_length: int, longueur paddée des réponses du batch A-1\n",
        "                answers: réponses isssues du dataset\n",
        "            Returns:\n",
        "                loss: loss de la dernière itération\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        # Génère num_samples réponses pour chaque prompt\n",
        "        outputs = generate(self.model,\n",
        "                            prompts,\n",
        "                            new_tokens = answers_length + 1,\n",
        "                            mode = \"sampling\",\n",
        "                            num_samples = self.num_samples,\n",
        "                            temperature = temperature) # (P+A, B*G)\n",
        "        \n",
        "        responses = outputs[prompt_length:, :] # (A, B*G)\n",
        "\n",
        "        text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                        for i in range(outputs.size(1))]\n",
        "\n",
        "        self.model.eval()\n",
        "        self.model_ref.eval()\n",
        "        with torch.inference_mode(): # Calcul de log_probs_old log_probs_ref sans backpropagation\n",
        "            log_probs_old = self._compute_log_probs(self.model, outputs, prompt_length) # (A, B*G, V)\n",
        "            log_probs_ref = self._compute_log_probs(self.model_ref, outputs, prompt_length) # (A, B*G, V)\n",
        "\n",
        "        # Calcule les avantages\n",
        "        rewards = compute_rewards(text_outputs, answers) # (B*G)\n",
        "        advantages = self._calculate_grpo_advantages(rewards) # (B*G)\n",
        "\n",
        "        for i in range(self.num_iterations): \n",
        "\n",
        "            self.optimizer.zero_grad()  \n",
        "\n",
        "            # Calcule de la loss avec les log_probs avec du modèle à jour\n",
        "            log_probs = self._compute_log_probs(self.model, outputs, prompt_length) # (A, B*G, V)\n",
        "            loss = self._compute_loss_deepseek(advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "    \n",
        "    def train(self, data_train):\n",
        "        \"\"\" \n",
        "        Entraîne le modèle sur tout le dataset d'entraînement\n",
        "            Args:\n",
        "                data_train: dataset d'entraînement\n",
        "        \"\"\"\n",
        "\n",
        "        self.config()\n",
        "\n",
        "        best_test_accuracy = None\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # switch eval for train model (enables dropout)\n",
        "        self.model_ref = copy.deepcopy(model)\n",
        "        epochs = self.epochs\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            epoch_start_time = time.time()\n",
        "            start_time = time.time()\n",
        "\n",
        "            total_loss=0\n",
        "            for batch, i in enumerate(range(0, len(data_train) - 1, self.batch_size)):\n",
        "\n",
        "                # get a batch of prompts and answers\n",
        "                prompts, _, prompt_length, answers_length, _, answers = get_batch(\"train\", i, self.batch_size)\n",
        "                prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "\n",
        "                # perform train_step\n",
        "                loss = self._train_step(prompts, prompt_length, answers_length, answers)\n",
        "                total_loss += loss\n",
        "                if batch%10 ==0:    \n",
        "                    print(f\"Batch {batch} loss : {loss}\")\n",
        "            \n",
        "            # Update ref_model\n",
        "            self.model_ref = copy.deepcopy(model)\n",
        "\n",
        "            avg_loss = total_loss / len(data_train)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")            \n",
        "\n",
        "            test_accuracy = evaluate()\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "            print('-' * 89)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffffd489",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| training hyper-parameters \n",
            "-----------------------------------------------------------------------------------------\n",
            "num_epochs: 3, batch_size: 16, batch_per_epochs: 3599\n",
            "num_samples: 10, num_iterations: 2\n",
            "eps: 0.2, beta: 0.4\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "Batch 0 loss : 383.66845703125\n",
            "Batch 10 loss : 4.35777473449707\n",
            "Batch 20 loss : 4.328964710235596\n",
            "Batch 30 loss : 4.015648365020752\n",
            "Batch 40 loss : 0.5023490190505981\n",
            "Batch 50 loss : 6.314966678619385\n",
            "Batch 60 loss : 4.686737537384033\n",
            "Batch 70 loss : 1.6669038534164429\n",
            "Batch 80 loss : 0.15876276791095734\n",
            "Batch 90 loss : 1.3763408660888672\n",
            "Batch 100 loss : 0.09982376545667648\n",
            "Batch 110 loss : 0.05856597423553467\n",
            "Batch 120 loss : 0.11770162731409073\n"
          ]
        }
      ],
      "source": [
        "Trainer =  GRPOTrainer(model, \n",
        "                 ref_model=None, \n",
        "                 learning_rate=1e-3, \n",
        "                 epochs = 3,\n",
        "                 batch_size = 16,\n",
        "                 num_samples = 10,\n",
        "                 temperature = 0.8,\n",
        "                 num_iterations = 2,\n",
        "                 eps=0.2,\n",
        "                 beta=0.4)\n",
        "\n",
        "Trainer.train(data_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aeVn935w5BSp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeVn935w5BSp",
        "outputId": "2014648b-68e6-4af6-ce9c-b28a3d580c7e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m     prompt, answers \u001b[38;5;241m=\u001b[39m data_test[i]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MVA_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
