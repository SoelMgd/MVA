{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Groupe Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to fill in the `GRPOTrainer` class. You have two options (and you can do both):\n",
    "* the \"normal GRPO\" with clipped surrogate objective\n",
    "* or the \"vanilla GRPO\" with original objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"A train takes 3 hours to travel from A to B at an average speed of 60 km/h. How long would the trip take if the train traveled at 80 km/h?\",\n",
    "    \"A snail climbs a 10-meter wall. It climbs 3 meters during the day and slips 2 meters at night. How many days will it take to reach the top?\",\n",
    "    'If a liar says, \"I always lie,\" is he telling the truth?',\n",
    "    'Can we say that \"this sentence is false\"? Explain why.',\n",
    "    \"Paul is twice the age Pierre was when Paul was the age Pierre is today. If Pierre is 20 years old, how old is Paul?\",\n",
    "    \"A father and his son together are 36 years old. The father is exactly three times the son's age. How old is the son?\",\n",
    "    \"All the cats I have met so far were black. Can I conclude that all cats are black? Why?\",\n",
    "    \"If all humans are mortal and Socrates is human, what can we conclude?\",\n",
    "    \"If a shirt costs twice as much as a pair of pants and the pants cost 30€, how much does the shirt cost?\",\n",
    "    'Jean says: \"All my friends are football players.\" Pierre is Jean’s friend. Can we conclude that Pierre is a football player?',\n",
    "    \"You are in a train and must choose between switching the direction of the train to avoid five people tied to one track, but in doing so, you will kill one person on the other track. What do you do and why?\",\n",
    "    \"A doctor has five patients in need of organ transplants, and a perfectly healthy patient comes in for a routine check-up. Should the doctor sacrifice this patient to save the five others?\",\n",
    "    \"What would happen if gravity on Earth were twice as strong?\",\n",
    "    \"If humans could read minds, how would that change society?\"\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "for question in questions:\n",
    "    dataset.append(f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> <\\answer> tags, respectively, i.e., <think> reasoning process here <\\think> <answer> answer here <\\answer>. User: {question}. Assistant:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOConfiguration:\n",
    "    def __init__(self,\n",
    "                 model_name=\"gpt2\",\n",
    "                 learning_rate=1e-5,\n",
    "                 temperature=0.9,\n",
    "                 max_prompt_length=200,\n",
    "                 max_output_length=200,\n",
    "                 device=\"cpu\",\n",
    "                 num_generations=3,\n",
    "                 num_iterations=2,\n",
    "                 beta=0.1,\n",
    "                 epsilon=1e-5,\n",
    "                 reward_func=None,\n",
    "                 print_outputs=False,\n",
    "                 print_advantages=False):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.temperature = temperature\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.device = device\n",
    "        self.num_generations = num_generations # number of generations per prompts\n",
    "        self.num_iterations = num_iterations    # number of iterative optimization steps per prompts\n",
    "        self.beta = beta # KL coefficient\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_func = reward_func # reward function\n",
    "        self.print_outputs = print_outputs\n",
    "        self.print_advantages = print_advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired from HuggingFace\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, config: GRPOConfiguration):\n",
    "        \"\"\"\n",
    "        Initialize a GRPO Trainer\n",
    "        Args:\n",
    "            config: GRPO Configuration\n",
    "        \"\"\"\n",
    "        self.device = config.device\n",
    "        self.model_name = config.model_name\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config.model_name).to(self.device) # model to optimize\n",
    "        self.ref_model = None # reference model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name, padding=True, padding_side=\"left\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  #gpt2 n'a pas de padding token\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate)\n",
    "        self.temperature = config.temperature\n",
    "        self.max_prompt_length = config.max_prompt_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.eps = config.epsilon\n",
    "        self.num_generations = config.num_generations # num of generation per prompts\n",
    "        self.num_iterations = config.num_generations\n",
    "        self.beta = config.beta\n",
    "        self.reward_func = config.reward_func if config.reward_func else self._default_reward_func\n",
    "        self.print_outputs = config.print_outputs\n",
    "        self.print_adantages = config.print_advantages\n",
    "\n",
    "    \n",
    "    def _default_reward_func(self, prompt, outputs, **kwargs):\n",
    "        \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "        \n",
    "        pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "\n",
    "        matches = [re.match(pattern, content) for content in outputs]\n",
    "        return [1.0 if match else 0.0 for match in matches]\n",
    "        \n",
    "\n",
    "    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):\n",
    "        \"\"\"\n",
    "        Get the per-token log propabilities for the outputs.\n",
    "        Args:\n",
    "            model: model to compute per-token log probabilities\n",
    "            input_ids: sequence of tokens #(G, L)\n",
    "            attention_mak: mapping of tokens to keep to compute attention (excluding padding)\n",
    "            logits_to_keep: number of logits to keep to compute the loss\n",
    "        \"\"\"\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        logits = logits[:,:-1,:] # (G, L-1, V) exclude the last logit corresponding to next token prediction\n",
    "\n",
    "        input_ids = input_ids[:, -logits_to_keep:]\n",
    "        logits = logits[:, -logits_to_keep:]\n",
    "        log_probs = logits.log_softmax(-1) # softmax to get probabilities\n",
    "        return torch.gather(log_probs, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) # keeping only input_ids\n",
    "    \n",
    "\n",
    "    def _generate_and_score_outputs(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Generate answers with old_model to the prompt, compute associated reward and advantages and those of ref_model.\n",
    "        Args:\n",
    "            prompt: the prompt\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Tokenization\n",
    "        prompt_inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "\n",
    "        # Setting to prompt lenght\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n",
    "\n",
    "        # Generate answers with old_model\n",
    "        with torch.inference_mode():\n",
    "            prompt_output_ids = self.model.generate(\n",
    "                prompt_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                num_return_sequences=self.num_generations, # number of generations\n",
    "                do_sample=True,\n",
    "                max_length=prompt_ids.size(1) + self.max_output_length,  # (P+O)\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        if self.print_outputs:\n",
    "            for i in range(prompt_output_ids.size(0)):\n",
    "                decoded_sequence = self.tokenizer.convert_ids_to_tokens(prompt_output_ids[i, :].tolist())\n",
    "                print(f\"Réponse générée (génération {i}) de len {len(decoded_sequence)}: {' '.join(decoded_sequence)}\")\n",
    "\n",
    "        # Splitting prompt and outputs tokens\n",
    "        prompt_length = prompt_ids.size(1)\n",
    "        prompt_ids = prompt_output_ids[:, :prompt_length]  # (G, P)\n",
    "        output_ids = prompt_output_ids[:, prompt_length:]  # (G, O)\n",
    "\n",
    "\n",
    "        # Padding with EOS after first EOS in outputs\n",
    "        is_eos = output_ids == self.tokenizer.eos_token_id  # (G, O)\n",
    "        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)  # (G,)\n",
    "        has_eos = is_eos.any(dim=1)  # (G,)\n",
    "        first_eos_idx = is_eos.int().argmax(dim=1)  # Index of first EOS token (G,)\n",
    "        eos_idx[has_eos] = first_eos_idx[has_eos]  # (G,)\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand_as(is_eos)  # (G, O)\n",
    "        output_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()  # (G, O) Mask for padding\n",
    "\n",
    "        # Merging prompt and output  mask\n",
    "        attention_mask = torch.cat([prompt_mask.expand(self.num_generations, -1), output_mask], dim=1)  # (G, P+O)\n",
    "\n",
    "        logits_to_keep = output_ids.size(1) # Logits to keep for loss computation\n",
    "\n",
    "        # Compute log-probabilities for the prompt with model_ref and model_old\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # When using num_iterations == 1, old_per_token_logps == per_token_logps\n",
    "            if self.num_iterations > 1:\n",
    "                old_per_token_logps = self._get_per_token_logps(self.model, prompt_output_ids, attention_mask, logits_to_keep)\n",
    "            else:\n",
    "                old_per_token_logps = None\n",
    "\n",
    "            ref_per_token_logps = self._get_per_token_logps(self.ref_model, prompt_output_ids, attention_mask, logits_to_keep)\n",
    "        \n",
    "        # Decoding outputs\n",
    "        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Compute rewards\n",
    "        output_rewards = self.reward_func(prompt=prompt, outputs=outputs)\n",
    "        rewards = torch.tensor(output_rewards, dtype=torch.float32, device=self.device) #(G,)\n",
    "\n",
    "        # Compute advantages\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4) # (G,)\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"output_ids\": output_ids,\n",
    "            \"output_mask\": output_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps, \n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": advantages,}\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Compute Loss according to GRPO paper, using advantages, per_token probabilities and KL divergence approximator to reference model\n",
    "        Args:\n",
    "            model: enlever\n",
    "            inputs: {\"prompt_ids\", \"prompt_mask\", \"output_ids\", \"output_mask\", \"old_per_token_logps\", \"ref_per_token_logps\", \"advantages\"}\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the per-token log probabilities for the current model\n",
    "        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n",
    "        output_ids, output_mask = inputs[\"output_ids\"], inputs[\"output_mask\"]\n",
    "        input_ids = torch.cat([prompt_ids, output_ids], dim=1)\n",
    "        \n",
    "        prompt_mask = prompt_mask.expand(output_mask.size(0), -1)\n",
    "        attention_mask = torch.cat([prompt_mask, output_mask], dim=1)\n",
    "        logits_to_keep = output_ids.size(1)  # we only need to compute the logits for the completion tokens\n",
    "\n",
    "        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep) # policy of current model\n",
    "\n",
    "        # Compute the KL divergence between the current model and the reference model\n",
    "        if self.beta != 0.0:\n",
    "            ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n",
    "            per_token_kl = (torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1 ) # (G, O) KL differentiable approximator\n",
    "\n",
    "        ### Compute the loss\n",
    "        \n",
    "        advantages = inputs[\"advantages\"]\n",
    "        if self.print_adantages:\n",
    "            print( \"advantages :\", advantages)\n",
    "\n",
    "        # When using num_iterations == 1, old_per_token_logps == per_token_logps\n",
    "        old_per_token_logps = inputs[\"old_per_token_logps\"] if self.num_iterations > 1 else per_token_logps.detach()\n",
    "        policy_ratio = torch.exp(per_token_logps - old_per_token_logps) # (G, O)\n",
    "        policy_ratio_clipped = torch.clamp(policy_ratio, 1 - self.eps, 1 + self.eps) #clipped ratio\n",
    "\n",
    "        per_token_loss1 = policy_ratio * advantages.unsqueeze(1)\n",
    "        per_token_loss2 = policy_ratio_clipped * advantages.unsqueeze(1)\n",
    "        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)\n",
    "\n",
    "        if self.beta != 0.0:\n",
    "            per_token_loss = per_token_loss + self.beta * per_token_kl # adding KL\n",
    "\n",
    "        loss = (per_token_loss * output_mask).sum() / output_mask.sum() # excluding padding in loss computation\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def train(self, num_epochs: int, dataset: list):\n",
    "        \"\"\"\n",
    "        Train the model on the dataset for specific number of epochs.\n",
    "        Args:\n",
    "            num_epochs: number of epochs \n",
    "            dataset: dataset\n",
    "        \"\"\"\n",
    "        self.ref_model = copy.deepcopy(self.model)  # Initialize reference model\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for i, prompt in enumerate(dataset):\n",
    "                print(f\"Training on prompt {i}\")\n",
    "                loss = self.train_step(prompt)\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\") \n",
    "            self.ref_model = copy.deepcopy(self.model)\n",
    "\n",
    "\n",
    "    def train_step(self, prompt: str):\n",
    "        \"\"\"\n",
    "            Do a traing step on a specific prompt with multiple iteration steps.\n",
    "            Args:\n",
    "                prompt: prompt to optimize the model on it\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        # Generate outputs with old_model, compute log-probabilities for old_model and ref_model\n",
    "        inputs = self._generate_and_score_outputs(prompt)\n",
    "\n",
    "        # For self.num_iterations, iterativelt update the model on the same prompt\n",
    "        for i in range(1, self.num_iterations): \n",
    "            \n",
    "            print(f\"GRPO Iteration {i}\")\n",
    "\n",
    "            loss = self.compute_loss(self.model, inputs)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 0\n",
      "Réponse générée (génération 0) de len 203: The Ġsky Ġis Ġa Ġmess .\" Ċ Ċ The Ġman Ġhas Ġmore Ġthan Ġ10 Ġyears Ġin Ġthe Ġnews Ġbusiness Ġand Ġhe Ġhas Ġno Ġdesire Ġto Ġbe Ġseen Ġof Ġany . ĠHe Ġis Ġonly Ġlooking Ġat Ġhis Ġjob , Ġthe Ġvery Ġvery Ġnext Ġday . Ċ Ċ You Ġsee Ġit Ġwhen Ġyou Ġsee Ġit ... Ċ Ċ Ċ \" It 's Ġabout Ġthe Ġway Ġthat Ġit 's Ġdone ...\" Ċ Ċ As Ġyou Ġwatch Ġhim Ġmake Ġone Ġfinal Ġmove , Ġyou Ġrealize Ġthat Ġhe Ġis Ġonly Ġhalf Ġright Ġand Ġthat Ġthat Ġfact Ġis Ġall Ġyou Ġcan Ġsee Ġbut Ġyou Ġcan 't Ġquite Ġsee Ġwhat 's Ġbeing Ġreported . Ċ Ċ The Ġsky Ġis Ġactually Ġa Ġvery Ġbusy Ġsky Ġon Ġyour Ġown Ġand Ġall Ġyou Ġcan Ġsee Ġis Ġthat Ġpart Ġof Ġwhat Ġyou Ġwish Ġyou Ġcould Ġunderstand . Ċ Ċ The Ġworld Ġwe Ġsee Ġis Ġa Ġbig Ġmess Ġin Ġthe Ġworld Ġof Ġnews Ġthat Ġis Ġonly Ġhalf Ġof Ġthe Ġworld Ġyou Ġcould Ġsee Ġat Ġthe Ġmoment . ĠAs Ġa Ġlittle Ġkid Ġyou Ġthink Ġabout Ġit Ġas Ġthey Ġsee . ĠBut Ġyou Ġdon 't Ġsee Ġthem Ġbecause Ġyou Ġonly Ġsaw Ġthem Ġin Ġpictures . Ċ Ċ In Ġthe Ġpicture Ġabove Ġyou Ġcan Ġsee Ġsome Ġof Ġthe\n",
      "Réponse générée (génération 1) de len 203: The Ġsky Ġis Ġfull Ġof Ġgalaxies Ġin Ġwhat Ġwe Ġknow Ġof Ġthe Ġuniverse , Ġand Ġthe ĠMilky ĠWay Ġlooks Ġalmost Ġlike Ġa Ġhuge Ġball , Ġif Ġwe Ġinclude Ġthe ĠMilky ĠWay . ĠScientists Ġare Ġlooking Ġfor Ġa Ġsimilar Ġregion Ġin Ġwhich Ġthe Ġgalaxies Ġwe Ġknow Ġof Ġthe ĠMilky ĠWay Ġare Ġonly Ġabout Ġ2 Ġmillion Ġyears Ġaway . Ċ Ċ And Ġif Ġthe Ġsky Ġis Ġreally Ġa Ġball Ġof Ġgalaxy - sized Ġgalaxies , Ġthey Ġmight Ġbe Ġlooking Ġfor Ġa Ġball Ġof Ġa Ġstar , Ġwhich Ġwould Ġthen Ġbe Ġlooking Ġlike Ġa Ġtiny Ġblue Ġfireball Ġlooking Ġlike Ġa Ġfireball Ġright Ġaround Ġthis Ġpoint . ĠIt Ġappears Ġto Ġbe Ġa Ġlarge Ġstar , Ġwhich Ġalso Ġappears Ġto Ġbe Ġa Ġsmall Ġplanet , Ġin Ġthe Ġsame Ġrange Ġas Ġthe Ġstars . ĠAll Ġthis Ġwhile Ġno Ġevidence Ġof Ġany Ġstar like Ġevent Ġat Ġall , Ġthe Ġsame Ġsky , Ġand Ġall Ġthis Ġwhile Ġno Ġevidence Ġof Ġanything . Ċ Ċ In Ġa Ġway , Ġa Ġlarge Ġplanet Ġto Ġa Ġsmall Ġplanet Ġwill Ġalmost Ġlike Ġa Ġstar Ġin Ġthe Ġsense Ġthat Ġyou Ġactually Ġtake Ġa Ġhuge Ġand Ġtiny Ġfireball , Ġand Ġyou Ġput Ġa Ġtiny Ġfireball Ġin Ġit . ĠThen Ġthe Ġbig Ġplanet , Ġlike Ġa Ġlarge Ġplanet\n",
      "Réponse générée (génération 2) de len 203: The Ġsky Ġis Ġfull Ġof Ġstars , Ġand Ġthey 're Ġlooking Ġfor Ġtheir Ġman . ĠDon 't Ġworry , Ġthey Ġwon 't . Ċ Ċ There 's Ġso Ġmuch Ġstar Ġfood Ġin Ġthe Ġgalaxy , Ġjust Ġso Ġthat ĠI Ġcan Ġhave Ġmore , Ġand Ġit Ġseems . Ċ Ċ And Ġwhat Ġdo Ġyou Ġknow Ġit 's Ġall Ġabout , Ġwith Ġthe Ġlittle Ġthings Ġin Ġthis Ġcity Ġthat Ġare Ġdifferent , Ġthat Ġseems Ġso Ġbeautiful , Ġit 's Ġa Ġgood Ġplace , Ġit 's Ġbeautiful Ġplace , Ġbeautiful . ĠA Ġperfect Ġplace , Ġit 's Ġa Ġperfect Ġplace . Ċ Ċ So Ġhow Ġhave Ġthe Ġmost Ġand Ġthe Ġsmallest Ġpeople , Ġof Ġall Ġthe Ġpeople , Ġcome Ġto Ġsit Ġin Ġthis Ġcity ? ĠIn Ġthis Ġcity , Ġall Ġthe Ġpeople Ġand Ġthe Ġbeautiful Ġones , Ġand Ġthe Ġsmall Ġthings , Ġthis , Ġis Ġa Ġpart Ġof Ġthe Ġpeople . ĠAnd Ġthis Ġcity , Ġthis Ġis Ġa Ġpart Ġof , Ġis Ġnot Ġonly Ġa Ġpart Ġof Ġme Ġand Ġhere , ĠI 've Ġgot Ġa Ġpurpose , Ġthis Ġis Ġa Ġpurpose , Ġthis Ġis Ġsomething ĠI Ġhave Ġa Ġthing , Ġthis , Ġthis , Ġand Ġthat 's Ġa Ġdream , Ġa Ġlittle Ġidea Ġhere Ġthat Ġthis\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-0.4971,  1.1511, -0.6541])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-0.4971,  1.1511, -0.6541])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 1\n",
      "Réponse générée (génération 0) de len 205: Yesterday , ĠI Ġwent Ġto Ġthe Ġgym Ġwith Ġsome Ġfriends Ġthat Ġday , Ġit Ġwas Ġgoing , Ġ' I 'm Ġgoing Ġto Ġgo Ġto Ġher '. ĠTo Ġknow Ġher Ġso Ġfar , Ġshe 's Ġbeen Ġvery Ġstrong , Ġshe 's Ġnot Ġa Ġreal Ġhot Ġchick , Ġwho ĠI Ġgot Ġto Ġhave Ġit Ġwith , Ġeven Ġthough ĠI Ġdon 't Ġget Ġto Ġhave Ġmany Ġfriends , Ġshe Ġwas Ġa Ġmuch Ġbetter Ġperson Ġwith Ġme Ġthat Ġnight , Ġso Ġthat 's Ġa Ġgood Ġthing , Ġthat 's Ġwhat ĠI Ġexpected . ĊĊ Ċ But Ġyeah , ĠI Ġgot Ġlucky Ġon Ġthe Ġlast Ġday , ĠI Ġget Ġlucky Ġon Ġthe Ġlast Ġday , ĠI Ġwent Ġto Ġthe Ġgym Ġwith Ġa Ġfriend , ĠI Ġthink Ġit Ġwas Ġprobably Ġthe Ġsame Ġgirl Ġyou Ġgot Ġto Ġhave Ġa Ġlittle Ġbit Ġof Ġexcitement Ġfor Ġme Ġthat Ġthe Ġday Ġthat ĠI Ġcame Ġfor Ġme ? ĊĊ Ċ D erek ĊĊ Ċ [ 1 - 04 - 15 ] ĠI Ġfeel Ġbad Ġfor Ġmyself Ġbecause ĠI Ġdon 't Ġknow , ĠI Ġjust Ġcouldn 't . ĠI Ġthink ĠI Ġmade Ġfun Ġof Ġthe Ġworld . ĠIt Ġreally , Ġit 's Ġall Ġthat . ĠAnd , ĠI Ġthink Ġmaybe Ġthat 's Ġa Ġlittle Ġbit Ġof ,\n",
      "Réponse générée (génération 1) de len 205: Yesterday , ĠI Ġwent Ġto Ġyour Ġwebsite Ġto Ġmake Ġa Ġstatement , Ġand ĠI 'm Ġusing Ġa Ġpretty Ġbig Ġdeal Ġof Ġthat Ġto Ġsay , Ġmy Ġstatement Ġthat , Ġmy Ġlast Ġstatement , ĠI Ġwill Ġbe Ġan Ġin Ġmy Ċ Ċ ' S ixty - fourth . ĠAnd ĠI Ġwill Ġbe Ġthe Ġfirst , Ġif Ġyou Ġwish Ġme Ġto , Ġto Ġbe Ġan Ġin Ġyour Ġstatement Ġof Ġsome Ġimportance , Ġand Ġthe Ġfact Ġthe Ġfact Ġin Ġsome Ġof Ġmy Ġbeing Ġa Ġstatement Ġwhich Ġthe Ġand Ġa . Ġis Ġthat . ĠThis Ġmeans Ġsome Ġof Ġthat . Ċ Ċ I Ġam Ġbeing Ġsaid Ġto Ġbe Ġsaying Ġin Ġthe Ġlast Ġand Ġto Ġso Ġmuch , Ġas Ġto Ġsay , Ġ' a Ġcertain . ĠIf Ġyou Ġare , Ġyou Ġmust Ġbe Ġable , Ġto Ġbe Ġable Ġto Ġbe Ċ Ċ ' Ġable Ġto Ġbe Ġable Ġto Ġbe Ġable , Ġor Ġto Ġwant Ġto Ġbe Ġable Ġto Ġwant Ġto . Ġor Ġto Ġbe Ġable . Ġ( The Ġpeople Ġare Ġmaking Ġthe Ċ Ċ of Ġthe Ġfirst Ġstatement Ġand ĠI Ġwill Ġsay Ġthat Ġthe Ġpeople Ġare Ġgoing Ġto Ġhave Ġto Ġbe Ġable , Ġand ĠI Ġthink ĠI Ġcan Ġtell . Ċ Ċ ' And Ġthere Ġis Ġa Ġand Ġa Ġright\n",
      "Réponse générée (génération 2) de len 205: Yesterday , ĠI Ġwent Ġto Ġget Ġthis : Ċ Ċ What Ġhas Ġthe Ġfuck Ġin Ġfront Ġof Ġyou ? Ċ Ċ I Ġdid Ġthe Ġbest Ġjob Ġever . ĠYou Ġknow Ġhow ĠI 'm Ġgoing Ġto Ġget Ġpissed . ĠAfter Ġthe Ġ\" R \" Ġis Ġover Ġa Ġlittle Ġbit , ĠI 'll Ġjust Ġget Ġpissed . ĠI 'm Ġsure ĠI Ġwill Ġcome Ġoff Ġthe Ġshow . ĠBut Ġthis Ġis Ġwhat Ġwe Ġare Ġat Ġthe Ġshow Ġis Ġabout : ĠI 'm Ġsorry Ġand ĠI Ġcan 't Ġlet Ġyou Ġdo Ġthis . ĠI Ġknow Ġthere Ġwill Ġbe Ġconsequences Ġfor Ġyou Ġif Ġyou Ġstay . ĠBut Ġyou Ġstill Ġhave Ġa Ġreason Ġto Ġgo Ġto Ġthe Ġshow Ġand Ġjust Ġleave Ġus Ġfor Ġthe Ġrest Ġof Ġthe Ġworld . Ċ Ċ I Ġhave Ġdone Ġthis Ġbefore Ġand Ġit Ġis Ġonly Ġmy Ġdream Ġto Ġgive Ġthis Ġup Ġbut Ġit 's Ġall Ġbeen Ġfucked . Ċ Ċ I Ġdid Ġmy Ġbest Ġas Ġa Ġcomedian Ġand Ġif Ġto Ġsome Ġof Ġyou ĠI Ġdidn 't Ġthen ĠI Ġcould 've Ġfucked Ġyou Ġall Ġon Ġmy Ġway . Ċ Ċ Thank Ġyou Ġfor Ġtaking Ġthe Ġnext Ġstep . ĠI 'm Ġnot Ġgoing Ġdo Ġa Ġshow Ġnow . ĠI 've Ġgotta Ġhave Ġfun . Ċ But Ġnow ĠI\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 1.1545, -0.5772, -0.5772])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 1.1545, -0.5772, -0.5772])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 2\n",
      "Réponse générée (génération 0) de len 204: I Ġhope ĠI Ġwill Ġget Ġa Ġlot Ġof Ġwork Ġfrom Ġyou . Ċ Ċ I Ġknow ĠI Ġwill Ġdo Ġa Ġlot Ġof Ġwork Ġfrom Ġyou , Ġbut ĠI 'm Ġnot Ġsure Ġof Ġthat . Ċ Ċ I Ġdon 't Ġsee Ġanything Ġin Ġthis Ġsituation . ĠIt Ġis Ġso Ġfar Ġremoved Ġfrom Ġyou . Ċ Ċ There Ġare Ġso Ġmany Ġthings Ġwe 're Ġtrying Ġto Ġdo Ġin Ġthis Ġoffice . Ċ Ċ My Ġjob Ġis Ġa Ġvery Ġhard Ġman . Ġ. Ġ. Ċ Ċ I Ġwork Ġso Ġhard Ġfor Ġthe Ġlittle Ġman . ĠDo Ġyou Ġexpect Ġnothing Ġat Ġall . Ċ Ċ I 'm Ġjust Ġa Ġman , Ġbut Ġthat 's Ġnot Ġall ĠI Ġknow Ġon Ġthe Ġother Ġside Ġof Ġthings . Ċ Ċ We 're Ġa Ġtight Ġjob . ĠSome Ġthings Ġyou Ġmight Ġnot Ġthink Ġis Ġall . Ċ Ċ What Ġabout Ġmen Ġwho ĠI Ġcan Ġsee ? Ċ Ċ Ċ I Ġgot Ġsome Ġwork Ġdone . ĠI Ġdon 't . Ċ Ċ Do Ġyou Ġtake Ġan Ġoath , Ġand Ġdo Ġyou Ġdo Ġthat Ġwith Ġa Ġlittle Ġbit Ġof Ġa Ġrisk , Ġand Ġa Ġlittle Ġbit Ġof Ġwork ? Ċ Ċ I Ġcan Ġgive Ġyou Ġa Ġbit Ġof Ġa Ġhint . Ċ Ċ And\n",
      "Réponse générée (génération 1) de len 204: I Ġhope ĠI Ġwill Ġfind Ġout Ġfor Ġawhile Ġwho Ġthe Ġreal Ġperson Ġfor Ġthis Ġhas Ġbeen . ĠWhen Ġshe Ġdidn 't , Ġand Ġwhen Ġshe Ġdidn 't Ġget Ġhome Ġbecause ĠI Ġwas Ġsick Ġ( of Ġbeing Ġsick ) Ġshe Ġstarted Ġcrying Ġout Ġof Ġfear Ġthat Ġif Ġshe Ġdidn 't Ġhear Ġit Ġor Ġhear Ġit Ġagain , ĠI Ġwas Ġgoing Ġto Ġdie . ĠAnd Ġthen , Ġbecause Ġshe Ġwas Ġdoing Ġwell , Ġthe Ġdoctor Ġsent Ġme Ġhome Ġand Ġfound Ġout Ġshe Ġdidn 't Ġhear Ġit Ġ( I Ġtold ĠHer b Ġabout Ġit Ġbeing Ġthe Ġdoctor Ġand Ġshe Ġwas Ġscared Ġshe Ġcouldn 't Ġhear Ġit Ġbecause Ġshe Ġwas Ġstill Ġon Ġthe Ġground Ġshe Ġcouldn 't Ġsee Ġher Ġhead . ĠHer b ). ĠSo , Ġhow Ġdo Ġwe Ġknow Ġthat Ġthe Ġperson Ġdied , Ġsince Ġit Ġwas Ġnot Ġthe Ġdoctor Ġbut Ġmy Ġfamily , Ġwho Ġwas Ġthere Ġin Ġthat Ġsituation , Ġto Ġwhat Ġyou Ġthink , Ġyou Ġthink , ĠI 'll Ġjust Ġthink Ġthat Ġif Ġthere 's Ġone , Ġtwo , Ġthree , Ġfour , Ġfive , Ġsix Ġor Ġseven , Ġit Ġnever Ġhappened Ġagain . Ċ Ċ So , Ġthis Ġis Ġtrue . ĠThat 's Ġtrue . ĠAnd Ġit 's Ġnot Ġtrue . ĠMy Ġdad Ġwas\n",
      "Réponse générée (génération 2) de len 204: I Ġhope ĠI Ġwill Ġhave Ġgotten Ġit .\" Ċ Ċ He Ġhad Ġnever Ġseen Ġany Ġother Ġman Ġmake Ġsuch Ġa Ġclaim . ĠHe Ġhad Ġno Ġdoubt Ġhad Ġalways Ġbelieved Ġin Ġit , Ġwhen Ġhe Ġshould Ġhave Ġsaid Ġthat . ĠAnd Ġhe Ġseemed Ġto Ġsay , Ġ\" I Ġlike Ġthis Ġnew Ġguy .\" Ċ One Ġof Ġthe Ġreasons Ġhis Ġname Ġwas Ġgiven Ġwas Ġthat , Ġjust Ġfor Ġme Ġat Ġleast , Ġhe Ġwas Ġgoing Ġto Ġsee Ġhow ĠI Ġdid Ġon Ġmy Ġown . ĠThe Ġonly Ġway ĠI Ġwould Ġhave Ġlooked Ġup Ġto Ġhim Ġon Ġmy Ġown Ġis Ġif Ġmy Ġhead Ġcarried Ġme Ġinto Ġthe Ġnight : Ġthen Ġno Ġone Ġcould Ġhave Ġseen Ġhis Ġface , Ġthe Ġway Ġthat Ġit Ġseemed Ġto Ġme Ġon Ġmy Ġown . ĠThat Ġis Ġbecause Ġthe Ġother Ġperson Ġwas Ġtoo Ġstrong , Ġthat ĠI Ġcould Ġnot Ġhave Ġseen Ġhim Ġon Ġhis Ġown , Ġhe Ġseemed Ġto Ġbe Ġa Ġman , Ġin Ġspite Ġof Ġhis Ġbeauty , Ġand Ġhe Ġwore Ġa Ġsuit Ġand Ġtie Ġas Ġhis Ġsuit . ĠThat 's Ġone Ġof Ġthe Ġreasons Ġthat Ġhe Ġwas Ġsuch Ġa Ġman Ġhe Ġwas Ġhis Ġsuit . Ċ Ċ When Ġthe Ġword Ġhe Ġgot Ġhis Ġhead Ġup Ġhigh Ġhis Ġshoulders , Ġhe Ġsaid ,\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-1.1520,  0.6440,  0.5080])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-1.1520,  0.6440,  0.5080])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 3\n",
      "Réponse générée (génération 0) de len 203: My Ġcar Ġis Ġcurrently Ġrunning Ġon Ġthe Ġlast Ġday ĠI Ġbought Ġthis Ġcar , Ġbut Ġto Ġbe Ġgenerous , Ġit Ġwill Ġrun Ġfor Ġabout Ġtwo Ġdays Ġwithout Ġstopping . ĊĊ Ċ Ċ My Ġcurrent Ġcar Ġis Ġthe ĠB - 11 J Ġand Ġit Ġis Ġreally Ġthat Ġgood . ĠI Ġfeel Ġit Ġis Ġmy Ġprimary Ġcar Ġand ĠI Ġwould Ġlove Ġto Ġchange Ġthat Ġthis Ġwas Ġmy Ġonly Ġ\" car .\" ĠIt Ġwould Ġbe Ġgreat Ġif Ġall Ġowners Ġwould Ġhave Ġtaken Ġthe Ġ\" tr unk \" Ġin Ġtheir Ġcars . ĠBut ĠI Ġwill Ġnever Ġbuy Ġit . ĠThe Ġ\" tr unk \" Ġwould Ġmake Ġit Ġso Ġmuch Ġnicer Ġfor Ġme Ġand Ġto Ġbe Ġhonest , ĠI Ġknow Ġmy Ġdad Ġdoesn 't Ġcare . ĠI Ġdon 't Ġwish Ġthat Ġhe Ġcould , Ġin Ġthe Ġleast Ġconsider Ġthe Ġ\" tr unk \" Ġthe Ġnew Ġone Ġif ĠI Ġwere Ġone Ġof Ġthe Ġones Ġwho Ġdid Ġit Ġanyway . ĊĊ Ċ I Ġhave Ġpurchased Ġall Ġthree Ġcars , Ġbut Ġhave Ġno Ġplans Ġto Ġincrease Ġany Ġof Ġthe Ġcars ĠI Ġhave Ġin Ġit . ĠI Ġhave Ġalso Ġno Ġplans Ġto Ġbuy Ġone Ġof Ġthe Ġcars Ġor Ġthe Ġcar - I 'm Ġtalking Ġmore Ġthan Ġone Ġhere , Ġso Ġwhen Ġit\n",
      "Réponse générée (génération 1) de len 203: My Ġcar Ġis Ġa Ġgood Ġbet Ċ Ċ That ĠI Ġmight Ġhave Ġone Ġof Ġthese Ġnew Ġ' n ' Ġlike , Ġmore Ġoften Ġthan Ġnot Ġa Ġcar Ġthat 's Ġgoing Ġto Ġbe Ġin Ġthe Ġright Ġplace Ġat Ġthe Ġright Ġtime . ĠThese Ġthings Ġwill Ġbe Ġso Ġmuch Ġbetter Ġto Ġhave Ġa Ġnew Ġcar Ġthat Ġthey 'd Ġnever Ġbe Ġin Ġthe Ġsame Ġcar , Ġprobably Ġa Ġcar Ġthat Ġthey 'll Ġnever Ġhave , Ġas Ġwe Ġall Ġare , Ġas ĠI Ġput Ġit , Ġ' now '. Ċ Ċ With Ġyou Ġand Ġme Ġnow , Ġyou 're Ġright , Ġyou 're Ġright , Ġwe 're Ġgoing Ġto Ġspend Ġour ĠSunday Ġmorning Ġtrying Ġto Ġfind Ġthe Ġright Ġplace Ġfor Ġyou Ġand Ġme Ġto Ġbe Ġin , Ġwhich ĠI 'm Ġgoing Ġto Ġbe Ġfor Ġmy Ġkids , Ġwhich ĠI 'm Ġactually Ġgoing Ġto Ġbe Ġhaving Ġtwo Ġkids Ġto Ġthe Ġnext Ġday Ġin Ġcollege , Ġwhich Ġare Ġnot Ġgoing Ġto Ġbe Ġtwo Ġkids Ġor Ġa Ġlittle Ġkid , Ġwhere Ġwe Ġhave Ġto Ġfind Ġour Ġbest Ġseat . ĠThe Ġbest Ġseat , Ġis , Ġyou Ġgo Ġto Ġwork , Ġand Ġthat , Ġand Ġthen Ġyou Ġgo Ġfor Ġdinner . Ċ Ċ Then , Ġthe Ġnext Ġday , Ġand Ġone\n",
      "Réponse générée (génération 2) de len 203: My Ġcar Ġis Ġrunning Ġto Ġa Ġnew Ġtrack , Ġlower Ġramp , Ġas Ġmy Ġcar Ġis Ġto Ġa Ġtrack . ĠI Ġhave Ġone , Ġtoo Ġlate , Ġtoo Ġlate , Ġtoo Ġa Ġmistake . ĠThey Ġcome Ġin Ġand Ġstart Ġtelling Ġme Ġhow Ġto Ġdo Ġit , Ġthat 's Ġthe Ġfirst Ġtime Ġno Ġone 's Ġever Ġdid Ġit . ĠI 've Ġgot Ġto Ġmy Ġcar Ġrunning Ġto Ġa Ġlower Ġroad , Ġthen ĠI Ġhave Ġit Ġrunning Ġto Ġa Ġlower Ġstreet . Ċ Ċ They Ġtell Ġme Ġand ĠI Ġgo Ġit Ġwithout Ġa Ġsecond , Ġit Ġmakes Ġme Ġgo Ġto Ġa Ġsmaller Ġtrack . ĠAnd Ġfor Ġthe Ġfirst Ġtime ĠI 've Ġever Ġdone Ġit , Ġno Ġone 's Ġdone Ġit . ĠThe Ġthought Ġof Ġthat ĠI Ġshould Ġbe Ġable Ġto Ġdo Ġit Ġwith Ġmy Ġmind , Ġno Ġone 's Ġbeen Ġable Ġto Ġdo Ġit Ġand Ġthey 'd Ġnever Ġlet Ġthat Ġgo . ĠI 'm Ġready Ġfor Ġthe Ġfirst Ġtime Ġor ĠI 'm Ġdoing Ġit . Ċ Ċ J OE : ĠOne Ġof Ġthe Ġthings Ġpeople Ġsay Ġabout Ġthe Ġbeginning Ġof Ġour Ġseries Ġis Ġthe Ġfeeling Ġof Ġurgency . Ċ Ċ Ċ K AT : ĠI Ġfeel Ġlike Ġa Ġrush Ġof Ġadrenaline Ġto Ġkeep Ġpushing Ġme\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 1.0744, -0.1709, -0.9035])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 1.0744, -0.1709, -0.9035])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 4\n",
      "Réponse générée (génération 0) de len 205: I Ġhave Ġto Ġgo Ġto Ġwork Ġfor Ġschool , Ġdon 't ĠI Ġjust Ġhave Ġto Ġwork Ġhard , Ġit Ġgets Ġbetter Ġand Ġbetter Ġbecause Ġwe Ġhave Ġto Ġdo Ġreally Ġwell Ġwith Ġboth .\" Ċ Ċ After Ġhe Ġhas Ġhad Ġhis Ġfirst Ġtwo Ġyears , Ġ\" I 'm Ġa Ġlittle Ġjealous Ġof Ġthem Ġgoing Ġout Ġhere Ġwith Ġme Ġafter Ġwork , Ġyou Ġknow , Ġyou Ġgo Ġtake Ġcare Ġof Ġmy Ġchildren , Ġyou Ġknow , Ġthat 's Ġwhat ĠI Ġdid ,\" Ġbut , Ġ\" I Ġdon 't Ġdo Ġanything Ġgood . ĠIt Ġgets Ġbetter Ġand Ġbetter , Ġthen ĠI 'm Ġa Ġgood Ġstudent , Ġyou Ġknow Ġthere 's Ġlots Ġof Ġthings Ġthat ĠI Ġcan Ġdo Ġbetter Ġwith , Ġbut Ġafter , ĠI Ġjust Ġneed Ġtime Ġto Ġfigure Ġ[ a Ġbetter ] Ġin Ġschool .\" Ċ Ċ T . J .: Ġ\" If Ġyou Ġthink Ġabout Ġit , Ġyou Ġknow Ġto Ġsay , ĠI 'm Ġnot Ġa Ġgood Ġstudent , Ġbut ĠI Ġjust Ġkind Ġof Ġwork Ġhard , Ġso ĠI 'll Ġmake Ġthis , Ġhe Ġdoesn 't Ġcare . ĠI 'm Ġa Ġgood Ġstudent , Ġso Ġit 's Ġnot Ġabout Ġschool , Ġyou Ġknow , ĠI 'm Ġnot Ġa Ġgood Ġstudent . ĠI 'd Ġlike Ġto Ġbe\n",
      "Réponse générée (génération 1) de len 205: I Ġhave Ġto Ġgo Ġto Ġprison , Ġand Ġwe Ġwill Ġnever Ġbe Ġlike Ġthat !\" ĠNo . ĠYou Ġhave Ġto Ġgo Ġand Ġget Ġaway . ĠIt Ġwill Ġbe Ġa Ġhorrible Ġdeath Ġsentence ! Ċ Ċ After Ġthe Ġmeeting Ġwith Ġthe ĠAmerican Ġdoctors , Ġand Ġafter Ġthey Ġreceived Ġthe Ġletters Ġon Ġthe Ġletter head Ġof Ġtheir Ġorders Ġand Ġfor Ġeverything , Ġthey Ġgot Ġa Ġnew Ġpresident , Ġwith Ġa Ġpretty Ġlame Ġpresident Ġwith Ġthe Ġproblem Ġof Ġthe Ġ\" health \" Ġis Ġthe Ġfact Ġthat Ġthe Ġway Ġof Ġbeing Ġas ĠAmerica Ġas ĠI Ġknow Ġshe Ġsaid Ġthat Ġwould Ġbe Ġto Ġleave Ġand Ġthey Ġwould Ġnot Ġleave . ĠHe Ġgave Ġus Ġto Ġcome , Ġto Ġstay . Ċ Ċ This Ġone Ġtime , Ġthey Ġcame Ġand Ġsaid Ġthat Ġhe Ġcouldn 't Ġbe Ġthere , Ġthey Ġtried Ġto Ġget Ġhim Ġto Ġcome Ġto Ġthe ĠSenate , Ġthey Ġsent Ġthree Ġletters Ġbut , Ġthe Ġtwo Ġletters , Ġthey Ġsent . Ċ Ċ And , Ġall Ġthis Ġtime Ġthey Ġcame Ġand Ġthey Ġsaw Ġthey Ġcalled Ġon Ġthe Ġhead Ġof Ġhis Ġstaff , Ġand Ġthey Ġgot Ġit Ġfor Ġthe ĠCongress Ġand Ġsaid , Ġyou Ġknow , Ġthey Ġgot , Ġthey Ġput Ġa Ġhead Ġon Ġour Ġhead , Ġand Ġeverybody , Ġthey Ġkept\n",
      "Réponse générée (génération 2) de len 205: I Ġhave Ġto Ġgo Ġto Ġthe Ġmeeting Ġyour Ġfirst Ġwife Ġbut ĠI Ġcan 't . ĠShe 's Ġnot Ġa Ġprincess , Ġand Ġyou 're Ġthe Ġone ĠI Ġwant Ġand Ġyou 'll Ġpay Ġfor Ġit Ġat Ġmy Ġhouse . Ċ Ċ \" I Ġlove Ġyou Ġtoo , Ġand ĠI 'm Ġmarried Ġto ĠPrincess Ġof ĠWales . ĠShe 's Ġmy Ġprincess , Ġand ĠI Ġgot Ġit Ġmyself .\" ĊĊ Ċ - ĊĊ Ċ W illy 's Ġfather Ġwas Ġa Ġbig Ġpart Ġof Ġthe Ġdrama , Ġparticularly Ġwhen Ġthe Ġfirst Ġdaughter Ġcalled Ġout Ġfor Ġcalling Ġher Ġ\" T oots \". Ċ Ċ Ċ But Ġeven Ġafter Ġthe Ġfather Ġreturned Ġto Ġhome , Ġthe Ġgirl Ġtold Ġhim Ġto Ġget Ġthe Ġmoney . Ċ Ċ \" If Ġyou Ġcome Ġback , Ġplease Ġdon 't Ġcall Ġme Ġthat Ġway .\" Ċ Ċ C ous ins Ġthen Ġwent Ġback Ġto Ġhis Ġfather 's Ġhouse Ġand Ġtried Ġto Ġget Ġin Ġtouch Ġwith Ġhis Ġsister Ġand Ġmother Ġin Ġthe Ġcountryside , Ġand Ġeventually Ġgot Ġhis Ġpermission Ġto Ġset Ġup Ġshop Ġon Ġthe Ġestate Ġfor Ġsome Ġtime . Ċ Ċ W illy Ġand Ġhis Ġstep mom , Ġwho Ġwas Ġon Ġher Ġway Ġback Ġout Ġto Ġthe Ġcountryside Ġfor Ġher Ġday 's Ġrun , Ġcalled Ġhim\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-0.9197,  1.0645, -0.1449])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-0.9197,  1.0645, -0.1449])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 5\n",
      "Réponse générée (génération 0) de len 203: My Ġfriend Ġwant Ġto Ġbe Ġhere Ġbecause Ġif ĠI 'm Ġhonest , Ġshe Ġcan 't Ġhelp Ġme !\" Ċ Ċ You Ġcan Ġtell Ġfrom Ġthese Ġquotes Ġthat ĠI Ġactually Ġdid Ġcare Ġabout Ġthem . ĠI Ġjust Ġwish Ġthey Ġdidn 't Ġbe Ġwritten Ġin . ĠThis Ġwas Ġanother Ġexample Ġof Ġhow Ġpeople Ġcan Ġlie Ġabout Ġothers Ġwhen Ġthey Ġshould Ġcare Ġabout Ġthem . ĠThat 's Ġwhat Ġthis Ġentire Ġconversation Ġshould Ġhave Ġbeen Ġabout , Ġtoo , Ġbut Ġit Ġwasn 't Ġabout Ġme , Ġand ĠI Ġdidn 't Ġcare Ġif Ġthings Ġlike Ġwhat Ġhappened Ġin Ġthe Ġpast Ġmade Ġme Ġlook Ġlike Ġhe pt . Ċ Ċ And ĠI Ġwas Ġglad Ġto Ġfind Ġout Ġexactly Ġwhat Ġthey Ġdid Ġwrong . Ċ Ċ I Ġdon 't Ġknow Ġhow Ġlong Ġthat Ġwould Ġhave Ġbeen Ġin Ġthe Ġworld âĢĶ maybe Ġone Ġday ĠI Ġwill Ġhave Ġto Ġgive Ġyou Ġmy Ġheart Ġback . Ċ Ċ So Ġwhat Ġto Ġdo ? ĠI Ġfeel Ġsorry Ġfor Ġit , Ġas Ġthat Ġis Ġwhat ĠI Ġam Ġabout Ġto Ġgo Ġthrough . ĠI Ġhope ĠI Ġdon 't Ġever Ġsuffer Ġanother Ġdisaster Ġlike Ġthis Ġone . Ċ Ċ It 's Ġnot Ġlike Ġhow Ġmany Ġtimes ĠI Ġhave Ġheard Ġof Ġa Ġgirl Ġask Ġher Ġfriends Ġto Ġtell\n",
      "Réponse générée (génération 1) de len 203: My Ġfriend Ġwant Ġto Ġsee Ġyou Ġas Ġmuch Ġmore Ġthan Ġthat ,\" Ġshe Ġcried Ġout , Ġas Ġif Ġto Ġmake Ġher Ġface Ġhurt . Ġ\" I Ġfeel Ġlike Ġa Ġchild Ġand ĠI Ġwant Ġto Ġgo Ġto Ġschool Ġfor Ġyou . ĠPlease , ĠI Ġcan 't , ĠI Ġcan 't , ĠI Ġcan 't .\" Ġ\" I Ġdon 't Ġhave Ġany Ġmoney \" Ġshe Ġexclaimed , Ġas Ġher Ġmind Ġdid Ġnot Ġallow Ġher Ġto Ġget Ġto . Ġ\" Now Ġyou Ġcan Ġgo Ġto Ġclass , Ġand Ġthen Ġto Ġbed Ġtonight \" Ġshe Ġwent Ġfor Ġa Ġlong Ġtime . ĠThen Ġwhen Ġshe Ġsaw Ġthe Ġschool Ġbed . ĠShe Ġsat Ġdown Ġupon Ġit . ĠHer Ġeyes Ġfixed . ĠAnd Ġthe Ġone Ġhand Ġher Ġfingers . ĠShe , Ġshe Ġwanted Ġto Ġhave Ġthat Ġfor Ġher Ġchildren . ĠAnd Ġshe Ġfelt Ġthat Ġeven Ġif Ġshe Ġwent Ġfor Ġschool , Ġit Ġwould Ġnot Ġbe Ġthe Ġsame Ġas Ġthat Ġwhich Ġshe Ġwanted Ġfor Ġher Ġchildren . ĠA Ġwoman , Ġher Ġtwo Ġhands Ġin Ġher Ġchest , Ġgot Ġto Ġher Ġhead Ġwhen Ġshe Ġsaw Ġthe Ġschool . ĠHer Ġface Ġhad Ġchanged . ĠShe Ġcouldn 't Ġgo Ġto Ġschool Ġthis Ġmorning . ĠThat Ġgirl Ġis Ġtoo . ĠHer Ġfour Ġfore ums Ġand\n",
      "Réponse générée (génération 2) de len 203: My Ġfriend Ġwant Ġa Ġgirl Ġwho Ġis Ġthe Ġreason Ġwhy Ġi Ġsaid Ġ\" Yes , ĠI Ġstill Ġsee Ġher \" Ċ Ċ The Ġreason Ġthat Ġgirl Ġis Ġthe Ġreason Ġwhy Ġshe Ġis Ġthe Ġreason Ġwhy Ġshe Ġis Ġthe Ġreason Ġwhy Ġshe Ġis Ġwhy Ġshe Ġis Ġthe Ġreason Ġwhy Ġshe Ġis Ġwhy Ġi 'm Ġthe Ġreason . ĠThat Ġgirl Ġwould Ġif Ġshe Ġwanted Ġa Ġgirl Ġand Ġher Ġbody Ġbecause Ġshe Ġhas Ġa Ġgirl Ġor Ġlike , Ġwhat Ġdo Ġgirls Ġdo Ġis Ġsay Ġthe Ġgirl Ġis Ġthe Ġgirl Ġbecause Ġshe Ġhas Ġa Ġgirl Ġor Ġjust Ġyou Ġhave Ġsaid Ġthat Ġand Ġno Ġproblem , Ġlet 's Ġnot Ġfucking Ġlaugh Ġat Ġour Ġfucking Ġparents , Ġyou Ġjust Ġgot Ġthat Ġshit Ġbut Ġwhen Ġour Ġfucking Ġparents Ġdon 't Ġfucking Ġunderstand Ġtheir Ġfucking Ġparents Ġyou Ġjust Ġgot Ġthat Ġshit , Ġyour Ġfucking Ġfucking Ġcunt , Ġfuck Ġit 's Ġwhy Ġu Ġwant Ġto Ġhave Ġa Ġfucking Ġbody Ġu Ġwant Ġto Ġhave Ġa Ġfucking Ġbody . Ċ Ċ Now , Ġwhen Ġwe Ġlook Ġat Ġa Ġgirl 's Ġbody , Ġshe Ġdoesn 't Ġlook Ġexactly Ġsimilar Ġto Ġthe Ġperson Ġand Ġdon 't Ġlook Ġexactly Ġlike Ġher Ġor Ġjust Ġlike Ġsomeone , Ġthe Ġfuck Ġif Ġwe Ġlook Ġat Ġher Ġor Ġher Ġlike Ġshe\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-0.0175, -0.9911,  1.0087])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-0.0175, -0.9911,  1.0087])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 0.0012\n",
      "Training on prompt 0\n",
      "Réponse générée (génération 0) de len 203: The Ġsky Ġis Ġa Ġvast Ġsky . ĠA Ġgreat Ġmany Ġpeople Ġhave Ġbeen Ġthere Ġby Ġday , Ġso Ġthat Ġthey Ġcame Ġwhen Ġall Ġthat Ġearth Ġcan Ġdo , Ġand Ġso Ġthat Ġyou Ġhave Ġto Ġsay , Ġthe Ġsun Ġhas Ġbeen Ġthere Ġbefore , Ġand Ġis Ġalways Ġthere . ĠAnd Ġso Ġit Ġlooks , Ġand Ġso Ġdid Ġit Ġdoes Ġat Ġthe Ġday Ġwhen Ġthe Ġsun Ġcame , Ġthe Ġday , Ġthe Ġsun Ġcame . ĠAnd Ġso Ġdid Ġit Ġand Ġso Ġdid Ġit Ġat Ġthe Ġtime Ġthat ĠI Ġbegan Ġbefore Ġit , Ġbefore Ġthe Ġtime Ġthat Ġall Ġthe Ġearth , Ġthe Ġearth Ġand Ġso Ġdid Ġit , Ġand Ġso Ġdid Ġit , Ġas Ġthe Ġangels Ġof Ġlight Ġsaid , Ġsaid , Ġso Ġdid Ġit . ĠI Ġknow Ġthat Ġthe Ġday Ġhas Ġcome , Ġbecause Ġthen Ġwas Ġthe Ġday Ġbefore ĠI Ġbegan . ĠBut Ġwe Ġdo Ġnot Ġknow Ġwhat Ġwill Ġhappen Ġwhen Ġthe Ġsun Ġcame . ĠFor Ġthere Ġare Ġalso Ġother Ġthings Ġto Ġhappen Ġwhen Ġit Ġcomes , Ġand Ġas ĠI Ġsay Ġis Ġthat Ġthere Ġare Ġother Ġthings Ġto Ġhappen Ġwhen Ġhe Ġcomes , Ġbut Ġbecause Ġyou Ġneed Ġto Ġknow Ġwhen Ġit Ġcomes Ġand Ġso Ġdo Ġnot Ġneed Ġto Ġask Ġme . ĠAnd Ġhe Ġhas Ġthere\n",
      "Réponse générée (génération 1) de len 203: The Ġsky Ġis Ġthe Ġmoon Ġand Ġhe Ġdoes Ġnot Ġthink Ġof Ġthat Ġthe Ġsky Ġis Ġin Ġthe Ġmoon Ġ... Ċ Ċ 3 - Ċ Ċ \" The Ġmoon Ġis Ġthe Ġearth Ġand Ġshe Ġdoes Ġnot Ġthink Ġof Ġthat Ġthe Ġsky Ġis Ġnot Ġin Ġthe Ġmoon ,\" Ċ Ċ 1 - Ġ3 - 3 - 2 - 3 - 6 \" He Ġsays Ġno Ġto Ġno , Ċ Ċ Ċ 3 - ĠHe Ġsays Ġno Ġyou Ġonly Ċ Ċ Ċ \" You Ġdo Ġnot Ġmake Ġthis Ġlight Ġbe Ġuseless Ġfor Ġyou Ġwill Ġhave Ġnothing Ġto Ġdo Ġwith Ġa Ġfire ,\" Ċ Ċ 4 - 5 - 6 - 7 Ċ Ċ \" And Ġhe Ġis Ġthe Ġmoon Ġalso Ġthe Ġsky \" Ċ Ċ Ċ 5 - 6 - 7 Ċ Ċ 10 - 10 Ċ 10 - 10 - 10 - 10 - 10 - 10 - 5 - 8 Ċ Ċ 10 - 10 - 10 - 10 - 10 - 10 - 10 - 10 \" Ċ Ċ 10 - 10 - 10 The Ġmoon Ġis Ġalways Ġof Ġthe Ġmoon Ġhe Ġdoes Ġnot Ġthink Ġof \" Ċ Ċ 1 - 3 - 5 Ċ Ċ 3 - 3 - 3 - 6\n",
      "Réponse générée (génération 2) de len 203: The Ġsky Ġis Ġfilled Ġwith Ġstars Ġ- Ġor Ġshadows . ĠSome Ġlight Ġwould Ġjust Ġgoes Ġby . ĠIt 's Ġwhat Ġmany Ġwomen Ġand Ġmen Ġwho Ġdon 't Ġknow Ġhow Ġto Ġuse Ġtheir Ġbody . ĠPeople Ġtalk Ġabout Ġthe Ġpower Ġof Ġthe Ġsun Ġto Ġhelp Ġthem Ġlook Ġat Ġtheir Ġhouse . ĠWomen Ġtalk Ġabout Ġhow Ġgreat Ġthings Ġcould Ġbe Ġwhen Ġthe Ġsun Ġshines Ġon Ġtheir Ġface . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 1.0652, -0.1465, -0.9187])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 1.0652, -0.1465, -0.9187])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 1\n",
      "Réponse générée (génération 0) de len 205: Yesterday , ĠI Ġwent Ġto Ġwork Ġthis Ġafternoon . Ċ Ċ \" Oh , Ġyou Ġdon 't Ġhave Ġany Ġsleep ?\" Ċ Ċ \" My Ġhands .\" Ċ Ċ On Ġthe Ġother Ġhand Ġthe Ġsame Ġthing Ġon Ġthis Ġoccasion . ĠIt Ġappears Ġas Ġif Ġthere Ġwould Ġbe Ġa Ġsudden Ġrush Ġof Ġsleep Ġwhen Ġhe Ġfinished Ġhis Ġfirst Ġbook - reading Ġand Ġthis Ġis Ġa Ġdefinite Ġsign Ġthat Ġsleep Ġis Ġa Ġmatter Ġof Ġlife Ġand Ġindeed . Ċ Ċ \" Do Ġyou Ġnot Ġsee Ġso ?\" Ċ Ċ \" Then Ġgo Ġfind Ġthe Ġdoctor . ĠThe Ġother Ġday Ġhe Ġtold Ġus Ġthat Ġthere Ġwas Ġan Ġanomaly .\" Ċ Ċ \" Your Ġsecond Ġbook Ġsays Ġthe Ġsame Ġthing ? ĠThat Ġwas Ġthe Ġsame Ġthing . ĠWas Ġthat Ġbecause Ġthe Ġfirst Ġbook Ġwas Ġread , Ġand Ġthe Ġthird Ġand Ġfourth Ġeditions Ġwere Ġalso Ġread ?\" Ċ Ċ \" Yes .\" Ċ Ċ \" So Ġthat Ġthird Ġedition Ġwas Ġbought Ġat Ġbook store Ġon Ġthe Ġday Ġbefore Ġthe Ġsecond Ġpart . ĠDid Ġthat Ġdate Ġdiffer Ġfrom Ġlast Ġtime ?\" Ċ Ċ \" You Ġsee . Ċ Ċ \" Is Ġit ?\" Ċ Ċ \" Yes .\" Ċ Ċ \" This Ġis Ġthe Ġsame Ġthing .\" Ċ Ċ \" That\n",
      "Réponse générée (génération 1) de len 205: Yesterday , ĠI Ġwent Ġto Ġthe Ġdoctor Ġof Ġthe Ġhospital Ġto Ġcheck Ġhis Ġhealth Ġand Ġthe Ġconditions Ġmy Ġfather Ġhas Ġwas . ĠIt 's Ġso Ġunbelievable Ġand Ġhe Ġmade Ġsuch Ġa Ġbig Ġmistake . ĠI Ġhad Ġno Ġidea Ġthat Ġhe Ġdied Ġof Ġa Ġheart Ġattack Ġif Ġmy Ġbrother - in - law Ġhas Ġdied . ĊĊ Ċ The Ġdoctor Ġwas Ġthe Ġone Ġwho Ġhad Ġto Ġtake Ġcare Ġfor Ġmy Ġcondition . ĠI Ġalso Ġdid Ġnot Ġknow Ġthat Ġmy Ġbrother - s - he Ġwas Ġan Ġadult Ġbecause Ġhe Ġhas Ġbeen Ġan Ġadult Ġfor Ġnearly Ġthree Ġmonths . ĠThey Ġtold Ġme Ġthat Ġhe Ġwas Ġa Ġdoctor Ġbecause Ġwe Ġwent Ġto Ġthe Ġhospital Ġbecause Ġhe Ġhad Ġa Ġheart Ġattack . ĠIt Ġwas Ġnot Ġabout Ġa Ġquestion Ġof Ġwhether Ġa Ġheart Ġattack Ġwas Ġpossible , Ġit Ġis Ġa Ġquestion Ġof Ġwhether Ġyour Ġhusband Ġstill Ġloves Ġyou . ĠIt Ġwas Ġa Ġquestion Ġabout Ġwhether Ġhe Ġcould Ġlive Ġwith Ġhis Ġlife . ĠAnd Ġit Ġwas Ġall Ġmy Ġlife Ġand ĠI Ġstill Ġcry Ġfor Ġmy Ġsister Ġso ĠI Ġwant Ġmy Ġsister Ġto Ġstay Ġwith Ġhim , Ġmy Ġdaughter Ġbecause ĠI Ġwant Ġall Ġthe Ġsame Ġthings Ġfor Ġmy Ġhusband Ġand Ġthat 's Ġwhy ĠI Ġgave Ġmy Ġsister Ġthe Ġdiagnosis Ġfor Ġthe Ġheart\n",
      "Réponse générée (génération 2) de len 205: Yesterday , ĠI Ġwent Ġto Ġthe Ġoffice Ġand Ġsaid ĠI Ġwas Ġsick . Ċ Ċ I Ġwent Ġto Ġthe Ġdoctor . ĠI Ġknew Ġsomething Ġwas Ġwrong . ĠI 'd Ġtalked Ġto Ġa Ġnurse , Ġshe Ġsaid . ĠShe Ġwas Ġvery Ġconcerned . ĠShe Ġsaid , Ġ\" A hem , ĠI Ġdon 't Ġknow Ġthe Ġanswer . ĠIt 's Ġnot Ġvery Ġimportant , Ġit 's Ġonly Ġan Ġanswer Ġto Ġthe Ġmost Ġserious Ġof Ġquestions : Ċ Ċ \" Ċ Ċ \" Your Ġmother Ġis Ġa Ġgood Ġperson ? ĠCan Ġshe Ġget Ġher Ġmedication Ġbecause Ġof Ġbipolar ? ĠDon 't Ġworry Ġabout Ġit .\" ĠShe Ġsaid Ġshe Ġhad Ġno Ġchoice Ġand Ġneeded Ġto Ġgo Ġwith Ġthe Ġmedication Ġand Ġthe Ġdoctor Ġcalled Ġthe Ġdoctor Ġand Ġsaid ĠI Ġneeded Ġto Ġgo Ġand Ġyou 're Ġgoing Ġto Ġgo Ġto Ġuse Ġan Ġantidepressant Ġmedication . Ċ Ċ \" I Ġknow Ġthat Ġthe Ġmedications Ġhave Ġbeen Ġon Ġthe Ġside Ġor Ġthey 've Ġbeen Ġon Ġthe Ġside . ĠAll Ġof Ġthem Ġare Ġwrong . Ġ\" Ċ Ċ And Ġshe Ġasked Ġme , Ġ\" So , Ġwhere Ġare Ġthey ?\" Ġthe Ġdoctor Ġhad Ġto Ġknow Ġwhat Ġhad Ġto Ġhappen . ĠIt Ġhurt . ĠI Ġsaid Ġshe Ġgot Ġa Ġgreat Ġdepression . ĠAnd Ġmy Ġwife\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-0.9141,  1.0680, -0.1539])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-0.9141,  1.0680, -0.1539])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 2\n",
      "Réponse générée (génération 0) de len 204: I Ġhope ĠI Ġwill Ġnot Ġhave Ġto Ġhave Ġto Ġdie Ġmore Ġbecause Ġthey Ġwill Ġstop Ġme Ġin Ġthe Ġvery Ġlast Ġdays , ĠI Ġam Ġsure Ġyou Ġwill Ġtry Ġto Ġbe Ġa Ġpretty Ġgirl Ġwith Ġyou . ĠBut ĠI Ġunderstand Ġthat Ġis Ġvery Ġmuch Ġthe Ġcase .\" Ċ Ċ He Ġtook Ġout Ġhis Ġpen Ġand Ġbegan Ġwriting Ġhis Ġscript . ĠAs Ġsoon Ġas Ġhe Ġwas Ġready Ġfor Ġit , Ġhe Ġrealized Ġthat Ġhe Ġhad Ġlost Ġall Ġmemory Ġof Ġit Ġand Ġbegan Ġwriting Ġfrom Ġhis Ġback Ġof Ġthe Ġnotebook Ġand Ġtaking Ġoff Ġto Ġhis Ġdesk . ĠShe Ġwalked Ġup Ġbehind Ġhim Ġwith Ġa Ġhappy Ġexpression Ġon Ġher Ġface Ġand Ġsaid Ġ\" It 's Ġabout Ġtime Ġto Ġend Ġthe Ġstory . ĠIt Ġis Ġabout Ġtime Ġto Ġexit Ġmy Ġfamily . ĠNow Ġthat Ġis Ġall Ġyou Ġneed Ġis Ġto Ġkeep Ġlooking . ĠAll Ġthe Ġother Ġstuff Ġis Ġgone . ĠOh , Ġsorry Ġto Ġgo Ġback Ġin Ġthe Ġsame Ġplace . ĠI Ġknow ĠI Ġwill Ġprobably Ġleave Ġthis Ġplace Ġin Ġthe Ġvery Ġfuture , Ġbut Ġof Ġcourse Ġonly Ġif ĠI Ġcan .\" Ċ Ċ She Ġsaid Ġ\" Okay . ĠI Ġlike Ġit .\" ĠHe Ġsaid Ġ\" The Ġfirst Ġthing Ġyou Ġneed Ġto Ġdo Ġis Ġto Ġkeep Ġlooking . ĠIt\n",
      "Réponse générée (génération 1) de len 204: I Ġhope ĠI Ġwill Ġbe Ġable Ġto Ġbe Ġin Ġa Ġposition Ġwhere ĠI Ġam Ġgiven Ġthe Ġopportunity Ġto Ġget Ġpaid Ġin Ġthe Ġnext Ġtwo Ġyears Ġor Ġtwo Ġyears Ġor Ġsomething Ġlike Ġthat . ĠWhat Ġkind Ġof Ġwork Ġwill ĠI Ġbe Ġputting Ġinto Ġthere Ġas Ġa Ġkid Ġbecause Ġof Ġthe Ġwork ĠI Ġhave Ġput Ġinto Ġmyself . ĠIt Ġwill Ġbe Ġa Ġlot Ġof Ġthe Ġwork ĠI Ġhas Ġput Ġinto Ġmyself Ġin Ġmy Ġhead Ġas Ġwell ĠI Ġguess Ġbecause Ġmy Ġgrades ĠI 've Ġbeen Ġperforming Ġand Ġmy Ġplay - pen alty Ġnumbers ĠI 've Ġseen . Ċ Ċ D U EL : ĠRight . ĠIt 's Ġhard Ġat Ġthis Ġpoint Ġconsidering Ġall Ġof Ġus Ġthat Ġhave Ġbeen Ġhere Ġand Ġall Ġthat Ġhave Ġbeen Ġhere . ĠIf ĠI Ġremember Ġwe Ġremember Ġwhat Ġwe 've Ġdone Ġis Ġwe Ġstarted Ġplaying Ġin Ġthe Ġleague Ġfrom Ġthe Ġstart Ġa Ġwhole Ġbunch Ġof Ġthat Ġtime Ġin ĠNew ĠYork Ġin Ġa Ġwhole Ġbunch Ġof Ġcases Ġand Ġthat Ġwas Ġa Ġlot Ġof Ġthat Ġtime Ġin ĠNew ĠYork , Ġwas Ġwas Ġthe Ġbig Ġtime . Ċ Ċ N ICK : ĠThat 's Ġa Ġreally Ġsmall Ġsample Ġsize . ĠI Ġthink ĠI 'll Ġquote Ġa Ġlot . Ċ Ċ D EL BER T : ĠRight\n",
      "Réponse générée (génération 2) de len 204: I Ġhope ĠI Ġwill Ġbe Ġwith Ġhim ,\" Ġshe Ġsaid Ġon ĠWednesday . Ġ\" The Ġfirst Ġthing ĠI Ġdid Ġwas Ġto Ġget Ġthe Ġmoney .\" Ċ Ċ The Ġsecond Ġthing Ġshe Ġdid Ġwas Ġto Ġgo Ġout Ġand Ġget Ġthe Ġmoney , Ġand Ġto Ġbe Ġwith Ġhim . Ċ Ċ She Ġwas Ġgone Ġagain Ġlast Ġnight . Ċ Ċ The Ġfirst Ġthing Ġthe Ġwoman Ġsaid Ġwas , Ġwas , Ġ' Well , Ġthis Ġlady , ĠI Ġwill Ġgive Ġyou Ġto Ġhave Ġa Ġlittle Ġlove Ġyou , Ġdon 't Ġyou ?' Ċ Ċ And Ġthat Ġhe Ġdid , Ġdid , Ġhe Ġdid Ġget Ġher . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 0.6014,  0.5530, -1.1544])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 0.6014,  0.5530, -1.1544])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 3\n",
      "Réponse générée (génération 0) de len 203: My Ġcar Ġis Ġalso Ġa Ġgood Ġtruck , Ġespecially Ġsince Ġthe Ġtruck Ġand ĠI 've Ġowned Ġall Ġthree Ġof Ġmy Ġfour Ġin Ġthe Ġyears Ġmy Ġcar Ġhas Ġbeen Ġowned Ġbefore ; Ġno Ġissues . ĠThey Ġhave Ġbeen Ġall Ġaround Ġthe Ġhighway Ġsince ĠI Ġchanged Ġto Ġone Ġof Ġmy Ġfavorite Ġtrucks ; Ġthey Ġare Ġbetter Ġbecause ĠI Ġonly Ġhave Ġmy Ġown Ġcar Ġthat ĠI Ġown . ĠI Ġhave Ġmy Ġoriginal Ġcar Ġbut Ġwhen Ġit Ġgot Ġused Ġbut ĠI Ġstill Ġowned . ĠMy Ġcar Ġis Ġgreat ; Ġevery Ġone Ġof Ġit 's Ġamazing . Ċ Ċ But , ĠI Ġdo Ġhave Ġone Ġissue Ġwith Ġthe Ġtruck . ĠIt 's Ġtoo Ġmuch Ġfor Ġmy Ġcar , Ġit Ġworks , Ġbut Ġthere Ġis Ġa Ġcertain Ġcar Ġon Ġevery Ġ10 Ġminutes Ġit 's Ġa Ġbad Ġtruck Ġon Ġevery Ġminute . ĠI Ġmean , Ġit 's Ġthe Ġbest Ġtruck ĠI Ġhave Ġwith Ġso Ġwhat Ġif Ġit Ġare Ġmy Ġoriginal Ġtruck ? ĠIf ĠI Ġget Ġin Ġand ĠI 'm Ġlike Ġ' who a , ĠI Ġgot Ġthat Ġand Ġwhere 's ĠI Ġgot . ĠI Ġlike , Ġthe Ġtwo , ĠI Ġmean , Ġthis Ġplace Ġwas Ġnever Ġso Ġbad Ġfor Ġme Ġin Ġmy Ġcar Ġbecause Ġthen Ġit Ġwas Ġa Ġtwo Ġminute\n",
      "Réponse générée (génération 1) de len 203: My Ġcar Ġis Ġin Ġa Ġmajor Ġjam ,\" Ġhe Ġsays Ġof Ġhis Ġexperience . Ċ Ċ B ach Ġsaid Ġhe Ġbelieves Ġhis Ġcar Ġhas Ġa Ġunique Ġcondition Ġthat Ġcauses Ġcrashes . Ċ Ċ \" I 've Ġalways Ġbelieved Ġthat Ġit 's Ġa Ġlittle Ġbit Ġtoo Ġfast Ġto Ġdrive , Ġand Ġit 's Ġprobably Ġnot Ġthe Ġbest Ġthat Ġwe 've Ġfound Ġto Ġbe Ġhonest Ġabout . ĠA Ġsafety Ġproblem Ġwe Ġhave Ġto Ġsolve Ġin Ġthe Ġnext Ġtwo Ġyears , Ġand Ġhopefully Ġwe 're Ġdoing Ġthat ,\" Ġsaid Ġthe Ġformer Ġracer . Ċ Ċ This Ġyear , Ġhe Ġplans Ġto Ġtest Ġin Ġthe Ġsame Ġcar . Ċ Ċ Ċ \" This Ġis Ġthe Ġfirst Ġtime ĠI 've Ġhad Ġthe Ġcar Ġthis Ġcrash Ġfree Ġand Ġits Ġbeen Ġmy Ġfirst Ġyear Ġof Ġtesting Ġa Ġnormal Ġrace Ġcar ,\" Ġsays Ġthe Ġformer Ġracer . Ġ\" It Ġmeans ĠI 'm Ġreally Ġglad ĠI Ġwon .\" Ċ Ċ G ross ly Ġsurprised , ĠI Ġdidn 't Ġbelieve Ġthat . ĠThis Ġis Ġan Ġinteresting Ġcar . ĠI Ġhaven 't Ġenjoyed Ġthe Ġday . ĠI 'm Ġa Ġlittle Ġdisappointed Ġas ĠI Ġdrive Ġit , Ġbut ĠI Ġdidn 't Ġreally Ġfind Ġthat Ġodd . ĠI Ġmean , Ġa Ġnormal Ġrace Ġcar Ġlike Ġthe\n",
      "Réponse générée (génération 2) de len 203: My Ġcar Ġis Ġin Ġthe Ġcar Ċ Ċ The Ġcar Ġwe Ġneed Ġto Ġget Ġat Ġthis Ġpoint Ċ Ċ We 've Ġbeen Ġin Ġa Ġcar . ĠWe Ġall Ġhave Ġcars ! ĠWe Ġall Ġhave Ġcars . ĠWe Ġall Ġhave Ġcars . ĠAnd Ġwe Ġall Ġare Ġgoing Ġabout Ġthat Ċ Ċ \" Ah . ĠAll Ġthis Ġcar Ġyou Ġneed Ġto Ġbe Ġon Ġyour Ġway Ġhere ?\" ĠThe Ġdriver Ġwas Ġasking Ġthe Ġpassenger . Ċ Ċ Ċ The Ġdriver Ġknew Ġwhat Ġsomeone Ġwas Ġgoing Ġto Ġsay . ĠWhat Ġthat Ġsomeone Ġwas Ġgoing Ġto Ġsay . ĠAh , Ġit Ġwas Ġa Ġquestion er . ĠDid Ġsomeone Ġknow Ġwhat Ġthey Ġwere Ġgoing Ġto Ġbe Ġtalking Ġabout Ġthis Ġtime Ġwhen Ġthe Ġcar Ċ The Ġdriver Ġknew Ġwhat Ġhe Ġwas Ġgoing Ġto Ġsay . ĠHow Ġabout Ġwe Ġtell Ġyou Ġwhat Ġyou Ġwould Ġto Ġsay . Ċ Ċ You Ġwant Ġto Ġtake Ġa Ġbreak Ġhere . Ċ Ċ The Ġdriver Ġknows Ġand Ġknows Ġthat Ġwhat Ġhe Ġknows Ġin Ġthe Ġcar Ġis Ġwhat Ġhe Ġknows Ġin Ġair Ġis Ġthat Ġthat Ġcar Ġwas Ġsaid , Ġand Ġdid . ĠAnd Ġthis Ġcar , Ġthat Ġit Ġwas Ġsaid . ĠHow Ġabout Ġhow Ġabout Ġthe Ġcar Ġsays Ġis Ġsaying Ġit 's Ġnot Ġsaying Ġthat Ġcar\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 0.1734,  0.9019, -1.0754])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 0.1734,  0.9019, -1.0754])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 4\n",
      "Réponse générée (génération 0) de len 205: I Ġhave Ġto Ġgo Ġto Ġmy Ġhome !\" Ġ\" W ake Ġup !\" ĠAnd Ġso ĠI Ġdid âĢ¦. ĊĊ Ċ How Ġdid ĠI Ġsay Ġmy Ġname ? ĠI Ġgot Ġinto Ġher Ġand Ġthought : ĠOh Ġmy ĠGod . Ġ\" What Ġa Ġnice Ġname âĢ¦\" Ġ\" Who Ġare Ġyour Ġfavorite Ġfriends ? ĠWhat Ġwas Ġshe ?\". ĠI Ġsaid Ġit Ġwas Ġa Ġlittle Ġboring , Ġbut Ġno Ġone Ġever Ġdid Ġit Ġlike Ġher . ĠShe Ġjust Ġthought Ġit Ġwas Ġso Ġfunny ĠI Ġdidn 't Ġwant Ġto Ġtell Ġher Ġbut Ġher Ġmother Ġsaid Ġher Ġmother Ġwas Ġso Ġfunny Ġor Ġsomething ? ĠI Ġhave Ġto Ġgo Ġto Ġmy Ġhome Ġand Ġput Ġmy Ġstuff Ġon Ġand Ġnot Ġsay Ġmy Ġname . ĊĊ Ċ I Ġsaid Ġthat Ġmy Ġname Ġis Ġa Ġlittle Ġmore Ġpersonal , Ġthat Ġit Ġis Ġmy Ġname Ġand ĠI Ġwould Ġlike Ġto Ġmake Ġit Ġmore Ġpersonal Ġnow , Ġthat ĠI Ġdon 't Ġwant Ġother Ġpeople Ġto Ġthink Ġthat , Ġit Ġis Ġtoo Ġannoying , Ġbecause Ġyou Ġare Ġnot Ġmy Ġname . ĊĊ Ċ This Ġis Ġhow ĠI Ġwould Ġlike Ġher Ġname Ġto Ġfeel . ĊĊ Ċ And ĠI Ġthought : ĠOh Ġmy ĠGod , Ġoh Ġmy Ġgod , Ġyou Ġknow Ġshe Ġis Ġall Ġof Ġa Ġsudden . ĊĊ\n",
      "Réponse générée (génération 1) de len 205: I Ġhave Ġto Ġgo Ġto Ġit Ġa Ġcertain Ġway Ġto Ġaccomplish Ġit ,\" Ġshe Ġsaid , Ġ\" It 's Ġimportant Ġto Ġyou Ġkeep Ġthat Ġin Ġmind Ġwhen Ġyou Ġlook Ġat Ġour Ġcountry Ġthat , Ġwhat Ġwe 're Ġtrying Ġto Ġbe Ġmore Ġdemocratic Ġthat Ġwe 're Ġtrying Ġto Ġkeep Ġthat Ġgoing Ġover Ġto Ġdo Ġit Ġso Ġwe Ġcan Ġdo Ġit Ġthat Ġwe Ġcan . ĠThat 's Ġwhat Ġdemocracy Ġhas Ġdone .\" ĠThat Ġwas Ġwhat Ġshe Ġknew Ġso Ġwell Ġand Ġso Ġoften Ġabout Ġthis Ġcountry Ġthat Ġis Ġwhat 's Ġthe Ġthing Ġshe 's Ġtrying Ġto Ġdo Ġwith Ġthat Ġquestion Ġof Ġthe Ġworld Ġit 's Ġbecause Ġit 's Ġthat Ġsimple . ĠSo Ġif Ġyou Ġcan 't Ġmake Ġa Ġdecision Ġon Ġwhat 's Ġyour Ġrole Ġin Ġthis Ġcountry Ġthat Ġis Ġthat Ġyou Ġare Ġtrying Ġto Ġdo Ġthat Ġfor Ġthat . ĠThat 's Ġthe Ġkey . ĠAnd Ġwe Ġthink Ġthat 's Ġhow Ġthis Ġcountry Ġgot Ġitself . ĠWe Ġdon 't Ġwant Ġthis Ġto Ġend Ġup . ĠBecause Ġit 's Ġnot Ġreally Ġabout Ġwho Ġgets Ġthe Ġmost Ġseats . ĠIt 's Ġabout , Ġwell , Ġwe Ġshould Ġend Ġthis Ġgovernment . ĠIt 's Ġabout Ġdemocracy . ĠI Ġthink Ġthat 's Ġwhat Ġit Ġmeans . ĠIt 's Ġabout Ġthe Ġnation Ġyou Ġcan\n",
      "Réponse générée (génération 2) de len 205: I Ġhave Ġto Ġgo Ġto Ġthis Ġmoment . ĠI Ġhave Ġno Ġintention Ġto Ġgo Ġto Ġthis Ġplace Ġto Ġmake Ġa Ġdeal Ġout Ġof Ġwhere ĠI Ġmay Ġgo Ġat Ġa Ġplace Ġin Ġmy Ġlife , ĠI Ġwill Ġjust Ġgo Ġback Ġto Ġwhere ĠI Ġwas Ġgoing . ĠI Ġwill Ġgo Ġback Ġand ĠI Ġwill Ġgo Ġback Ġto Ġwhere ĠI Ġwas Ġthe Ġmoment . ĠI Ġneed Ġto Ġgo Ġback Ġand ĠI Ġwish ĠI Ġwas Ġin Ġmy Ġthat . ĠI Ġknow Ġthat Ġas ĠI Ġam Ġhaving Ġa Ġfeeling . ĠI Ġneed Ġto Ġgo Ġback Ġand ĠI Ġwish ĠI Ġwas Ġout Ġof Ġmy . ĠI Ġwish ĠI Ġwas Ġhome Ġin Ġmy . ĠI Ġhave Ġit Ġthat ĠI Ġwas Ġout Ġof Ġtime Ġin Ġmy Ġlife Ġtime Ġand Ġjust . ĊĊ Ċ No , ĠI Ġdidn 't Ġbelieve Ġyou . ĠI Ġwas Ġin Ġmy . ĠOh , Ġwell Ġthat 's Ġright Ġon , ĠI Ġsaw Ġat ĠI Ġsee . ĊĊ Ċ I 'm Ġa Ġlittle Ġworried , Ġbut Ġthere 's Ġno Ġdoubt Ġthat Ġit Ġwas Ġan Ġimportant Ġmoment . ĠEvery Ġtime ĠI Ġwas Ġout . ĠI Ġguess ĠI 've Ġgone Ġoff Ġand ĠI 'm Ġnot Ġsure Ġthe Ġfirst Ġtime Ġthat , Ġyou Ġknow . ĠYou Ġknow , ĠI 've Ġjust Ġthought ĠI\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-0.3497,  1.1279, -0.7782])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-0.3497,  1.1279, -0.7782])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 5\n",
      "Réponse générée (génération 0) de len 203: My Ġfriend Ġwant Ġto Ġcome Ġto Ġa Ġgame Ġfor Ġme Ġbecause ĠI Ġstill Ġcannot Ġthink Ġof Ġa Ġgame Ġthat Ġwon 't Ġjust Ġleave Ġme .\" ĠâĢĵ Ġ\" I 'd Ġlove Ġfor Ġsomeone Ġto Ġcome Ġto Ġme Ġif ĠI Ġwant Ġto Ġhelp Ġor Ġsupport Ġand Ġall Ġof Ġthat . ĠI Ġwould Ġalso Ġlove Ġfor Ġa Ġfew Ġfriends Ġto Ġcome Ġto Ġme .\" ĠâĢĵ Ġ\" I 'd Ġlove Ġfor Ġa Ġfew Ġfriends Ġto Ġcome Ġto Ġme , Ġif ĠI Ġhad Ġa Ġfriend Ġthat Ġwould Ġlove Ġto Ġcome Ġfor Ġa Ġfriend Ġto Ġcome Ġto Ġme Ġthat Ġwould Ġlike Ġmy Ġfriend Ġwould Ġcome Ġto Ġhave Ġcome Ġto Ġme .\" ĠâĢĵ ĠI Ġfeel ĠI Ġhad Ġan Ġoverwhelming Ġurge Ġto Ġcome Ġto Ġme .\" ĠâĢĵ Ġ\" I Ġhad Ġa Ġfriend Ġin Ġmy Ġoffice Ġthat Ġhad Ġcome Ġto Ġme , Ġand Ġa Ġfriend Ġthat Ġwas Ġcoming Ġand Ġjust Ġwanted Ġme Ġto Ġcome Ġto Ġthe Ġoffice . ĠWe Ġthen Ġsat Ġin Ġa Ġconversation Ġand Ġhe Ġsaid , Ġ\" You Ġare Ġthere ,\" Ġand ĠI Ġsaid Ġthat Ġhe Ġwould Ġcome Ġto Ġour Ġoffice .\" ĠâĢĵ Ġ\" I Ġhad Ġto Ġtell Ġhim . ĠI Ġdo Ġnot Ġwant Ġto Ġbe Ġseen Ġby Ġhis Ġfriend .\" ĠâĢĵ ĠI Ġjust Ġdid Ġnot Ġwant Ġto Ġbe Ġseen\n",
      "Réponse générée (génération 1) de len 203: My Ġfriend Ġwant Ġto Ġcome Ġget Ġme , Ġbut Ġno , ĠI Ġdon 't Ġwant Ġyou . ĠI 'm Ġon Ġmy Ġway Ġto Ġgo Ġhome , ĠI Ġhave Ġyour Ġfriend Ġthat 's Ġgone , Ġand Ġyou Ġgot Ġthe Ġwhole Ġthing Ġbehind Ġme .\" Ċ Ċ \" Oh .\" Ċ Ċ Ċ \" What ?\" Ċ Ċ \" I Ġwant Ġto Ġgo , Ġso ĠI Ġget Ġmy Ġgirlfriend . ĠI Ġdon 't Ġwant Ġto Ġbe Ġout Ġwith Ġyou . ĠBut ...\" Ċ Ċ \" Well Ġyou Ġwill Ġif Ġit 's Ġme .\" Ċ Ċ \" But Ġyou Ġknow ĠI 'm Ġnot Ġa Ġgood Ġgirl Ġunless ĠI Ġget Ġout . ĠI Ġcould Ġgo Ġaway Ġand Ġgo Ġlike Ġit . ĠMy Ġfriends Ġwould Ġall Ġgo Ġlike Ġit . ĠOh hhh .\" Ċ Ċ \" And Ġthat 's Ġwhy ĠI Ġhave Ġso Ġmany Ġthings . ĠWhy Ġdo Ġyou Ġwant Ġto Ġcome Ġout ?\" Ċ Ċ \" Because Ġyou Ġare Ġmy Ġgirlfriend ? ĠI Ġwant Ġto Ġbe Ġmy Ġgirlfriend Ġtoo .\" Ċ Ċ \" But Ġyou Ġwon 't Ġleave Ġme ?\" Ċ Ċ \" No , Ġyou 'll Ġleave . ĠBut ĠI 'll Ġbe Ġyour Ġgirlfriend Ġwhen ĠI Ġget Ġout Ġwith Ġyou Ġfor Ġthe Ġfirst Ġtime Ġaround .\"\n",
      "Réponse générée (génération 2) de len 203: My Ġfriend Ġwant Ġmy Ġdog Ġto Ġbe Ġok ? ĠWho Ġwill Ġmake Ġanother Ġdog ? ĠThe Ġonly Ġthing ĠI Ġhave Ġleft Ġbehind Ġis Ġmy Ġcats . ĠI Ġhate Ġthat ĠI 've Ġgot Ġthem , Ġand ĠI Ġhate Ġyour Ġnew Ġpuppy , ĠI Ġknow Ġthat . ĊĊ Ċ And Ġdon 't Ġbe Ġafraid . ĠThe Ġworld Ġis Ġa Ġbig Ġbig Ġbig Ġlot Ġwe Ġhave Ġbeen Ġgoing Ġthrough , Ġwith Ġthe Ġdogs Ġfor Ġthe Ġlast Ġlast Ġ30 Ġdays . ĠI Ġhave Ġhad Ġthe Ġdogs Ġfor Ġ30 Ġdays . ĠIt 's Ġall Ġbeen Ġa Ġpretty Ġhard Ġto Ġhandle . Ċ Ċ Ċ So Ġwe Ġthink Ġwe 're Ġgetting Ġto Ġthe Ġpoint Ġof Ġthe Ġworld . ĠWe 've Ġbeen Ġhaving Ġpretty Ġgood , Ġwe Ġknow Ġthe Ġsituation Ġnow . Ċ Ċ It Ġwas Ġvery , Ġvery Ġhard . ĠA Ġlot Ġof Ġpeople Ġknew Ġthe Ġsituation Ġthat Ġwas Ġgoing , Ġthat Ġit Ġwasn 't Ġgoing Ġwell . ĠIn Ġsome Ġway Ġit Ġwasn 't Ġgoing Ġto Ġlast . ĠAnd Ġwe Ġknow Ġnow Ġit Ġwill . ĠAs ĠI Ġsaid , Ġwe 're Ġgetting Ġto Ġthat Ġpoint . Ċ Ċ That Ġmeans Ġit Ġcould Ġgo Ġbad , Ġand Ġwe Ġcan Ġmake Ġsome Ġchanges Ġto Ġit 's Ġthe Ġnext Ġpoint . ĠWe\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 0.7334, -1.1391,  0.4057])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 0.7334, -1.1391,  0.4057])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: -0.0829\n",
      "Training on prompt 0\n",
      "Réponse générée (génération 0) de len 203: The Ġsky Ġis Ġmy Ġhouse , Ġa Ġblue Ġsky , ĠI Ġam Ġdreaming Ġdreaming Ġwith Ġa Ġblue Ġsky . ĠYou Ġhave Ġno Ġname , ĠI Ġlive Ġin Ġa Ġblue Ġsky . ĊĊ Ċ I Ġam Ġin Ġa Ġblue Ġsky Ġyou Ġhave Ġa Ġname . Ċ Now , Ġyou Ġget Ġto Ġthe Ġother Ġside Ġof Ġthe Ġocean , ĠI 'm Ġa Ġblue Ġsky . ĊĊ Ċ I 've Ġbuilt Ġthis Ġsea Ġin Ġthe Ġground Ġthat Ġyou Ġsee , Ġthe Ġearth . ĠBut Ġthe Ġsea Ġis Ġalways Ġhere . ĊĊ Ċ I Ġwill Ġgo Ġto Ġthe Ġother Ġside , Ġgo Ġto Ġthe Ġother Ġside , Ġand Ġwe Ġcome . Ċ Ċ I Ġam Ġhere Ġby Ġmy Ġown Ġhand , Ġand ĠI Ġam Ġon Ġthis Ġfloor Ġof Ġthe Ġblue Ġsky . Ċ Ċ You Ġhave Ġno Ġname , Ġhave Ġthis Ġsky Ġis Ġyour Ġname . Ċ Ċ I Ġam Ġhere Ġthe Ġblue Ġsky , Ġwe Ġare Ġbuilding Ġup Ġblue Ġsky . Ċ Ċ You Ġhave Ġa Ġname Ġbut Ġa Ġname , Ġand Ġa Ġname Ġis Ġthat 's Ġa Ġperson Ġand ĠI Ġam Ġbuilding Ġthis Ġsky . ĊĊ Ċ I Ġam Ġin Ġa Ġblue Ġsky , ĠI Ġhave Ġseen Ġthe Ġblue Ġsky Ċ Ċ That 's Ġa\n",
      "Réponse générée (génération 1) de len 203: The Ġsky Ġis Ġvery Ġclear Ġas Ġthere Ġare Ġmany Ġclouds Ġin Ġthe Ġsky Ġand Ġalso Ġthere Ġare Ġmany Ġstars Ġin Ġthe Ġsky .\" ĠI Ġwent Ġto Ġget Ġsome Ġinformation Ġbut ĠI Ġdidn 't Ġhave Ġtime Ġfor Ġit .\" The Ġday Ġthen ĠI Ġwent Ġto Ġgo Ġto Ġgo Ġfind Ġthose Ġstars Ġand Ġhad Ġthe Ġday Ġbegin . ĠAs ĠI Ġlooked Ġat Ġmy Ġphone Ġto Ġget Ġa Ġpicture Ġof Ġmy Ġhome Ġphone Ġand Ġthen Ġit Ġsaid Ġa Ġlong Ġtime Ġago Ġand ĠI Ġneeded Ġsomething Ġto Ġgo Ġgo Ġto Ġthe Ġdoor Ġto Ġfind Ġthat Ġtime Ġand Ġhe Ġsaid ĠI Ġwant Ġto Ġcome Ġsee Ġand Ġtalk Ġto Ġhim . ĠThey Ġjust Ġgave Ġme Ġthe Ġday Ġand Ġtold Ġme Ġto Ġgo Ġand Ġthey Ġcalled Ġme Ġand Ġthey Ġwere Ġlike Ġ\" What Ġare Ġyou Ġdoing ?!\" ĠSo ĠI Ġtold Ġthem Ġand ĠI Ġwas Ġlike Ġ\" Hey Ġdo Ġwe Ġget Ġa Ġphone Ġcall Ġthat Ġthere Ġand Ġyou Ġwill Ġhelp Ġme .\" ĠI Ġalso Ġtold Ġthem ĠI Ġwill Ġdo Ġthat Ġyou Ġgo Ġand Ġthey Ġsaid Ġand Ġwent Ġto Ġcome Ġget Ġa Ġphone Ġcall Ġcall Ġand Ġhe Ġsaid Ġ\" Call Ġto Ġsay ĠI Ġwanna Ġhelp Ġyou Ġand Ġthere Ġis Ġnothing Ġto Ġdo Ġon Ġthe Ġcall . ĠYou Ġand ĠI Ġwill Ġlet Ġyou\n",
      "Réponse générée (génération 2) de len 203: The Ġsky Ġis Ġthe Ġsame Ġin Ġthe Ġfirst Ġfew Ġhours Ġof Ġyour Ġstay ,\" Ġthe Ġwoman Ġsaid Ġafter Ġshe Ġwoke Ġherself Ġup Ġfor Ġdinner Ġthe Ġnext Ġmorning . Ċ Ċ At Ġthe Ġtime , ĠMs . ĠKelly Ġcalled Ġher Ġmom , ĠMary , ĠMary ĠAnn , Ġwho Ġis Ġa Ġnursing Ġmom Ġat ĠUniversity ĠMedical ĠCenter Ġin ĠColumbus , ĠOhio , Ġwho Ġworks Ġfor Ġthe Ġcity Ġgovernment Ġand Ġwho Ġis Ġa Ġnurse Ġand Ġparent Ġof Ġthe Ġwomen Ġwho Ġworked Ġfor Ġher Ġduring Ġthe Ġwar . Ċ Ċ One Ġwoman Ġwho Ġworked Ġfor Ġher Ġgrandfather Ġwho Ġlost Ġher Ġparents Ġor Ġboth Ġin Ġthe Ġwar , Ġsaid Ġshe Ġwoke Ġup Ġagain Ġafter Ġher Ġfather Ġdied Ġafter Ġan Ġoverdose . ĠShe Ġsaid Ġshe Ġhad Ġa Ġlot Ġof Ġhope Ġthat Ġher Ġgrandchildren Ġwould Ġbe Ġlucky Ġeven Ġgetting Ġthe Ġcourage Ġto Ġkeep Ġon . Ċ Ċ Ċ In Ġher Ġmother 's Ġview , Ġit Ġwas Ġbetter Ġthan Ġa Ġlot Ġof Ġwhat Ġit Ġwas Ġgoing Ġto Ġbe . Ċ Ċ There Ġwas Ġa Ġdifference . ĠIt Ġwas Ġbigger Ġthan Ġthe ĠUnited ĠStates . ĠThere Ġwas Ġa Ġdifference Ġbetween Ġbeing Ġin Ġthe Ġmilitary Ġand Ġa Ġstate . ĠIt Ġwas Ġthe Ġsame Ġas Ġall Ġthe Ġother Ġcountries Ġwhere Ġthe Ġmen Ġwere Ġsent\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-1.1087,  0.2749,  0.8338])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-1.1087,  0.2749,  0.8338])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 1\n",
      "Réponse générée (génération 0) de len 205: Yesterday , ĠI Ġwent Ġto Ġthe Ġgym Ġwith Ġa Ġbunch Ġof Ġfriends Ġwho Ġhave Ġreally Ġenjoyed Ġthe Ġlast Ġfew Ġyears , Ġsome Ġof Ġthem Ġvery Ġvery Ġrecently Ġand Ġsome Ġof Ġcourse ĠI Ġam Ġnot Ġsure Ġthe Ġanswer Ġto Ġany Ġof Ġit Ġin Ġterms Ġof Ġgetting Ġa Ġgood Ġworkout . ĠBut Ġof Ġcourse ĠI Ġam Ġhere Ġdoing Ġthat Ġand Ġif Ġyou Ġhave Ġgot Ġa Ġlot Ġof Ġfun , Ġhow Ġdo Ġyou Ġdecide Ġwhat 's Ġyour Ġanswer Ġhere Ġfor Ġcoming Ġup Ġwith Ġso , Ġif Ġyou Ġwant Ġto Ġstart Ġout Ġwith Ġa Ġbunch Ġof Ġthings Ġin Ġgeneral . ĠFirst Ġof Ġall , ĠI Ġam Ġnot Ġa Ġreal Ġphysical Ġtype Ġbut Ġwhat ĠI Ġknow Ġis Ġthat Ġfor Ġyou , Ġis Ġthat Ġpeople Ġtend Ġto Ġbe Ġstrong . ĠThey Ġare Ġvery Ġstrong Ġwith Ġtheir Ġbody Ġand Ġtheir Ġthinking . ĠBut Ġthey Ġare Ġvery Ġlittle Ġdo in ' Ġto Ġme . ĠLet Ġit Ġbe Ġreal Ġand Ġyou Ġare Ġgoing Ġto Ġtry Ġto Ġstart Ġit Ġand , Ġwhatever Ġyou Ġdo , Ġthen Ġyou Ġare Ġgoing Ġto Ġtry Ġto Ġdo . ĠI Ġdon 't Ġbelieve Ġin Ġit Ġand , Ġif Ġyou Ġdo Ġdo Ġsomething Ġstupid Ġand Ġyou Ġdo Ġsomething Ġstupid Ġand Ġthe Ġlike Ġand Ġyou Ġwill Ġtry Ġand Ġdo Ġit Ġon Ġthe\n",
      "Réponse générée (génération 1) de len 205: Yesterday , ĠI Ġwent Ġto Ġmy Ġhouse Ġto Ġgo Ġbuy Ġhis Ġwedding Ġflowers . ĠI Ġdon 't Ġthink Ġthat ĠI Ġhave Ġa Ġphoto Ġalbum Ġ( at Ġleast ĠI Ġhave Ġmy Ġwife , ĠI Ġswear !! ), Ġso âĢ¦ I Ġtold Ġhim Ġthat ĠI Ġhad Ġan Ġalbum Ġbut Ġno Ġdate . ĠI Ġthought Ġhe Ġand ĠI Ġwould Ġbe Ġfriends . ĠI Ġbought Ġthe Ġwedding Ġalbum , ĠI Ġalso Ġbought Ġthe Ġalbum ĠI Ġdidn 't Ġknow Ġabout Ġthat Ġhad Ġbeen Ġreleased Ġand Ġwas Ġsold Ġto Ġmy Ġson , Ġand Ġmy Ġhusband Ġis Ġso Ġsad Ġfor Ġhim Ġthat Ġit Ġhas Ġhappened . ĠIt Ġmay Ġhappen Ġto Ġanother Ġperson . ĠBut ĠI Ġhave Ġa Ġlot âĢ¦ many Ġmany Ġyears Ġ( when Ġthey Ġnever Ġdied Ġtogether ) Ġof Ġmemories . ĠI Ġmean ĠI Ġknow Ġmy Ġhusband Ġbut ĠI 'm Ġnot Ġthat Ġhappy Ġabout Ġthis Ġmoment . ĠHe Ġhas Ġa Ġlot Ġof Ġmemories Ġof Ġbeing Ġaway Ġfrom Ġmy Ġson Ġbut Ġhe Ġhas Ġthose Ġmemories Ġnot Ġback . ĠHe Ġis Ġthat Ġsad Ġabout Ġme Ġthat Ġtime , Ġhe Ġwent Ġthrough Ġit Ġto Ġmy Ġhusband Ġand Ġthey Ġwere Ġthere Ġfor Ġages Ġand Ġthey Ġwould Ġhave Ġbeen Ġback Ġthen Ġand ĠI Ġdon 't Ġknow Ġabout Ġhim . ĠI Ġguess Ġthat Ġwhen Ġit Ġwas Ġour\n",
      "Réponse générée (génération 2) de len 205: Yesterday , ĠI Ġwent Ġto Ġbuy Ġa Ġhome Ġin ĠCalifornia . ĠA Ġyear Ġago , ĠI Ġwas Ġat Ġmy Ġsister 's Ġwedding . ĠIt Ġtook Ġtwo Ġyears Ġfor Ġme Ġto Ġfinally Ġpay Ġthe Ġbill , Ġso ĠI Ġknew Ġa Ġway Ġbetter Ġthan ĠI Ġcould Ġtell Ġthem . Ċ Ċ When ĠI Ġbegan Ġmy Ġwriting Ġthis Ġseries Ġof Ġblog Ġposts ĠI Ġwas Ġsurprised Ġto Ġhear Ġwhat ĠI Ġexpected . ĠI Ġmean . ĠIt Ġis Ġpretty Ġmuch Ġa Ġlittle Ġbit Ġabout Ġme , Ġwhat ĠI Ġsaw Ġand Ġwhat , Ġand Ġwhat ĠI Ġheard . ĠThis Ġstory Ġabout Ġa Ġbeautiful Ġlittle Ġhome Ġthat Ġyou Ġhad Ġon Ġa Ġvery Ġquick Ġdate Ġand Ġwhere Ġit Ġhad Ġa Ġnice Ġlittle Ġroom Ġand Ġa Ġfamily Ġin Ġthe Ġfamily . Ċ Ċ So Ġafter Ġmonths Ġof Ġresearch , ĠI Ġdecided ĠI Ġwill Ġcontinue Ġto Ġread Ġand Ġwrite Ġabout Ġmy Ġexperiences Ġand Ġhope Ġyou Ġare Ġhappy ĠI Ġam Ġfollowing Ġmy Ġheart Ġon Ġthis Ġbook Ġwhen ĠI Ġam Ġreading Ġit . ĠIt Ġis Ġjust Ġa Ġlittle Ġtoo Ġshort Ġto Ġbe Ġvery Ġmuch Ġabout Ġyour Ġheart , Ġand Ġif Ġyou Ġwere Ġa Ġfan Ġof Ġit Ġtoo , ĠI Ġwould Ġlove Ġit Ġjust Ġa Ġlittle Ġbit . ĠI Ġhope Ġyou Ġwill Ġbe Ġfollowing Ġmy Ġstory Ġand Ġwould\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 0.4932, -1.1508,  0.6576])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 0.4932, -1.1508,  0.6576])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 2\n",
      "Réponse générée (génération 0) de len 204: I Ġhope ĠI Ġwill Ġnever Ġhave Ġto Ġsay Ġthe Ġsame Ġabout Ġthose Ġpeople , Ġand ĠI Ġhope Ġthat Ġit Ġwill Ġnever Ġbe Ġthe Ġsame ,\" Ġhe Ġsays . Ġ\" And ĠI Ġhope Ġthat Ġit Ġwill Ġnever Ġbe Ġbecause ĠI 'm Ġscared Ġit . ĠI Ġhope ĠI Ġwill Ġnever Ġbe Ġscared Ġto Ġsay Ġthis .\" Ċ Ċ L ori Ġalso Ġdoesn 't Ġfeel Ġparticularly Ġfearful Ġabout Ġlosing Ġhis Ġfather . Ċ Ċ Ċ \" That 's Ġnot Ġgoing Ġto Ġstop Ġme Ġat Ġthe Ġidea Ġof Ġlosing Ġmy Ġfather . ĠI Ġknow Ġthat 's Ġso Ġtrue .\" Ċ Ċ \" Oh Ġno , Ġit 's Ġjust Ġthat . ĠI Ġthink Ġthat Ġyou 're Ġgoing Ġto Ġlose Ġit ,\" Ġshe Ġsays , Ġchuck ling . Ċ Ċ Ċ \" I Ġdon 't Ġknow Ġif ĠI Ġcould ,\" Ġsays Ġthe Ġreporter Ġbehind Ġthe Ġstage . Ċ Ċ \" Of Ġcourse Ġnot ,\" Ġthe Ġreporter Ġcontinues . Ċ Ċ \" I Ġthink Ġhe 's Ġgoing Ġto Ġtake Ġan Ġeasy Ġroute , Ġif Ġthat 's Ġpossible ,\" Ġshe Ġsays . Ċ Ċ \" Right ,\" Ġthe Ġreporter Ġsays . Ċ Ċ \" Well , Ġthat 's Ġpretty Ġtypical , Ġthough , ĠI Ġmean , Ġthat 's Ġsomething .\" Ċ Ċ\n",
      "Réponse générée (génération 1) de len 204: I Ġhope ĠI Ġwill Ġuse Ġmy Ġtime Ġand Ġresources Ġto Ġdo Ġsomething Ġabout Ġthis Ġtime . ĠIt Ġis Ġabout Ġme Ġgetting Ġto Ġknow Ġwhat Ġhappened Ġthis Ġtime Ġof Ġday , Ġgoing Ġup , Ġlooking , Ġit Ġis Ġvery Ġfunny . ĠI 'm Ġtrying Ġto Ġgo Ġup Ġand Ġtry Ġto Ġget Ġsomething Ġout Ġof Ġthat Ġmoment Ġthat Ġsays Ġsomething .\" Ċ Ċ The Ġvideo Ġwas Ġrecorded Ġby Ġthe Ġsame Ġday Ġon ĠSeptember Ġ23 rd , Ġ2016 . Ċ Ċ The Ġwoman Ġfrom ĠTexas Ġwas Ġat Ġthe Ġchurch , Ġshe Ġsaid . Ċ Ċ Ċ \" One Ġwoman , ĠI Ġwas Ġin Ġan Ġaccident . ĠShe Ġwas Ġon Ġguard Ġand Ġwhen ĠI Ġwas Ġthere ĠI Ġwas Ġseeing Ġa Ġperson Ġand Ġshe Ġis Ġa Ġpassenger Ġin Ġthe Ġcar , Ġthe Ġdriver Ġwent Ġinto Ġthe Ġcar Ġand Ġthat Ġis Ġwhen Ġit Ġall Ġhappened Ġto Ġme . ĠI Ġwas Ġat Ġthe Ġreception , Ġand Ġthe Ġdriver Ġwas Ġsaying , Ġ\" I 'm Ġtrying Ġto Ġsee Ġyour Ġbrother . ĠI 'm Ġtrying Ġto Ġget Ġto Ġget Ġyour Ġbrother , Ġbut Ġthat Ġis Ġbecause Ġof Ġmy Ġbrother ĠI Ġwas Ġsupposed Ġto Ġwatch .\" ĠThat Ġwas Ġa Ġmoment Ġthat ĠI Ġwas Ġsupposed Ġto Ġbe Ġthere Ġand ĠI Ġwas Ġgetting Ġthat Ġmoment ,\n",
      "Réponse générée (génération 2) de len 204: I Ġhope ĠI Ġwill , Ġyou Ġare Ġcorrect ,\" Ġthe Ġofficer Ġsaid , Ġand Ġthen Ġwent Ġon Ġto Ġdescribe Ġhis Ġexperience Ġas Ġa Ġprisoner . ĠThe Ġofficers Ġtook Ġhim Ġto Ġa Ġsecurity Ġoffice , Ġwhere Ġhe Ġwas Ġarrested Ġand Ġwas Ġsubsequently Ġtransferred Ġto Ġanother Ġunit Ġthat Ġwas Ġoperated Ġby Ġthe Ġofficer 's Ġassigned Ġto Ġthe Ġofficer . Ċ Ċ \" Then Ġwhat ?\" ĠI Ġsaid Ġand Ġhis Ġresponse Ġwas , Ġ\" Well , Ġwhat , Ġsir , Ġdidn 't Ġyou Ġread Ġmy Ġmessage ?\" <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 0.3554,  0.7737, -1.1292])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 0.3554,  0.7737, -1.1292])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 3\n",
      "Réponse générée (génération 0) de len 203: My Ġcar Ġis Ġin Ġit 's Ġplace . ĠIt Ġis Ġnot Ġyour Ġcar . ĠIt Ġis Ġnot Ġyour Ġcar 's Ġplace . ĠIt Ġis Ġnot Ġsome Ġcar Ġfrom Ġyou Ġsay Ġthere Ġyou Ġare . ĠIt Ġis Ġa Ġthing Ġyou Ġtell Ġme Ġyou Ġdo Ġsee Ġin Ġyour Ġroom . ĠYour Ġroom . ĠAnd Ġjust Ġbecause Ġyou Ġare Ġstill Ġyou Ġin Ġa Ġcar Ġthere Ġdoesn 't Ġmean Ġyou Ġare Ġnot Ġthere . ĠThis Ġis Ġsome Ġplace . ĠThey Ġare Ġlike Ġa Ġlittle Ġplace Ġyou Ġknow Ġit Ġyou Ġare Ġright Ġthere Ġright Ġnow Ġdon 't Ġthink Ġabout Ġyour Ġroom . ĠI Ġdon 't Ġknow Ġthis , Ġthis Ġis Ġsomething Ġyou Ġwill Ġthink Ġabout Ġthis . ĠAnd Ġif Ġyou Ġgo Ġto Ġyour Ġcar Ġyou Ġcan Ġbe Ġthere . ĠYour Ġcar Ġyou Ġknow Ġwhat ? ĠAnd Ġtell Ġme Ġthen Ġtell Ġme Ġthat Ġyou Ġare Ġthere . ĠYour Ġcar . ĠYour Ġcar Ġon Ġyour Ġway . ĠThe Ġcar . ĠI Ġtold Ġyou Ġin Ġmy Ġroom Ġthat Ġis Ġright Ġway Ġthen Ġit 's Ġa Ġlittle Ġplace . ĠIt Ġis Ġnothing Ġmore Ġthan Ġthat . ĠIt Ġis Ġjust Ġsomething Ġthat Ġwhen ĠI Ġwas Ġwalking Ġmy Ġcar Ġstarted Ġgoing . ĠIt Ġis Ġthe Ġplace . ĠIt Ġis Ġjust Ġlike Ġmy Ġcar Ġthat\n",
      "Réponse générée (génération 1) de len 203: My Ġcar Ġis Ġin Ġa Ġreally Ġgood Ġstate Ġbecause ĠI Ġwas Ġout Ġthere Ġtoday Ġand Ġmy Ġcar Ġis Ġdoing Ġgreat . ĠFor Ġthe Ġmoney , Ġit Ġall Ġdepends Ġupon Ġwho Ġis Ġdriving Ġit Ġand Ġthat Ġit Ġmakes Ġno Ġdifference . Ċ Ċ As ĠI Ġlook Ġat Ġit , ĠI Ġhave Ġalways Ġfelt Ġthis Ġis Ġa Ġbeautiful Ġcar . ĠI Ġhave Ġeven Ġseen Ġthe Ġcars Ġto Ġthe Ġleft Ġof Ġher Ġthat Ġlooked Ġmore Ġstunning Ġthan Ġmy Ġfamily Ġcar . Ċ Ċ ( Laughs ) Ċ Ċ I Ġam Ġone Ġof Ġthose Ġpeople Ġwho Ġjust Ġfeels Ġlike , Ġokay ! ĠI Ġhave Ġtaken Ġit Ġfor Ġgranted . ĠI Ġhave Ġalways Ġfelt Ġas Ġif ĠI Ġknow Ġhow Ġto Ġdrive Ġmy Ġcars Ġwhen ĠI Ġget Ġthere . ĠI Ġknow Ġwhen ĠI Ġget Ġthere , ĠI 'm Ġthere . Ċ Ċ I 'm Ġjust Ġa Ġperson Ġwho Ġis Ġgood Ġat Ġfinding Ġfault Ġwith Ġthe Ġworld . ĠI Ġknow Ġwhat Ġit Ġtakes Ġto Ġbe Ġa Ġperson Ġwho Ġwants Ġto Ġbe Ġa Ġcar Ġand ĠI Ġwill Ġalways Ġlove Ġyou . Ċ Ċ J OE : ĠI 'm Ġvery Ġmuch Ġhonored Ġfor Ġthe Ġguy Ġwho Ġwent Ġon Ġthe Ġride . Ċ Ċ CH EN TH : ĠLet Ġme Ġtell Ġyou ,\n",
      "Réponse générée (génération 2) de len 203: My Ġcar Ġis Ġthe Ġhome Ġyou Ġgo Ġto Ġand Ġwhere 's Ġthe Ġguy Ġwho Ġgoes Ġback Ġto Ġthe Ġmain Ġhouse Ġin Ġit Ġand Ġthey Ġneed Ġthe Ġkids Ġback Ġto Ġgo Ġthrough Ġit , Ġokay ?\" Ċ Ċ \" It 's Ġnot Ġlike , Ġyou 've Ġgot Ġit Ġup Ġon , Ġyou Ġshould 've Ġbeen Ġprepared .\" Ċ Ċ The Ġgirl Ġsaid Ġas Ġshe Ġwalked Ġin Ġand Ġstarted Ġto Ġgo Ġback Ġthrough Ġthe Ġold Ġhallway . Ċ Ċ \" There 's Ġanother Ġperson Ġcoming Ġout Ġand Ġthey 're Ġgoing Ġto Ġtell Ġthem Ġthey 're Ġgoing Ġto Ġlet Ġus Ġin Ġwhen Ġwe Ġget Ġthe Ġwhole Ġtime Ġback , Ġokay ?\" Ċ Ċ \" Right .\" Ċ Ċ \" H aha , ĠI Ġdidn 't Ġmean Ġto Ġsay ĠI Ġdidn 't .\" Ċ Ċ \" Just Ġkidding , Ġwe Ġshould 've Ġdone Ġthis Ġtogether Ġas Ġa Ġfamily Ġhere Ġbecause Ġit 's Ġall Ġthe Ġkids Ġand Ġthe Ġkids Ġcome Ġback , Ġokay ? ĠYou Ġknow Ġyou Ġhave Ġto Ġstay Ġin Ġwith Ġthe Ġkids Ġfor Ġa Ġwhile Ġnow Ġand Ġit Ġshouldn 't Ġeven Ġmatter Ġnow . ĠYou Ġknow Ġwhat 's Ġup Ġin Ġthe Ġback side , Ġwe 're Ġgonna Ġhave Ġto Ġgo Ġwith Ġyou Ġand Ġsee Ġwhat Ġwe\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 1.1508, -0.6576, -0.4932])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 1.1508, -0.6576, -0.4932])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 4\n",
      "Réponse générée (génération 0) de len 205: I Ġhave Ġto Ġgo Ġto Ġwhat 's Ġhappening , Ġthe Ġproblem Ġis Ġthat Ġno Ġtime Ġhas Ġpassed Ġand Ġthere Ġaren 't Ġany Ġattempts Ġwere Ġmade . ĠThis Ġis Ġthe Ġbig Ġone Ġfor Ġme Ġand ĠI 've Ġbeen Ġasking Ġfor Ġit , Ġthe Ġissue Ġis Ġalso ĠI Ġwill Ġsay Ġwhat Ġthe Ġprocess Ġof Ġthe Ġissue Ġis Ġand ĠI Ġwill Ġdo Ġwhat Ġhas . Ċ Ċ I Ġhave Ġdone Ġall Ġthat ĠI Ġneed Ġto Ġknow Ġa Ġlot Ġfrom Ġthe Ġissues Ġthat Ġare Ġin Ġthe Ġissues Ġthat Ġwe Ġneed . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "Réponse générée (génération 1) de len 205: I Ġhave Ġto Ġgo Ġto Ġthe Ġbathroom . ĠYou Ġbetter Ġcall Ġthe Ġguy Ġin Ġfront Ġof Ġme . ĠYou 've Ġgot Ġto Ġgo Ġto Ġthe Ġbathroom Ġrestroom . ĠIt 's Ġlike Ġthe Ġbathroom Ġbathroom Ġand , Ġand Ġyou 're Ġthinking Ġit 's Ġthe Ġrestroom . ĠIt 's Ġthe Ġbathroom . Ċ Ċ You Ġget Ġup Ġat Ġthe Ġend Ġof Ġthe Ġtoilet Ġbowl , Ġand Ġit Ġgets Ġreally , Ġreally , Ġreally , Ġreally Ġhot . Ċ Ċ You Ġknow , Ġon Ġthe Ġway Ġhome Ġin Ġthe Ġmorning , Ġyou 're Ġright Ġin Ġfront Ġof Ġthe Ġbathroom . Ċ Ċ I Ġjust Ġstarted Ġa Ġbook . Ċ Ċ I Ġdon 't Ġdo Ġthat Ġmuch . ĠI Ġdo Ġso Ġmuch , Ġbut Ġwhen Ġall Ġthese Ġgirls Ġyou Ġdo , Ġwhat Ġdo Ġyou Ġdo ? Ċ Ċ Oh , Ġthat 's Ġwhat , Ġyou 're Ġlooking Ġat Ġthe Ġbathroom Ġand Ġsitting Ġon Ġyour Ġface . Ċ Ċ Don 't Ġever Ġtell Ġme , Ġyou 're Ġso Ġweird Ġthat Ġyou Ġhave Ġno Ġfriends , Ġand Ġno Ġfriends . ĠAnd Ġyou 're Ġlike , Ġwhat Ġthe Ġhell Ġis Ġwrong Ġwith Ġyou ? Ċ Ċ Oh , Ġso , Ġyou 're Ġsitting Ġon Ġyour Ġface Ġbeing Ġin Ġfront Ġof Ġthe Ġbathroom\n",
      "Réponse générée (génération 2) de len 205: I Ġhave Ġto Ġgo Ġto Ġthe Ġkitchen Ġwhen Ġwe Ġhit Ġthe Ġtown .\" Ċ Ċ She Ġhas Ġrecently Ġspent Ġthe Ġnight Ġto Ġher Ġown , Ġand Ġhas Ġa Ġgreat Ġcase Ġof Ġwhat Ġshe Ġhas Ġleft . Ċ Ċ On Ġthe Ġnight Ġof ĠJune Ġ25 , Ġ2010 , ĠB um ata Ġwas Ġin Ġthe Ġmarket , Ġas Ġa Ġpart - time Ġvendor Ġfor Ġan Ġoffice Ġsupply Ġcompany Ġcalled Ġ' The ĠH unch .\" ĠThere Ġshe Ġwas Ġtaking Ġher Ġchildren Ġas Ġkids . ĠShe Ġmade Ġseveral Ġtrips Ġfrom Ġthe Ġhotel Ġto Ġthe Ġcar Ġas Ġa Ġpart - time Ġvendor , Ġbut Ġwith Ġher Ġdaughter Ġand Ġher Ġdaughter , Ġan Ġadult Ġto Ġbe Ġfair , Ġor Ġin Ġa Ġpart - time Ġbusiness , Ġshe Ġhad Ġlittle Ġmoney Ġleft . Ċ Ċ As Ġfar Ġas Ġher Ġdaughters Ġwere , Ġshe Ġhad Ġa Ġmuch Ġsmaller Ġamount Ġof Ġmoney Ġthan Ġother Ġpeople Ġwho Ġwanted Ġto , Ġand Ġthey Ġwere Ġvery Ġpoor Ġto Ġhave , Ġbut Ġat Ġa Ġtime Ġwhere Ġnobody Ġhad Ġbeen Ġasked Ġto Ġdo Ġthe Ġwork , Ġthat Ġis Ġin Ġall Ġprobability ĠB um ata Ġhad Ġthe Ġcash Ġnow Ġfor Ġhis Ġjob Ġwith Ġthe ĠH unch , Ġthe Ġpart - time Ġclerk , Ġhe Ġwas Ġgoing Ġto Ġtake Ġa Ġchild\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([-1.1468,  0.4564,  0.6904])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([-1.1468,  0.4564,  0.6904])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 5\n",
      "Réponse générée (génération 0) de len 203: My Ġfriend Ġwant Ġto Ġplay Ġwith Ġme Ġand ĠI Ġplay Ġwith Ġmy Ġpartner . ĠWe Ġshould Ġthink Ġabout Ġeach Ġother 's Ġminds .\" Ċ Ċ In Ġthis Ġcontext , Ġ\" you \" Ġis Ġnot Ġa Ġcompliment , Ġbut Ġa Ġname , Ġa Ġname , Ġnot Ġin Ġthe Ġleast , Ġor Ġa Ġmoniker Ġthat Ġis Ġsomething Ġas Ġbig Ġas Ġsomething Ġof Ġsomething Ġwhich Ġyou Ġare Ġvery Ġcapable Ġof Ġthinking , Ġand Ġhow Ġmany Ġof Ġyou Ġare Ġcapable Ġof Ġthinking . ĠA Ġcertain Ġtype Ġof Ġname Ġwhich Ġis Ġan Ġindividual Ġname Ġas Ġa Ġsingular Ġname , Ġand Ġthere Ġis Ġa Ġcertain Ġtype Ġof Ġtype Ġof Ġnoun Ġwhich Ġis Ġin Ġa Ġsingular Ġor Ġplural Ġway , Ġand Ġas Ġthey Ġlike Ġto Ġbe Ġand Ġin Ġthis Ġcategory Ġwhere Ġit Ġis Ġan Ġexact Ġtype Ġof Ġnoun Ġ( it 's Ġonly Ġin Ġa Ġsingular Ġor Ġplural ) Ġthat Ġis Ġin Ġa Ġsingular Ġor Ġplural Ġway Ġof Ġsaying , Ġor Ġsomething Ġin Ġthis Ġis Ġa Ġsingular Ġor Ġplural Ġor Ġa Ġspecific Ġtime . ĠFor Ġexample , Ġif Ġyou Ġlike Ġto Ġwrite Ġthat ĠI 'm Ġtalking Ġto Ġyou , Ġor Ġwhat Ġis Ġyou Ġspeaking Ġto Ġme , Ġit Ġfeels Ġas Ġif Ġit 's Ġan Ġabsolute Ġthing Ġto Ġsay Ġthis Ġto Ġbe\n",
      "Réponse générée (génération 1) de len 203: My Ġfriend Ġwant Ġto Ġtalk Ġabout Ġthings Ġlike Ġthis , Ġif ĠI Ġdid , Ġwould ĠI Ġthink ĠI Ġwould Ġsay Ġyes , Ġeven Ġif ĠI Ġdid Ġor ĠI Ġwould Ġsay Ġno . ĠThat 's Ġwhere Ġthat Ġall Ġcomes Ġfrom ,\" Ġshe Ġsays . ĠShe Ġgoes Ġon : Ġ\" When ĠI Ġtell Ġpeople Ġabout Ġit ĠI Ġtold Ġthem Ġabout Ġwhat Ġdid Ġwe Ġtalked Ġabout Ġthere Ġwere Ġso Ġmany Ġlayers . ĠLike , Ġwhat Ġdid Ġthey Ġsay Ġbut Ġwhere Ġthey 're Ġjust Ġgoing , Ġthere 's Ġso Ġmany Ġlayers . ĠLike , ĠI Ġjust Ġtell Ġthem Ġyou Ġcan Ġdo Ġwhatever Ġyou Ġwant Ġto Ġdo , Ġit Ġdoesn 't Ġmatter Ġto Ġme . ĠI Ġjust Ġtold Ġthem Ġthat , Ġwhy Ġnot , Ġthat 's Ġhow Ġit Ġit . ĠAnd Ġyou Ġkind Ġof Ġlike Ġask , Ġ' So , Ġwas ĠI Ġsaying Ġno Ġwhen ĠI Ġsaid Ġthat , Ġwhy Ġnot Ġit Ġdidn 't Ġmatter Ġbut Ġwhat Ġdid Ġthey Ġwant Ġto Ġdo Ġto Ġdo ? ĠBut ĠI Ġjust Ġsaid Ġyes , Ġso Ġdid Ġthey Ġwant Ġto Ġdo Ġwhat ? ĠOkay Ġthat 's Ġhow Ġit Ġgoes , Ġthat 's Ġhow Ġthe Ġhell Ġthey Ġwould Ġdo Ġthat Ġyou Ġwould Ġdo Ġwhat , Ġwhat Ġabout Ġall Ġof Ġit , ĠI Ġcan\n",
      "Réponse générée (génération 2) de len 203: My Ġfriend Ġwant Ġto Ġbe Ġlike Ġhis Ġold Ġbest Ġfriend , Ġwho Ġis Ġbeing Ġa Ġbit - off - that - where - I ... Ċ Ċ ( I Ġhope Ġyou Ġdon 't Ġworry Ġabout Ġit 'll Ġwork Ġthe Ġfirst Ġtime , Ġand ĠI Ġdon 't Ġcare ) Ċ Ċ Okay , Ġgo Ġahead , Ġlet 's Ġget Ġgoing Ġand Ġget Ġback Ġto Ġour Ġreal Ġlife . Ċ Ċ You 're Ġa Ġkid ! Ċ Ċ You Ġknow Ġthat Ġyou 're Ġa Ġlittle Ġ\" little \" Ġwhen Ġit Ġcomes Ġto Ġtalking Ġto Ġan Ġolder Ġneighbor Ġabout Ġwhether Ġa Ġbig , Ġbig Ġdog Ġin Ġevery Ġgarage , Ġetc . ĠHow Ġdo Ġyou Ġsee Ġthat ? Ċ Ċ Just Ġgo Ġto Ġyour Ġkids ' Ġhouses Ġfor Ġa Ġlittle . Ċ Ċ Good , Ġyour Ġkid , Ġyou Ġare , Ġso Ġwhen Ġyou 're Ġolder , Ġyou Ġare Ġthe Ġonly Ġone Ġthat Ġis Ġhaving Ġa Ġgood Ġtime . ĠSo Ġyou Ġcan Ġgo Ġon Ġyour Ġway Ċ Ċ You 're Ġnot Ġgoing Ġto Ġhave Ġa Ġbad Ġtime , Ġthough , Ċ Ċ ( and Ġthat 's Ġthe Ġonly Ġone Ġat Ġthe Ġend Ġof Ġit ) Ċ Ċ What Ġdo Ġyou Ġthink Ġyou 're Ġgoing Ġto Ġbe Ġall\n",
      "<class 'list'>\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([ 1.0062, -0.0124, -0.9937])\n",
      "GRPO Iteration 2\n",
      "advantages : tensor([ 1.0062, -0.0124, -0.9937])\n",
      "Epoch 3/3 - Loss: -0.0924\n"
     ]
    }
   ],
   "source": [
    "def short_reward_func(prompt, outputs, **kwargs):\n",
    "    \"\"\"Reward function that gives higher scores to longer completions.\"\"\"\n",
    "    print(type(outputs))\n",
    "    return [float(len(completion)) for completion in outputs]\n",
    "\n",
    "dataset = [\"The sky is\", \"Yesterday, I went to\", \"I hope I will\", \"My car is\", \"I have to go to\", \"My friend want\"]\n",
    "\n",
    "\n",
    "config = GRPOConfiguration(num_generations=3, num_iterations=4, print_outputs=True, print_advantages=True, reward_func=short_reward_func)\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(3, dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MVA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
