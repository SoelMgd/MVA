{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80517dbc",
      "metadata": {
        "id": "80517dbc"
      },
      "source": [
        "# GRPO Training project: teach an LLM to do additions, again"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DAEbh3jBadc7",
      "metadata": {
        "id": "DAEbh3jBadc7"
      },
      "source": [
        "In this notebook, you'll find:\n",
        "* A basic Transformer with basic tokenizer\n",
        "* A basic dataset for additions\n",
        "* A classical pre-trainer, minimizing cross entropy loss\n",
        "* A Vanilla GRPO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MB8lj855LgHl",
      "metadata": {
        "id": "MB8lj855LgHl"
      },
      "source": [
        "You're not supposed to edit the existing code (you can if you want to...).\n",
        "You should implement one (or more) of the following:\n",
        "* GRPO with PPO (the `usual` one)\n",
        "* RLOO\n",
        "* ReMax\n",
        "* DPO\n",
        "* RAFT\n",
        "* your own RLHF method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ae993bb9",
      "metadata": {
        "id": "ae993bb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "OzGh9ahKF17h",
      "metadata": {
        "id": "OzGh9ahKF17h"
      },
      "outputs": [],
      "source": [
        "num_digits = 3\n",
        "\n",
        "dataset_size = 64_000\n",
        "train_proportion = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fabd151a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fabd151a",
        "outputId": "e7b36965-8f38-4c49-cf03-0109ab723eca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c054bed",
      "metadata": {
        "id": "6c054bed"
      },
      "source": [
        "## Step 1: Construct a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "t6aC9uNeIR6C",
      "metadata": {
        "id": "t6aC9uNeIR6C"
      },
      "outputs": [],
      "source": [
        "pad_token=\"[PAD]\"\n",
        "eos_token=\"[EOS]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "g2QiF-otFur3",
      "metadata": {
        "id": "g2QiF-otFur3"
      },
      "outputs": [],
      "source": [
        "class character_level_tokenizer:\n",
        "    \"\"\"\n",
        "    character-level\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
        "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
        "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
        "        self.ntokens = len(self.vocab)\n",
        "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
        "\n",
        "    def clean(self, text):\n",
        "        \"\"\"\n",
        "        removes all characters not in the vocabulary\n",
        "        \"\"\"\n",
        "        out = re.sub(self.pattern, \"\", text)\n",
        "        return out\n",
        "\n",
        "    def pre_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        character-level\n",
        "        \"\"\"\n",
        "        return [c for c in text]\n",
        "\n",
        "    def encode(self, text):\n",
        "        text_list = self.pre_tokenization(self.clean(text))\n",
        "        return [self.token_to_id[c] for c in text_list]\n",
        "\n",
        "    def decode(self, token_list):\n",
        "        return \"\".join([self.id_to_token[x] for x in token_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "QuCc6jF5F8hK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuCc6jF5F8hK",
        "outputId": "e42dddaa-dacd-471e-b7d0-48f17004dd87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = character_level_tokenizer()\n",
        "ntokens = tokenizer.ntokens\n",
        "ntokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8FXW2K-1Jd-P",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXW2K-1Jd-P",
        "outputId": "fff6c129-8820-4b3c-8caa-b43710370098"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1, 2, 10, 4, 2, 11], '12+42=')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"12 + 42 =\"\n",
        "inputs = tokenizer.encode(prompt)\n",
        "inputs, tokenizer.decode(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "491af297",
      "metadata": {
        "id": "491af297"
      },
      "source": [
        "## Step 2: Create a dataset for arithmetic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "daa90f31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa90f31",
        "outputId": "c23f0b74-6948-43a9-9e6f-5846f4b325b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('629+685=', '1314')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sample_datapoint(num_digits = 3):\n",
        "    a_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    b_list = [random.randint(0, 9) for _ in range(num_digits)]\n",
        "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
        "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
        "    a_str = \"\".join([str(x) for x in a_list])\n",
        "    b_str = \"\".join([str(x) for x in b_list])\n",
        "    sum_int = a_int + b_int\n",
        "    return (a_str + \"+\" + b_str + \"=\", str(sum_int))\n",
        "\n",
        "sample_datapoint(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b6e861d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e861d2",
        "outputId": "7b639165-7809-4f2d-a067-f21547c293b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('473+618=', '1091'),\n",
              " ('965+073=', '1038'),\n",
              " ('606+123=', '729'),\n",
              " ('944+297=', '1241')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = []\n",
        "for _ in range(dataset_size):\n",
        "    data.append(sample_datapoint(num_digits))\n",
        "data[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fee85050",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fee85050",
        "outputId": "69def39c-084c-4074-f296-908925038748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(57600, 6400)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train = data[: int(train_proportion * dataset_size)]\n",
        "data_test = data[int(train_proportion * dataset_size):]\n",
        "\n",
        "len(data_train),len(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37200598",
      "metadata": {
        "id": "37200598"
      },
      "source": [
        "## Step 3: Construct a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "91674239",
      "metadata": {
        "id": "91674239"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eb278ab",
      "metadata": {
        "id": "4eb278ab"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp,\n",
        "                                               nhead=nhead,\n",
        "                                               dim_feedforward=nhid,\n",
        "                                               num_encoder_layers=nlayers)\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.decoder = nn.Linear(ninp, ntoken) # projection sur le vocabulaire\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        return torch.log(torch.tril(torch.ones(sz,sz))) # masque de l'attention\n",
        "\n",
        "    def forward(self, src):\n",
        "        # S = sequence lenght = len(src), B = batch size, E = embedding dimension = ninp, V = vocab size = ntoken\n",
        "        # src: (S, B)\n",
        "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "        self.src_mask = mask # (S, S)\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp) # (S, B, E)\n",
        "        src = self.pos_encoder(src)  # (S, B, E)\n",
        "        output_enc = self.encoder(src, mask=self.src_mask) # (S, B, E) couches MultiHeadSelfAttention() pytorch\n",
        "        output_dec = self.decoder(output_enc) # (S, B, V)\n",
        "        return F.log_softmax(output_dec, dim=-1), output_enc # (S, B, V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1d568cc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d568cc4",
        "outputId": "7677a7ea-d06a-46a8-85f3-44274cc008a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-7): 8 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
              "  (input_emb): Embedding(14, 128)\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TransformerModel(ntoken = ntokens,\n",
        "                         ninp = 128,\n",
        "                         nhead = 16,\n",
        "                         nhid = 64,\n",
        "                         nlayers = 8)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a6PmJSo95N4C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6PmJSo95N4C",
        "outputId": "f0177342-43b6-46c6-b079-aab40db326d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 668942\n"
          ]
        }
      ],
      "source": [
        "print(\"number of parameters: {}\".format(sum([x.numel() for x in model.parameters()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e35d113",
      "metadata": {
        "id": "2e35d113"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8f2f06e0",
      "metadata": {
        "id": "8f2f06e0"
      },
      "outputs": [],
      "source": [
        "def generate(model, prompts, new_tokens = 5, mode = \"greedy\", num_samples = 1, temperature = 0.8):\n",
        "    input_tensor = torch.repeat_interleave(prompts, repeats = num_samples, dim = 1).to(device) # (S, B*G) G= num generations per prompt\n",
        "    # (prompt_length, batch_size * num_samples)\n",
        "    for _ in range(new_tokens):\n",
        "        output, _ = model(input_tensor) # (prompt_length, batch_size * num_samples, ntokens)\n",
        "        logits = output[-1,:,:] # dernier token (batch_size * num_samples, ntokens)\n",
        "        if mode == \"greedy\":\n",
        "            tokens = torch.argmax(logits, -1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        else: # mode == \"sampling\"\n",
        "            logits /= temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            tokens = torch.multinomial(probs, num_samples = 1).view((1,-1)) # (1, batch_size * num_samples)\n",
        "        input_tensor = torch.cat((input_tensor, tokens), 0)\n",
        "    return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d76d1b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d76d1b19",
        "outputId": "7c084e50-8269-4f24-859a-4f95f8bd143d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 2, 10,  3, 11,  6,  7,  6,  7,  7]]), '2+3=67677')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"2+3=\"\n",
        "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "output = generate(model, prompt_tensor).view((1,-1))\n",
        "output, tokenizer.decode(output[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "00954ddc",
      "metadata": {
        "id": "00954ddc"
      },
      "outputs": [],
      "source": [
        "def pad(token_list, type_list = \"prompts\"):\n",
        "    max_length = max([len(x) for x in token_list]) # taille commune des sequences\n",
        "    out = []\n",
        "    for x in token_list:\n",
        "        if type_list == \"prompts\":\n",
        "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x) # padding à gauche\n",
        "        if type_list == \"answers\":\n",
        "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x))) # padding à droite\n",
        "    return out, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2c84beab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c84beab",
        "outputId": "3e53a680-31e1-4cd6-8ce0-d96009def972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
        "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
        "padded_prompts, _ = pad(prompts, \"prompts\")\n",
        "padded_answers, _ = pad(answers, \"answers\")\n",
        "padded_prompts, padded_answers\n",
        "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "264f9227",
      "metadata": {
        "id": "264f9227"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, i, batch_size):\n",
        "    data = data_train if split == 'train' else data_test\n",
        "\n",
        "    prompts = [data[i][0] for i in range(i, i + batch_size)]\n",
        "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
        "    padded_prompts, prompt_length = pad(encoded_prompts, \"prompts\")\n",
        "\n",
        "    answers = [data[i][1] for i in range(i, i + batch_size)]\n",
        "    encoded_answers = [tokenizer.encode(answer) for answer in answers]\n",
        "    padded_answers, answers_length = pad(encoded_answers, \"answers\")\n",
        "\n",
        "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
        "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
        "    return X, Y, prompt_length, answers_length, prompts, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "91e281ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e281ad",
        "outputId": "2a97779d-08e9-485a-9588-07e699dc6b77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 16]), torch.Size([5, 16]), 8, 4, '037+338=', '375')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, Y, prompt_length, answers_length, prompts, answers = get_batch(\"train\", 43, 16)\n",
        "X.shape, Y.shape, prompt_length, answers_length, prompts[0], answers[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113e1fd1",
      "metadata": {
        "id": "113e1fd1"
      },
      "source": [
        "## Step 4: Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "KfmcSdPwp3K6",
      "metadata": {
        "id": "KfmcSdPwp3K6"
      },
      "outputs": [],
      "source": [
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1cfcd10a",
      "metadata": {
        "id": "1cfcd10a"
      },
      "outputs": [],
      "source": [
        "def evaluate(batch_size = batch_size):\n",
        "    # Turn on evaluation mode disables dropout.\n",
        "    model.eval()\n",
        "    correct = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"test\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size) (P, B)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size) (A, B)\n",
        "            output = generate(model, prompts, answers_length + 1) # (prompt_length + answers_length + 1, batch_size) (P+A, B)\n",
        "            answers_tokens = output[prompt_length:, :] # (answers_length + 1, batch_size), contains tokens (A, B)\n",
        "            equality_test = answers_tokens == target_answers # (answers_length + 1, batch_size), contains boolean values\n",
        "            correct += torch.all(equality_test, axis=0).float().sum()\n",
        "        accuracy = correct / len(data_test)\n",
        "    return accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ac335b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac335b05",
        "outputId": "1355d497-45f0-440a-90b8-30a7f2818091"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c54061a",
      "metadata": {
        "id": "4c54061a"
      },
      "source": [
        "## Step 5: Train the model, classical approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b827e567",
      "metadata": {
        "id": "b827e567"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "5b140ba3",
      "metadata": {
        "id": "5b140ba3"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 16\n",
        "learning_rate = 8e-4\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3638a75d",
      "metadata": {
        "id": "3638a75d"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss = 0.\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "            prompts, target_answers, prompt_length, answers_length, _, _ = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size) (P, B)\n",
        "            target_answers = target_answers.to(device) # (answers_length + 1, batch_size) (A, B)\n",
        "            input_tensor = torch.cat((prompts, target_answers), 0) # (prompt_length + answers_length + 1, batch_size) (P+A, B)\n",
        "            model.zero_grad()\n",
        "            output, _ = model(input_tensor) # (prompt_length + answers_length + 1, batch_size, ntokens) # (P+A, B, V) probabilité sur le vocabulaire pour chaque token\n",
        "            output_answers = output[prompt_length-1:-1,:,:].reshape(-1, ntokens) # ((answers_length + 1) * batch_size, ntokens) # (A*B, V) prob sur le vocab de la réponse\n",
        "            target_answers = target_answers.view(-1) # (A*B)\n",
        "            loss = F.cross_entropy(output_answers, target_answers)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                cur_loss = total_loss / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
        "                                                                                                            elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4e2a8490",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e2a8490",
        "outputId": "492562cc-d243-4314-ab56-d17d41c070ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.36 | loss  0.09 | perplexity     1.09\n",
            "|  1200/ 3600 batches | ms/batch  2.41 | loss  0.07 | perplexity     1.08\n",
            "|  1800/ 3600 batches | ms/batch  2.33 | loss  0.07 | perplexity     1.07\n",
            "|  2400/ 3600 batches | ms/batch  2.33 | loss  0.07 | perplexity     1.07\n",
            "|  3000/ 3600 batches | ms/batch  2.36 | loss  0.07 | perplexity     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 152.85s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.39 | loss  0.07 | perplexity     1.07\n",
            "|  1200/ 3600 batches | ms/batch  2.28 | loss  0.07 | perplexity     1.07\n",
            "|  1800/ 3600 batches | ms/batch  2.30 | loss  0.06 | perplexity     1.07\n",
            "|  2400/ 3600 batches | ms/batch  2.70 | loss  0.06 | perplexity     1.06\n",
            "|  3000/ 3600 batches | ms/batch  2.75 | loss  0.06 | perplexity     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 160.96s | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.54 | loss  0.06 | perplexity     1.06\n",
            "|  1200/ 3600 batches | ms/batch  2.43 | loss  0.06 | perplexity     1.06\n",
            "|  1800/ 3600 batches | ms/batch  2.41 | loss  0.05 | perplexity     1.05\n",
            "|  2400/ 3600 batches | ms/batch  2.33 | loss  0.05 | perplexity     1.05\n",
            "|  3000/ 3600 batches | ms/batch  2.43 | loss  0.04 | perplexity     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 159.01s | test accuracy  0.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.44 | loss  0.04 | perplexity     1.05\n",
            "|  1200/ 3600 batches | ms/batch  2.53 | loss  0.04 | perplexity     1.04\n",
            "|  1800/ 3600 batches | ms/batch  2.44 | loss  0.04 | perplexity     1.04\n",
            "|  2400/ 3600 batches | ms/batch  2.35 | loss  0.03 | perplexity     1.03\n",
            "|  3000/ 3600 batches | ms/batch  2.38 | loss  0.05 | perplexity     1.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 155.90s | test accuracy  0.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "|   600/ 3600 batches | ms/batch  2.52 | loss  0.03 | perplexity     1.03\n",
            "|  1200/ 3600 batches | ms/batch  2.79 | loss  0.02 | perplexity     1.02\n",
            "|  1800/ 3600 batches | ms/batch  2.55 | loss  0.01 | perplexity     1.01\n",
            "|  2400/ 3600 batches | ms/batch  2.90 | loss  0.01 | perplexity     1.01\n",
            "|  3000/ 3600 batches | ms/batch  2.57 | loss  0.00 | perplexity     1.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 169.21s | test accuracy  0.85\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "56d9d440",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56d9d440",
        "outputId": "b876390d-4396-4a95-e600-e2230317dbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "288+799=1087[EOS]\t actual result: 1087\n",
            "714+051=765[EOS]\t actual result: 765\n",
            "417+294=711[EOS]\t actual result: 711\n",
            "869+641=1510[EOS]\t actual result: 1510\n",
            "925+275=1100[EOS]\t actual result: 1200\n",
            "532+127=659[EOS]\t actual result: 659\n",
            "728+077=705[EOS]\t actual result: 805\n",
            "195+420=615[EOS]\t actual result: 615\n",
            "556+200=756[EOS]\t actual result: 756\n",
            "830+397=1227[EOS]\t actual result: 1227\n",
            "322+759=1081[EOS]\t actual result: 1081\n",
            "163+111=274[EOS]\t actual result: 274\n",
            "558+635=1193[EOS]\t actual result: 1193\n",
            "315+416=721[EOS]\t actual result: 731\n",
            "442+639=1081[EOS]\t actual result: 1081\n",
            "088+161=249[EOS]\t actual result: 249\n",
            "445+142=587[EOS]\t actual result: 587\n",
            "771+557=1328[EOS]\t actual result: 1328\n",
            "033+828=761[EOS]\t actual result: 861\n",
            "179+053=222[EOS]\t actual result: 232\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa4c591",
      "metadata": {
        "id": "cfa4c591"
      },
      "source": [
        "## Step 4 bis: Vanilla GRPO training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aff83f72",
      "metadata": {
        "id": "aff83f72"
      },
      "source": [
        "### Custom reward functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3c548bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c548bf7",
        "outputId": "b5d45345-22eb-445a-b888-6ad22e96d7b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.0)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def accuracy_reward(output, answer):\n",
        "    # 1 si output=answer\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return 1. if output == answer else 0.\n",
        "\n",
        "accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), accuracy_reward(\"123\", \"124\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e1f02762",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f02762",
        "outputId": "7271bdf4-4ecf-44ee-ec86-8c815ffaa9a5",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0, 0.008064516129032258)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def distance_accuracy_reward(output, answer):\n",
        "    # ecart relatif entre output et answer\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    int_output = int(output)\n",
        "    int_answer = int(answer)\n",
        "    return abs(int_output - int_answer) / max(int_output, int_answer)\n",
        "\n",
        "distance_accuracy_reward(\"123[EOS]\", \"123\"), distance_accuracy_reward(\"123[PAD]\", \"124\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b42a0d70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42a0d70",
        "outputId": "a79ca9d6-a79b-4640-c172-4db78e433256"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def digit_accuracy_reward(output, answer):\n",
        "    # ecart relatif entre les digits\n",
        "    pattern = r\"\\[EOS\\]\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    pattern = r\"(\\[PAD\\])*$\"\n",
        "    output = re.sub(pattern, \"\", output)\n",
        "    return sum(c1 == c2 for (c1,c2) in zip(output, answer)) / max(len(output), len(answer))\n",
        "\n",
        "digit_accuracy_reward(\"123[EOS][PAD][PAD]\", \"123\"), digit_accuracy_reward(\"123[EOS]\", \"123\"),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a41603b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a41603b2",
        "outputId": "33dbf431-4177-4ae3-d36c-08ff8a2261fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 1.0, 0.0)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def reward_format(output):\n",
        "    # 1 si format avec EOS\n",
        "    pattern = r\"\\d+\\[EOS\\](\\[PAD\\])*$\"\n",
        "    return 1. if bool(re.match(pattern, output)) else 0.\n",
        "\n",
        "reward_format(\"123[EOS][PAD][PAD]\"), reward_format(\"123[EOS]\"), reward_format(\"123\"),"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4482e411",
      "metadata": {
        "id": "4482e411"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf764cb0",
      "metadata": {
        "id": "cf764cb0"
      },
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batch_size = 16 # B\n",
        "learning_rate = 1e-4\n",
        "num_samples = 16 # G\n",
        "temperature = .8\n",
        "\n",
        "reporting_per_epoch = 5\n",
        "log_interval = len(data_train) // (reporting_per_epoch + 1)\n",
        "assert(log_interval % batch_size == 0)\n",
        "\n",
        "reward_fun = digit_accuracy_reward\n",
        "reward_format = reward_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15cdced",
      "metadata": {
        "id": "f15cdced"
      },
      "outputs": [],
      "source": [
        "def compute_rewards(text_outputs, answers):\n",
        "    # reward pondérée\n",
        "    repeated_answers = [answer for answer in answers for _ in range(num_samples)]\n",
        "    rewards = torch.tensor(\n",
        "        [0.2 * reward_format(output) + 0.8 * reward_fun(output, answer)\n",
        "         for output, answer in zip(text_outputs, repeated_answers)],\n",
        "        dtype=torch.float32,\n",
        "        device=device\n",
        "    )\n",
        "    return rewards # (B*G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "0c505deb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1', '1', '1', '22', '22', '22', '3', '3', '3']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers = [\"1\", \"22\", \"3\"]\n",
        "[answer for answer in answers for _ in range(3)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e22be0d4",
      "metadata": {
        "id": "e22be0d4"
      },
      "outputs": [],
      "source": [
        "def calculate_grpo_advantages(rewards):\n",
        "    # reshape rewards to group by prompt\n",
        "    # compute mean and standard deviation for each prompt group\n",
        "    # rewards (B*G,)\n",
        "    mean_rewards = rewards.view(-1, num_samples).mean(dim=1) # (B,)\n",
        "    std_rewards = rewards.view(-1, num_samples).std(dim=1) # (B,)\n",
        "    # expand the means and stds to match the original flat rewards tensor shape\n",
        "    mean_rewards = mean_rewards.repeat_interleave(num_samples, dim=0) # (B*G,)\n",
        "    std_rewards = std_rewards.repeat_interleave(num_samples, dim=0) # (B*G,)\n",
        "    # normalize rewards to get advantages\n",
        "    advantages = (rewards - mean_rewards) / (std_rewards + 1e-5) # (B*G,)\n",
        "    return advantages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faac5c99",
      "metadata": {
        "id": "faac5c99"
      },
      "outputs": [],
      "source": [
        "def compute_log_probs(model, outputs, prompt_length):\n",
        "    \"\"\" \n",
        "    Calcule les log-probabilités (sur tout le vocabulaire) des tokens des réponses.\n",
        "    \"\"\"\n",
        "    logits, _ = model(outputs) #(P+A, B*G, V)\n",
        "    # logits.shape = (prompt_length + answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # we only need the log probabilities for the new tokens\n",
        "    # this introduces a shift: the logits for a position are the predictions for the next token\n",
        "    logits = logits[prompt_length-1:-1, :, :] #(A, B*G, V)\n",
        "    # logits.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "\n",
        "    # convert raw logits into log probabilities along the vocabulary axis\n",
        "    log_probs = F.log_softmax(logits, dim=-1) #(A, B*G, V)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size)\n",
        "    return log_probs #(A, B*G, V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2612214",
      "metadata": {
        "id": "b2612214"
      },
      "outputs": [],
      "source": [
        "def compute_loss(advantages, log_probs, responses):\n",
        "    \"\"\" \n",
        "    Calcule la loss d'un batch\n",
        "    \"\"\"\n",
        "    # reshape responses from (answers_length + 1, batch_size * num_samples)\n",
        "    # to (answers_length + 1, batch_size * num_samples, 1) for gathering\n",
        "    responses = responses.unsqueeze(-1) #(A, B*G, 1)\n",
        "    # log_probs.shape = (answers_length + 1, batch_size * num_samples, vocab_size) (A, B*G, V)\n",
        "    # responses.shape = (answers_length + 1, batch_size * num_samples) \n",
        "    \n",
        "    # gather the log probability for each token in responses\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses)\n",
        "    # remove the extra last dimension to get back to shape (answers_length + 1, batch_size * num_samples).\n",
        "    selected_log_probs = selected_log_probs.squeeze(-1) # (A, B*G)\n",
        "\n",
        "    # normalize\n",
        "    selected_log_probs = (selected_log_probs - selected_log_probs.mean(-1, keepdim=True)) / (selected_log_probs.std(-1, keepdim=True) + 1e-5) # (A, B*G)\n",
        "\n",
        "    # advantages.shape = (batch_size * num_samples) (B*G)\n",
        "    # we use the same advantages for all tokens in the response\n",
        "    loss = -(advantages.unsqueeze(dim=0) * selected_log_probs).mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ccea9fd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "### MON CODE\n",
        "\n",
        "def compute_loss_DeepSeek(advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref):\n",
        "    \"\"\" \n",
        "    Calcule la loss GRPO (DeepSeek) d'un batch, en utilisant les politiques du modèle, de model_ref et model_old.\n",
        "    Args:\n",
        "        advantages: (B*G) avantage de chaque séquence générée\n",
        "        responses: (A, B*G) token générés par le modèle old pour chaque séquence générée\n",
        "        answers_length: int longueur des réponses générées (complétée par du padding)\n",
        "        log_probs: (A, B*G, V) log-probabilités des tokens du vocabulaire, pour chaque étape de la génération par le modèle (mis à jour plusieurs fois par batch)\n",
        "        log_probs_old: (A, B*G, V) même chose pour le modèle old (mis à jour une fois par batch)\n",
        "        log_probs_ref: (A, B*G, V) même chose pour le modèle de réference (mis à jour une fois par époque)\n",
        "\n",
        "    \"\"\"\n",
        "    A, B, G = answers_length+1, batch_size, num_samples # dimensions\n",
        "    eps = 0.2 # clipping coefficient\n",
        "    beta = 0.4 # KL coefficient\n",
        "\n",
        "    responses = responses.unsqueeze(-1) # (A, B*G) -> (A, B*G, 1) pour gathering\n",
        "\n",
        "    # log_probs dimensions: (A, B*G, V)\n",
        "\n",
        "    # Récupère et reformatte les log-probs des tokens générés par le modèle\n",
        "    selected_log_probs = log_probs.gather(dim=-1, index=responses) # selectionne uniquement les probabilités des tokens générés\n",
        "    selected_log_probs = selected_log_probs.squeeze(-1) # (A, B*G)\n",
        "    selected_log_probs = selected_log_probs.view(A, B, G).permute(1, 2, 0) # reformatte en (B, G, A) pour le calcul de la loss\n",
        "\n",
        "    # Récupère et reformatte les log-probs des tokens générés par le model_old\n",
        "    selected_log_probs_old = log_probs_old.gather(dim=-1, index=responses)\n",
        "    selected_log_probs_old = selected_log_probs_old.squeeze(-1) # (A, B*G)\n",
        "    selected_log_probs_old = selected_log_probs_old.view(A, B, G).permute(1, 2, 0) # (A, B*G) -> (B, G, A) pour uniformiser les log_probs\n",
        "\n",
        "    # Récupère et reformatte les log-probs des tokens générés par le model_ref\n",
        "    selected_log_probs_ref = log_probs_ref.gather(dim=-1, index=responses)\n",
        "    selected_log_probs_ref = selected_log_probs_ref.squeeze(-1) # (A, B*G)\n",
        "    selected_log_probs_ref = selected_log_probs_ref.view(A, B, G).permute(1, 2, 0) # reformatte en (B, G, A) pour le calcul de la loss\n",
        "\n",
        "    # Expension des avantages pour avoir le même format que les politiques\n",
        "    advantages = advantages.view(B, G) # (B*G)  -> (B, G)\n",
        "    advantages = advantages.unsqueeze(-1).expand(-1, -1, A) # (B, G) -> (B, G, A) on étend les avantages sur les token pour les mutliplier avec les ratios\n",
        "\n",
        "    # Calcul des ratios de politiques entre model et model_old sur les token générés\n",
        "    ratios_old = torch.exp(selected_log_probs - selected_log_probs_old) # (B, G, A) rapport des probabilités sur les tokens générés entre model et model_old\n",
        "    clipped_ratios = torch.clamp(ratios_old, 1 - eps, 1 + eps) # (B, G, A) clipped ratio\n",
        "\n",
        "    coeff_1 = ratios_old * advantages # (B, G, A)\n",
        "    coeff_2 = clipped_ratios * advantages # (B, G, A)\n",
        "\n",
        "    # Calcul de la KL divergence entre la policy du modèle et celle de model_ref\n",
        "    KL = torch.exp(selected_log_probs_ref - selected_log_probs) - (selected_log_probs_ref - selected_log_probs) -1 # (B, G, A) Estimateur différentiable de la KL\n",
        "    \n",
        "    loss_tensor = torch.minimum(coeff_1, coeff_2) - beta*KL # (B, G, A)\n",
        "    loss = - loss_tensor.mean(dim=-1).mean(dim=-1).sum() #moyenne selon A puis G puis on fait la somme des moyennes sur G\n",
        "\n",
        "    return loss\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "3c40a38a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3599 batches to go!\n",
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Batch 0 loss : -0.511246919631958\n",
            "Batch 1 loss : -0.23518361151218414\n",
            "Batch 2 loss : -0.16449134051799774\n",
            "Batch 3 loss : 0.031492091715335846\n",
            "Batch 4 loss : -0.04103913903236389\n",
            "Batch 5 loss : 0.23252439498901367\n",
            "Batch 6 loss : 0.22064754366874695\n",
            "Batch 7 loss : 0.15894721448421478\n",
            "Batch 8 loss : 0.08772365003824234\n",
            "Batch 9 loss : 0.14583761990070343\n",
            "Batch 10 loss : 0.3208909034729004\n",
            "Batch 11 loss : 0.3034358024597168\n",
            "Batch 12 loss : 0.23609769344329834\n",
            "Batch 13 loss : 0.27937084436416626\n",
            "Batch 14 loss : 0.17302371561527252\n",
            "Batch 15 loss : 0.30169597268104553\n",
            "Batch 16 loss : 0.1210220530629158\n",
            "Batch 17 loss : 0.31678953766822815\n",
            "Batch 18 loss : 0.38117867708206177\n",
            "Batch 19 loss : 0.2225230634212494\n",
            "Batch 20 loss : 0.3279609680175781\n",
            "Batch 21 loss : 0.4489544928073883\n",
            "Batch 22 loss : 0.27135026454925537\n",
            "Batch 23 loss : 0.2816661596298218\n",
            "Batch 24 loss : 0.2669328451156616\n",
            "Batch 25 loss : 0.3233377933502197\n",
            "Batch 26 loss : 0.254479318857193\n",
            "Batch 27 loss : 0.29625755548477173\n",
            "Batch 28 loss : 0.31376755237579346\n",
            "Batch 29 loss : 0.517318844795227\n",
            "Batch 30 loss : 0.40488678216934204\n",
            "Batch 31 loss : 0.3199295699596405\n",
            "Batch 32 loss : 0.19705253839492798\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[104], line 190\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m89\u001b[39m)\n\u001b[0;32m    189\u001b[0m Trainer \u001b[38;5;241m=\u001b[39m  GRPOTrainer(model)\n\u001b[1;32m--> 190\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[104], line 173\u001b[0m, in \u001b[0;36mGRPOTrainer.train\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    170\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (prompt_length, batch_size)\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# perform train_step\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswers_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[104], line 142\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[1;34m(self, prompts, prompt_length, answers_length, questions, answers)\u001b[0m\n\u001b[0;32m    137\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_deepseek(advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m#print(f\"Loss iteration {i}, {loss}\")\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "class GRPOTrainer:\n",
        "    def __init__(self, model, ):\n",
        "\n",
        "        self.model = model\n",
        "        self.ref_model = None # reference model\n",
        "        self.learning_rate = 1e-4\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.epochs = 20\n",
        "        self.batch_size = 16 # B\n",
        "        self.num_samples = 5 # G\n",
        "        self.temperature = .8\n",
        "        self.num_iterations=3\n",
        "\n",
        "    def calculate_grpo_advantages(self, rewards):\n",
        "        # rewards (B*G,)\n",
        "        mean_rewards = rewards.view(-1, self.num_samples).mean(dim=1) # (B,)\n",
        "        std_rewards = rewards.view(-1, self.num_samples).std(dim=1) # (B,)\n",
        "        # expand the means and stds to match the original flat rewards tensor shape\n",
        "        mean_rewards = mean_rewards.repeat_interleave(self.num_samples, dim=0) # (B*G,)\n",
        "        std_rewards = std_rewards.repeat_interleave(self.num_samples, dim=0) # (B*G,)\n",
        "        # normalize rewards to get advantages\n",
        "        advantages = (rewards - mean_rewards) / (std_rewards + 1e-5) # (B*G,)\n",
        "        return advantages\n",
        "    \n",
        "    def compute_log_probs(self, model, outputs, prompt_length):\n",
        "        \"\"\" \n",
        "        Calcule les log-probabilités (sur tout le vocabulaire) des tokens des réponses.\n",
        "        \"\"\"\n",
        "        logits, _ = model(outputs) #(P+A, B*G, V)\n",
        "\n",
        "        # we only need the log probabilities for the new tokens\n",
        "        # this introduces a shift: the logits for a position are the predictions for the next token\n",
        "        logits = logits[prompt_length-1:-1, :, :] #(A, B*G, V)\n",
        "\n",
        "        # convert raw logits into log probabilities along the vocabulary axis\n",
        "        log_probs = F.log_softmax(logits, dim=-1) #(A, B*G, V)\n",
        "        return log_probs #(A, B*G, V)\n",
        "\n",
        "    def compute_loss_deepseek(self, advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref):\n",
        "        \"\"\" \n",
        "        Calcule la loss GRPO (DeepSeek) d'un batch, en utilisant les politiques du modèle, de model_ref et model_old.\n",
        "        Args:\n",
        "            advantages: (B*G) avantage de chaque séquence générée\n",
        "            responses: (A, B*G) token générés par le modèle old pour chaque séquence générée\n",
        "            answers_length: int longueur des réponses générées (complétée par du padding)\n",
        "            log_probs: (A, B*G, V) log-probabilités des tokens du vocabulaire, pour chaque étape de la génération par le modèle (mis à jour plusieurs fois par batch)\n",
        "            log_probs_old: (A, B*G, V) même chose pour le modèle old (mis à jour une fois par batch)\n",
        "            log_probs_ref: (A, B*G, V) même chose pour le modèle de réference (mis à jour une fois par époque)\n",
        "\n",
        "        \"\"\"\n",
        "        A, B, G = answers_length+1, self.batch_size, self.num_samples # dimensions\n",
        "        eps = 0.2 # clipping coefficient\n",
        "        beta = 0.4 # KL coefficient\n",
        "\n",
        "        responses = responses.unsqueeze(-1) # (A, B*G) -> (A, B*G, 1) pour gathering\n",
        "\n",
        "        # log_probs dimensions: (A, B*G, V)\n",
        "\n",
        "        # Récupère et reformatte les log-probs des tokens générés par le modèle\n",
        "        selected_log_probs = log_probs.gather(dim=-1, index=responses) # selectionne uniquement les probabilités des tokens générés\n",
        "        selected_log_probs = selected_log_probs.squeeze(-1) # (A, B*G)\n",
        "        selected_log_probs = selected_log_probs.view(A, B, G).permute(1, 2, 0) # reformatte en (B, G, A) pour le calcul de la loss\n",
        "\n",
        "        # Récupère et reformatte les log-probs des tokens générés par le model_old\n",
        "        selected_log_probs_old = log_probs_old.gather(dim=-1, index=responses)\n",
        "        selected_log_probs_old = selected_log_probs_old.squeeze(-1) # (A, B*G)\n",
        "        selected_log_probs_old = selected_log_probs_old.view(A, B, G).permute(1, 2, 0) # (A, B*G) -> (B, G, A) pour uniformiser les log_probs\n",
        "\n",
        "        # Récupère et reformatte les log-probs des tokens générés par le model_ref\n",
        "        selected_log_probs_ref = log_probs_ref.gather(dim=-1, index=responses)\n",
        "        selected_log_probs_ref = selected_log_probs_ref.squeeze(-1) # (A, B*G)\n",
        "        selected_log_probs_ref = selected_log_probs_ref.view(A, B, G).permute(1, 2, 0) # reformatte en (B, G, A) pour le calcul de la loss\n",
        "\n",
        "        # Expension des avantages pour avoir le même format que les politiques\n",
        "        advantages = advantages.view(B, G) # (B*G)  -> (B, G)\n",
        "        advantages = advantages.unsqueeze(-1).expand(-1, -1, A) # (B, G) -> (B, G, A) on étend les avantages sur les token pour les mutliplier avec les ratios\n",
        "\n",
        "        # Calcul des ratios de politiques entre model et model_old sur les token générés\n",
        "        ratios_old = torch.exp(selected_log_probs - selected_log_probs_old) # (B, G, A) rapport des probabilités sur les tokens générés entre model et model_old\n",
        "        clipped_ratios = torch.clamp(ratios_old, 1 - eps, 1 + eps) # (B, G, A) clipped ratio\n",
        "\n",
        "        coeff_1 = ratios_old * advantages # (B, G, A)\n",
        "        coeff_2 = clipped_ratios * advantages # (B, G, A)\n",
        "\n",
        "        # Calcul de la KL divergence entre la policy du modèle et celle de model_ref\n",
        "        KL = torch.exp(selected_log_probs_ref - selected_log_probs) - (selected_log_probs_ref - selected_log_probs) -1 # (B, G, A) Estimateur différentiable de la KL\n",
        "        \n",
        "        loss_tensor = torch.minimum(coeff_1, coeff_2) - beta*KL # (B, G, A)\n",
        "        loss = - loss_tensor.mean(dim=-1).mean(dim=-1).sum() #moyenne selon A puis G puis on fait la somme des moyennes sur G\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, prompts, prompt_length, answers_length, questions, answers):\n",
        "        \"\"\"\n",
        "            Do a traing step with multiple iterations on a specific batch.\n",
        "            Args:\n",
        "                prompts: prompts to optimize the model on it\n",
        "        \"\"\"\n",
        "        \n",
        "        self.model.train()\n",
        "\n",
        "        # generate samples for each prompt in inference_mode as, it is just a sampling step\n",
        "        #with torch.inference_mode():\n",
        "        outputs = generate(self.model,\n",
        "                            prompts,\n",
        "                            new_tokens = answers_length + 1,\n",
        "                            mode = \"sampling\",\n",
        "                            num_samples = self.num_samples,\n",
        "                            temperature = temperature) # (P+A, B*G)\n",
        "        \n",
        "        responses = outputs[prompt_length:, :] # (A, B*G)\n",
        "\n",
        "        text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                        for i in range(outputs.size(1))]\n",
        "\n",
        "        # Calcul de log_probs_old log_probs_ref sans backpropagation pour la loss\n",
        "        self.model.eval()\n",
        "        self.model_ref.eval()\n",
        "        with torch.inference_mode():\n",
        "            log_probs_old = self.compute_log_probs(self.model, outputs, prompt_length) # (A, B*G, V)\n",
        "            log_probs_ref = self.compute_log_probs(self.model_ref, outputs, prompt_length) # (A, B*G, V)\n",
        "\n",
        "        # Calcul des avantages\n",
        "        rewards = compute_rewards(text_outputs, answers) # (B*G)\n",
        "        advantages = self.calculate_grpo_advantages(rewards) # (B*G)\n",
        "\n",
        "        for i in range(self.num_iterations): # num_optimizations steps of the model on the same batch\n",
        "            #print(f\"GRPO iteration {i}\")\n",
        "            self.optimizer.zero_grad()  \n",
        "\n",
        "            # Calcul de log_probs avec le modèle à jour\n",
        "            log_probs = self.compute_log_probs(self.model, outputs, prompt_length) # (A, B*G, V)\n",
        "                \n",
        "            # compute loss\n",
        "            loss = self.compute_loss_deepseek(advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref)\n",
        "\n",
        "            #print(f\"Loss iteration {i}, {loss}\")\n",
        "\n",
        "            # optimize\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "    \n",
        "    def train(self, verbose = False):\n",
        "\n",
        "        print(f\"{(len(data_train) - 1)//self.batch_size} batches to go!\")\n",
        "\n",
        "        best_test_accuracy = None\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "        print('-' * 89)\n",
        "\n",
        "        # switch eval for train model (enables dropout)\n",
        "        self.model_ref = copy.deepcopy(model)\n",
        "        epochs = self.epochs\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            epoch_start_time = time.time()\n",
        "            start_time = time.time()\n",
        "\n",
        "            total_loss=0\n",
        "            for batch, i in enumerate(range(0, len(data_train) - 1, self.batch_size)):\n",
        "\n",
        "                # get a batch of prompts and answers\n",
        "                prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, self.batch_size)\n",
        "                prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "\n",
        "                # perform train_step\n",
        "                loss = self.train_step(prompts, prompt_length, answers_length, questions, answers)\n",
        "                total_loss += loss\n",
        "                print(f\"Batch {batch} loss : {loss}\")\n",
        "            \n",
        "            # Update ref_model\n",
        "            self.model_ref = copy.deepcopy(model)\n",
        "\n",
        "            avg_loss = total_loss / len(data_train)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")            \n",
        "\n",
        "            test_accuracy = evaluate()\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "            print('-' * 89)\n",
        "\n",
        "\n",
        "Trainer =  GRPOTrainer(model)\n",
        "Trainer.train()\n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "ffffd489",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "Batch 0\n",
            "torch.Size([5, 40, 14]) torch.Size([5, 40, 14])\n",
            "GRPO iteration 0\n",
            " log_probs, log_probs_old, log_probs_ref torch.Size([5, 40, 14]) torch.Size([5, 40, 14]) torch.Size([5, 40, 14])\n",
            "A, B, G 5 16 16 torch.Size([5, 40])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[5, 16, 16]' is invalid for input of size 200",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[87], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Trainer \u001b[38;5;241m=\u001b[39m  GRPOTrainer(model)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[86], line 171\u001b[0m, in \u001b[0;36mGRPOTrainer.train\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    168\u001b[0m     prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (prompt_length, batch_size)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# perform train_step\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswers_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Update ref_model\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[86], line 137\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[1;34m(self, prompts, prompt_length, answers_length, questions, answers)\u001b[0m\n\u001b[0;32m    134\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_log_probs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, outputs, prompt_length) \u001b[38;5;66;03m# (A, B*G, V)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_deepseek\u001b[49m\u001b[43m(\u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswers_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "Cell \u001b[1;32mIn[86], line 66\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss_deepseek\u001b[1;34m(self, advantages, responses, answers_length, log_probs, log_probs_old, log_probs_ref)\u001b[0m\n\u001b[0;32m     64\u001b[0m selected_log_probs \u001b[38;5;241m=\u001b[39m selected_log_probs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (A, B*G)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA, B, G\u001b[39m\u001b[38;5;124m\"\u001b[39m, A, B, G, selected_log_probs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 66\u001b[0m selected_log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mselected_log_probs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# reformatte en (B, G, A) pour le calcul de la loss\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Récupère et reformatte les log-probs des tokens générés par le model_old\u001b[39;00m\n\u001b[0;32m     69\u001b[0m selected_log_probs_old \u001b[38;5;241m=\u001b[39m log_probs_old\u001b[38;5;241m.\u001b[39mgather(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mresponses)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[5, 16, 16]' is invalid for input of size 200"
          ]
        }
      ],
      "source": [
        "Trainer =  GRPOTrainer(model)\n",
        "Trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "c93b0adb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "GRPO iteration 0\n",
            "GRPO iteration 1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 14]], which is output 0 of AsStridedBackward0, is at version 36014; expected version 36012 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_DeepSeek_GRPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[69], line 58\u001b[0m, in \u001b[0;36mtrain_DeepSeek_GRPO\u001b[1;34m(verbose, num_iterations)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m batch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 14]], which is output 0 of AsStridedBackward0, is at version 36014; expected version 36012 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ],
      "source": [
        "train_DeepSeek_GRPO(verbose = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824ca075",
      "metadata": {
        "id": "824ca075"
      },
      "outputs": [],
      "source": [
        "def train_vanilla_GRPO(verbose = False):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    best_test_accuracy = None\n",
        "    test_accuracy = evaluate()\n",
        "    print('-' * 89)\n",
        "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
        "    print('-' * 89)\n",
        "\n",
        "    # switch eval for train model (enables dropout)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        start_time = time.time()\n",
        "        for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
        "\n",
        "            # get a batch of prompts and answers\n",
        "            prompts, _, prompt_length, answers_length, questions, answers = get_batch(\"train\", i, batch_size)\n",
        "            prompts = prompts.to(device) # (prompt_length, batch_size)\n",
        "\n",
        "            # generate samples for each prompt\n",
        "            outputs = generate(model,\n",
        "                               prompts,\n",
        "                               new_tokens = answers_length + 1,\n",
        "                               mode = \"sampling\",\n",
        "                               num_samples = num_samples,\n",
        "                               temperature = temperature) # (P+A, B*G)\n",
        "            # outputs.shape = (prompt_length + answers_length + 1, batch_size * num_samples)\n",
        "            text_outputs = [tokenizer.decode(outputs[prompt_length:, i].tolist())\n",
        "                            for i in range(outputs.size(1))]\n",
        "\n",
        "            # compute rewards\n",
        "            rewards = compute_rewards(text_outputs, answers) # (B*G)\n",
        "\n",
        "            # compute advantages\n",
        "            advantages = calculate_grpo_advantages(rewards) # (B*G)\n",
        "\n",
        "            # compute log probabilities pour tous les tokens des réponses\n",
        "            log_probs = compute_log_probs(model, outputs, prompt_length) # (A, B*G, V)\n",
        "\n",
        "            # compute loss\n",
        "            responses = outputs[prompt_length:, :] # (A, B*G)\n",
        "            loss = compute_loss(advantages, log_probs, responses)\n",
        "\n",
        "            # optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if i % log_interval == 0 and batch > 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| {:5d}/{:5d} batches | ms/batch {:5.2f}'.format(batch, len(data_train) // batch_size, elapsed))\n",
        "                if verbose:\n",
        "                    print(\"\\nquestion:\", questions[0],\n",
        "                      \"\\nanswer\", answers[0],\n",
        "                      \"\\noutput:\", text_outputs[:num_samples],\n",
        "                      \"\\nreward:\", rewards[:num_samples],\n",
        "                      \"\\nadvantage:\", advantages[:num_samples], \"\\n\")\n",
        "\n",
        "                start_time = time.time()\n",
        "        test_accuracy = evaluate()\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the test accuracy is the best we've seen so far.\n",
        "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
        "            with open(\"arithmetic_vanilla_GRPO.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_test_accuracy = test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "b02716ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "b02716ff",
        "outputId": "d3ae59c3-1d5f-4e40-e428-097a04b4edb1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| initialisation | test accuracy  0.85\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'compute_loss' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_vanilla_GRPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[47], line 44\u001b[0m, in \u001b[0;36mtrain_vanilla_GRPO\u001b[1;34m(verbose)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[0;32m     43\u001b[0m responses \u001b[38;5;241m=\u001b[39m outputs[prompt_length:, :] \u001b[38;5;66;03m# (A, B*G)\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m(advantages, log_probs, responses)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'compute_loss' is not defined"
          ]
        }
      ],
      "source": [
        "train_vanilla_GRPO(verbose = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "id": "aeVn935w5BSp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeVn935w5BSp",
        "outputId": "2014648b-68e6-4af6-ce9c-b28a3d580c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "128+705=833[EOS]\t actual result: 833\n",
            "953+527=1480[EOS]\t actual result: 1480\n",
            "262+889=1151[EOS]\t actual result: 1151\n",
            "222+843=1065[EOS]\t actual result: 1065\n",
            "777+632=1409[EOS]\t actual result: 1409\n",
            "719+662=1381[EOS]\t actual result: 1381\n",
            "789+402=1191[EOS]\t actual result: 1191\n",
            "587+869=1456[EOS]\t actual result: 1456\n",
            "386+632=1018[EOS]\t actual result: 1018\n",
            "038+837=875[EOS]\t actual result: 875\n",
            "200+272=472[EOS]\t actual result: 472\n",
            "680+684=1364[EOS]\t actual result: 1364\n",
            "594+534=1128[EOS]\t actual result: 1128\n",
            "350+593=943[EOS]\t actual result: 943\n",
            "867+755=1622[EOS]\t actual result: 1622\n",
            "978+223=1201[EOS]\t actual result: 1201\n",
            "571+512=1083[EOS]\t actual result: 1083\n",
            "148+465=613[EOS]\t actual result: 613\n",
            "873+373=1246[EOS]\t actual result: 1246\n",
            "700+356=1056[EOS]\t actual result: 1056\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(20):\n",
        "    prompt, answers = data_test[i]\n",
        "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
        "    output = generate(model, prompt_tensor, len(answers) + 1).view((1,-1))\n",
        "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MVA_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
