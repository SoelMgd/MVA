{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Groupe Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to fill in the `GRPOTrainer` class. You have two options (and you can do both):\n",
    "* the \"normal GRPO\" with clipped surrogate objective\n",
    "* or the \"vanilla GRPO\" with original objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"A train takes 3 hours to travel from A to B at an average speed of 60 km/h. How long would the trip take if the train traveled at 80 km/h?\",\n",
    "    \"A snail climbs a 10-meter wall. It climbs 3 meters during the day and slips 2 meters at night. How many days will it take to reach the top?\",\n",
    "    'If a liar says, \"I always lie,\" is he telling the truth?',\n",
    "    'Can we say that \"this sentence is false\"? Explain why.',\n",
    "    \"Paul is twice the age Pierre was when Paul was the age Pierre is today. If Pierre is 20 years old, how old is Paul?\",\n",
    "    \"A father and his son together are 36 years old. The father is exactly three times the son's age. How old is the son?\",\n",
    "    \"All the cats I have met so far were black. Can I conclude that all cats are black? Why?\",\n",
    "    \"If all humans are mortal and Socrates is human, what can we conclude?\",\n",
    "    \"If a shirt costs twice as much as a pair of pants and the pants cost 30€, how much does the shirt cost?\",\n",
    "    'Jean says: \"All my friends are football players.\" Pierre is Jean’s friend. Can we conclude that Pierre is a football player?',\n",
    "    \"You are in a train and must choose between switching the direction of the train to avoid five people tied to one track, but in doing so, you will kill one person on the other track. What do you do and why?\",\n",
    "    \"A doctor has five patients in need of organ transplants, and a perfectly healthy patient comes in for a routine check-up. Should the doctor sacrifice this patient to save the five others?\",\n",
    "    \"What would happen if gravity on Earth were twice as strong?\",\n",
    "    \"If humans could read minds, how would that change society?\"\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "for question in questions:\n",
    "    dataset.append(f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> <\\answer> tags, respectively, i.e., <think> reasoning process here <\\think> <answer> answer here <\\answer>. User: {question}. Assistant:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOConfiguration:\n",
    "    def __init__(self,\n",
    "                 model_name=\"gpt2\",\n",
    "                 learning_rate=1e-5,\n",
    "                 temperature=0.9,\n",
    "                 max_length=50000,\n",
    "                 device=\"cpu\",\n",
    "                 num_generations=5,\n",
    "                 num_iterations=1,\n",
    "                 beta=0.1,\n",
    "                 epsilon=1e-5):\n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.num_generations = num_generations\n",
    "        self.num_iterations = num_iterations\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 2\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 4\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n",
      "ref_per_token_logps.shape, per_token_logps.shape torch.Size([5, 200]) torch.Size([5, 200])\n",
      "advantages : tensor([0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 237\u001b[0m\n\u001b[0;32m    235\u001b[0m config \u001b[38;5;241m=\u001b[39m GRPOConfiguration()\n\u001b[0;32m    236\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(config)\n\u001b[1;32m--> 237\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[87], line 203\u001b[0m, in \u001b[0;36mGRPOTrainer.train\u001b[1;34m(self, num_epochs, dataset)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 203\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    206\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "Cell \u001b[1;32mIn[87], line 225\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Pour chaque GRPO iteration, on met à jour le modèle sur le même batch\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations): \n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# Calcul de la loss (utilise le modèle actuel qui se met à jour dans la boucle et on calcule la loss par rapport aux log_prob de old_model (dans inputs))\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[87], line 161\u001b[0m, in \u001b[0;36mGRPOTrainer.compute_loss\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m    158\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([prompt_mask, completion_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m logits_to_keep \u001b[38;5;241m=\u001b[39m completion_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# we only need to compute the logits for the completion tokens\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m per_token_logps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_per_token_logps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Compute the KL divergence between the model and the reference model\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[87], line 38\u001b[0m, in \u001b[0;36mGRPOTrainer._get_per_token_logps\u001b[1;34m(self, model, input_ids, attention_mask, logits_to_keep)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_per_token_logps\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, input_ids, attention_mask, logits_to_keep):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    Get the per-token log propabilities for the completions for the model and the reference model\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     39\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:] \u001b[38;5;66;03m#(B, L-1, V) exclude the last logit corresponding to next token prediction\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m-\u001b[39mlogits_to_keep:]\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    912\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m         output_attentions,\n\u001b[0;32m    920\u001b[0m     )\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:404\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    402\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    403\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 404\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    413\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:293\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m     query_states, key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    295\u001b[0m shape_q \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39mquery_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    296\u001b[0m shape_kv \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39mkey_states\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:123\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    122\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[1;32m--> 123\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inspired from HuggingFace\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, config: GRPOConfiguration):\n",
    "        self.device = config.device\n",
    "        self.model_name = config.model_name\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config.model_name).to(self.device) # model to optimize\n",
    "        self.ref_model = None # reference model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name, padding=True, padding_side=\"left\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  #gpt2 n'a pas de padding token\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate)\n",
    "        self.temperature = config.temperature\n",
    "        self.max_prompt_length = config.max_length\n",
    "        self.eps = config.epsilon\n",
    "        self.num_generations = config.num_generations # num of generation per prompts\n",
    "        self.num_iterations = config.num_generations\n",
    "        self.beta = config.beta\n",
    "\n",
    "    \n",
    "    def reward_func(self, prompt, completions, **kwargs):\n",
    "        \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "        \n",
    "        pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "\n",
    "        matches = [re.match(pattern, content) for content in completions]\n",
    "        return [1.0 if match else 0.0 for match in matches]\n",
    "        \n",
    "\n",
    "    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):\n",
    "        \"\"\"\n",
    "        Get the per-token log propabilities for the completions for the model and the reference model\n",
    "        \"\"\"\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        logits = logits[:,:-1,:] #(B, L-1, V) exclude the last logit corresponding to next token prediction\n",
    "\n",
    "        input_ids = input_ids[:, -logits_to_keep:]\n",
    "        logits = logits[:, -logits_to_keep:]\n",
    "        return torch.gather(logits.log_softmax(-1), dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) #log soft max to get probabilities\n",
    "    \n",
    "\n",
    "    def _generate_and_score_completions(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Génère plusieurs réponses pour un seul prompt et applique un masque après le premier EOS.\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Tokenisation du prompt\n",
    "        prompt_inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "\n",
    "        # Tronquer si nécessaire\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n",
    "\n",
    "        # Génération des réponses\n",
    "        with torch.inference_mode():\n",
    "            prompt_completion_ids = self.model.generate(\n",
    "                prompt_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                num_return_sequences=self.num_generations,\n",
    "                do_sample=True,\n",
    "                max_length=prompt_ids.size(1) + 200,  # Taille du prompt + max réponse\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        #print(\"(prompt_completion_ids.size(), prompt_ids.size(), prompt_mask.size()\", prompt_completion_ids.size(), prompt_ids.size(), prompt_mask.size())\n",
    "\n",
    "        # Séparation du prompt et de la complétion\n",
    "        prompt_length = prompt_ids.size(1)\n",
    "        prompt_ids = prompt_completion_ids[:, :prompt_length]  # (G, P)\n",
    "        completion_ids = prompt_completion_ids[:, prompt_length:]  # (G, C)\n",
    "        #print(\"prompt_ids.size(), completion_ids.size()\", prompt_ids.size(), completion_ids.size())\n",
    "\n",
    "        # Détection du premier EOS\n",
    "        is_eos = completion_ids == self.tokenizer.eos_token_id  # (G, C)\n",
    "        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)  # (G,)\n",
    "\n",
    "        has_eos = is_eos.any(dim=1)  # Vérifie si un EOS est présent (G,)\n",
    "        first_eos_idx = is_eos.int().argmax(dim=1)  # Index du premier EOS (G,)\n",
    "        eos_idx[has_eos] = first_eos_idx[has_eos]  # Appliquer seulement si EOS trouvé\n",
    "\n",
    "        # Création du masque après EOS\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand_as(is_eos)  # (G, C)\n",
    "        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()  # (G, C)\n",
    "\n",
    "        # Fusion avec le masque du prompt\n",
    "        attention_mask = torch.cat([prompt_mask.expand(self.num_generations, -1), completion_mask], dim=1)  # (G, P+C)\n",
    "\n",
    "        # Logits à conserver : seulement ceux des complétions\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "\n",
    "        #print(\"completion_mask.size(), attention_mask.size(), logits_to_keep\", completion_mask.size(), attention_mask.size(), logits_to_keep)\n",
    "\n",
    "\n",
    "        # Calcul les log-probabilités des tokens générés\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's\n",
    "            # computation here, and use per_token_logps.detach() instead.\n",
    "            if self.num_iterations > 1:\n",
    "                old_per_token_logps = self._get_per_token_logps(\n",
    "                    self.model, prompt_completion_ids, attention_mask, logits_to_keep\n",
    "                )\n",
    "            else:\n",
    "                old_per_token_logps = None\n",
    "\n",
    "            if self.ref_model is not None:\n",
    "                ref_per_token_logps = self._get_per_token_logps(self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep)\n",
    "            else: # Si il n'y a pas de ref model, on prend self.model\n",
    "                ref_per_token_logps = self._get_per_token_logps(self.model, prompt_completion_ids, attention_mask, logits_to_keep)\n",
    "        \n",
    "        # Décodage des réponses\n",
    "        completions = self.tokenizer.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calcul des rewards\n",
    "        #print(completions, type(completions))\n",
    "        output_rewards = self.reward_func(prompt=prompt, completions=completions)\n",
    "        rewards = torch.tensor(output_rewards, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Calcul des avantages\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"completion_ids\": completion_ids,\n",
    "            \"completion_mask\": completion_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps,  # Vérifier si bien calculé\n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": advantages,}\n",
    "\n",
    "\n",
    "    def _prepare_inputs(self, inputs):\n",
    "        return self._generate_and_score_completions(inputs) # Sans buffering\n",
    "    \n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Compute Loss according to GRPO paper, using advantages, per_token probabilities and KL divergence to reference model\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the per-token log probabilities for the model\n",
    "        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n",
    "        completion_ids, completion_mask = inputs[\"completion_ids\"], inputs[\"completion_mask\"]\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        \n",
    "        prompt_mask = prompt_mask.expand(completion_mask.size(0), -1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens\n",
    "\n",
    "        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)\n",
    "\n",
    "\n",
    "        # Compute the KL divergence between the model and the reference model\n",
    "        if self.beta != 0.0:\n",
    "            ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n",
    "            print( \"ref_per_token_logps.shape, per_token_logps.shape\", ref_per_token_logps.shape, per_token_logps.shape)\n",
    "            per_token_kl = (\n",
    "                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1\n",
    "            )\n",
    "\n",
    "        # Compute the loss\n",
    "        advantages = inputs[\"advantages\"]\n",
    "        print( \"advantages :\", advantages)\n",
    "        # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's computation (see\n",
    "        # _generate_and_score_completions) and use per_token_logps.detach() instead.\n",
    "        old_per_token_logps = inputs[\"old_per_token_logps\"] if self.num_iterations > 1 else per_token_logps.detach()\n",
    "        coef_1 = torch.exp(per_token_logps - old_per_token_logps)\n",
    "        coef_2 = torch.clamp(coef_1, 1 - self.eps, 1 + self.eps) #clipped ratio\n",
    "\n",
    "        per_token_loss1 = coef_1 * advantages.unsqueeze(1)\n",
    "        per_token_loss2 = coef_2 * advantages.unsqueeze(1)\n",
    "        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)\n",
    "        if self.beta != 0.0:\n",
    "            per_token_loss = per_token_loss + self.beta * per_token_kl # on ajoute la KL\n",
    "        loss = (per_token_loss * completion_mask).sum() / completion_mask.sum() # on calcule la loss que sur la complétion\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def train(self, num_epochs, dataset):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle GRPO sur num_epochs en utilisant les prompts du dataset.\n",
    "        - Met à jour `self.ref_model` à chaque epoch.\n",
    "        - Effectue des `train_step()` sur chaque prompt.\n",
    "        \"\"\"\n",
    "        self.ref_model = copy.deepcopy(self.model)  # Initialise le modèle de référence\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for i, prompt in enumerate(dataset):\n",
    "                print(f\"Prompt {i}\")\n",
    "                loss = self.train_step(prompt)\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\") \n",
    "            self.ref_model = copy.deepcopy(self.model)\n",
    "\n",
    "\n",
    "    def train_step(self, prompt):\n",
    "        \"\"\"\n",
    "            A training step with num_iterations optimization setp on given prompts mini-batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        # génère les réponses avec model = model_old, renvoie \"old_per_token_logps\" et \"ref_per_token_logps\"\n",
    "        inputs = self._prepare_inputs(prompt)\n",
    "\n",
    "        # Pour chaque GRPO iteration, on met à jour le modèle sur le même batch\n",
    "        for i in range(self.num_iterations): \n",
    "\n",
    "            # Calcul de la loss (utilise le modèle actuel qui se met à jour dans la boucle et on calcule la loss par rapport aux log_prob de old_model (dans inputs))\n",
    "            loss = self.compute_loss(self.model, inputs)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "config = GRPOConfiguration()\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(1, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens après padding :\n",
      "0: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'Bon', 'j', 'our', ',', 'Ġcomment', 'ĠÃ', '§', 'a', 'Ġva', 'Ġ?']\n",
      "1: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'Je', 'Ġv', 'ais', 'Ġb', 'ien', '.']\n",
      "2: ['Q', 'uel', 'Ġest', 'Ġton', 'Ġmod', 'Ã¨', 'le', 'Ġde', 'Ġlang', 'age', 'Ġpr', 'Ã©', 'f', 'Ã©', 'rÃ©', 'Ġ?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "        # On suppose que prompt_completion_ids a la forme (batch_size, num_generations, sequence_length)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.num_generations):\n",
    "                # Décoder chaque séquence de tokens (prompt + réponse)\n",
    "                decoded_sequence = self.tokenizer.convert_ids_to_tokens(prompt_completion_ids[i, j, :].tolist())\n",
    "                print(f\"Réponse générée (batch {i}, génération {j}) de len {len(decoded_sequence)}: {' '.join(decoded_sequence)}\")\n",
    "\n",
    "# Charger le tokenizer GPT-2 avec padding à gauche\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "\n",
    "# Ajouter un vrai token de padding\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Charger le modèle et adapter les embeddings (important pour qu'il reconnaisse [PAD])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Exemple de phrases\n",
    "sentences = [\n",
    "    \"Bonjour, comment ça va ?\",\n",
    "    \"Je vais bien.\",\n",
    "    \"Quel est ton modèle de langage préféré ?\"\n",
    "]\n",
    "\n",
    "# Tokenization avec padding\n",
    "encoded = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Affichage\n",
    "print(\"\\nTokens après padding :\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"{i}: {tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masques d'attention :  tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Tokens (avant padding après EOS) :\n",
      "0: ['<|endoftext|>', '<|endoftext|>', 'Bon', 'j', 'our', ',', 'Ġcomment', 'ĠÃ', '§', 'a', 'Ġva', 'Ġ?', '<|endoftext|>', 'bl', 'abl', 'aba']\n",
      "1: ['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', 'Je', 'Ġv', 'ais', 'Ġb', 'ien', '.']\n",
      "2: ['Q', 'uel', 'Ġest', 'Ġton', 'Ġmod', 'Ã¨', 'le', 'Ġde', 'Ġlang', 'age', 'Ġpr', 'Ã©', 'f', 'Ã©', 'rÃ©', 'Ġ?']\n",
      "\n",
      "Tokens (après padding après EOS) :\n",
      "0: ['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "1: ['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "2: ['Q', 'uel', 'Ġest', 'Ġton', 'Ġmod', 'Ã¨', 'le', 'Ġde', 'Ġlang', 'age', 'Ġpr', 'Ã©', 'f', 'Ã©', 'rÃ©', 'Ġ?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer GPT-2 avec padding à gauche\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 n'a pas de token PAD par défaut\n",
    "# Ajouter un vrai token de padding\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "# Charger le modèle et adapter les embeddings (important pour qu'il reconnaisse [PAD])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Exemple de phrases\n",
    "sentences = [\n",
    "    \"Bonjour, comment ça va ?\" + tokenizer.eos_token + \"blablaba\",  # Phrase normale\n",
    "    \"Je vais bien.\",  # Phrase plus courte\n",
    "    \"Quel est ton modèle de langage préféré ?\"  # Phrase plus longue\n",
    "]\n",
    "\n",
    "# Tokenization avec padding (longueur max auto)\n",
    "encoded = tokenizer(sentences, padding=True, padding_side=\"left\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérer les tokens\n",
    "completion_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"masques d'attention : \", attention_mask)\n",
    "\n",
    "# Affichage\n",
    "print(\"\\nTokens (avant padding après EOS) :\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"{i}: {tokenizer.convert_ids_to_tokens(completion_ids[i])}\")\n",
    "\n",
    "# Détection des tokens EOS\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "is_eos = completion_ids == eos_token_id  \n",
    "\n",
    "# Initialisation des indices EOS\n",
    "eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "\n",
    "# Trouver les séquences contenant au moins un <EOS>\n",
    "has_eos = is_eos.any(dim=1)\n",
    "\n",
    "# Trouver le premier <EOS>\n",
    "first_eos_idx = is_eos.int().argmax(dim=1, keepdim=False)  \n",
    "\n",
    "# Appliquer seulement aux séquences avec EOS\n",
    "if has_eos.any():\n",
    "    eos_idx[has_eos] = first_eos_idx[has_eos.nonzero(as_tuple=True)[0]]\n",
    "\n",
    "# Créer le masque\n",
    "sequence_indices = torch.arange(completion_ids.size(1), device=completion_ids.device).expand(completion_ids.size(0), -1)\n",
    "completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "# Appliquer le padding après EOS\n",
    "padded_sequences = completion_ids * completion_mask + (1 - completion_mask) * eos_token_id\n",
    "\n",
    "# Affichage final\n",
    "print(\"\\nTokens (après padding après EOS) :\")\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"{i}: {tokenizer.convert_ids_to_tokens(padded_sequences[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GRPOConfiguration()\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(1, dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m GRPOConfiguration()\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(config)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 185\u001b[0m, in \u001b[0;36mGRPOTrainer.train\u001b[1;34m(self, num_epochs, dataset, batch_size)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset), batch_size):\n\u001b[0;32m    184\u001b[0m     batch_prompts \u001b[38;5;241m=\u001b[39m dataset[i:i\u001b[38;5;241m+\u001b[39mbatch_size]  \u001b[38;5;66;03m# Sélectionne un batch\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Effectue un train_step\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    188\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n",
      "Cell \u001b[1;32mIn[27], line 202\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[1;34m(self, prompts)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# génère les réponses avec model = model_old, renvoie \"old_per_token_logps\" et \"ref_per_token_logps\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Pour chaque GRPO iteration, on met à jour le modèle sur le même batch\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations): \n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# Calcul de la loss (utilise le modèle actuel qui se met à jour dans la boucle et on calcule la loss par rapport aux log_prob de old_model (dans inputs))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 129\u001b[0m, in \u001b[0;36mGRPOTrainer._prepare_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 77\u001b[0m, in \u001b[0;36mGRPOTrainer._generate_and_score_completions\u001b[1;34m(self, prompts)\u001b[0m\n\u001b[0;32m     75\u001b[0m eos_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),), is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43meos_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mis_eos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(is_eos\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), is_eos\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     79\u001b[0m sequence_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mexpand(is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m completion_mask \u001b[38;5;241m=\u001b[39m (sequence_indices \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eos_idx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mint()\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "config = GRPOConfiguration()\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(1, dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102],\n",
      "        [ 101, 2986, 1010, 4283,  999,  102,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer([\"Hello, how are you?\", \"Fine, thanks!\"], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 learning_rate = 1e-5, \n",
    "                 temperature = 1.0, \n",
    "                 max_length = 100, \n",
    "                 device = \"cpu\"):\n",
    "        self.llm = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.AdamW(self.llm.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        input = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        output = None\n",
    "        loss = None\n",
    "\n",
    "        text = self.tokenizer.decode(output[0])\n",
    "        return loss, text\n",
    "\n",
    "    def calculate_reward(self, output):\n",
    "        \"\"\"\n",
    "            Calcule the reward of a single output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculate_GRPO_advantages(self, outputs):\n",
    "        \"\"\"\n",
    "            Calculate the advantages of each output\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def train_step(self, prompt):\n",
    "        \"\"\"\n",
    "            A training step on a single prompt\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(model, tokenizer)\n",
    "prompts = [\"The best way to learn coding is\", \"The future of AI is\"]\n",
    "\n",
    "for epoch in range(3): # Train for a few epochs\n",
    "    loss = 0\n",
    "    for prompt in prompts:\n",
    "        loss += trainer.train_step(prompts)        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss / len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.generate_text(prompts)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MVA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
