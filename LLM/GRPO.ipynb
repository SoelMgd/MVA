{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Groupe Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to fill in the `GRPOTrainer` class. You have two options (and you can do both):\n",
    "* the \"normal GRPO\" with clipped surrogate objective\n",
    "* or the \"vanilla GRPO\" with original objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 learning_rate=1e-5, \n",
    "                 temperature=1.0, \n",
    "                 max_length=100, \n",
    "                 device=\"cpu\",\n",
    "                 clip_epsilon=0.2,  # Seuil pour le clipping dans \"normal GRPO\"\n",
    "                 use_clipped=True):  # Active Clipped GRPO si True, sinon Vanilla GRPO\n",
    "        self.llm = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.AdamW(self.llm.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.use_clipped = use_clipped  # Toggle entre normal et vanilla GRPO\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        \"\"\"\n",
    "        Generate text from a prompt using the LLM.\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.llm.generate(\n",
    "                input_ids,\n",
    "                max_length=self.max_length,\n",
    "                temperature=self.temperature,\n",
    "                top_k=50,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "    def calculate_reward(self, text):\n",
    "        \"\"\"\n",
    "        Calculate reward using inverse perplexity.\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.llm(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss  # NLL Loss (Negative Log-Likelihood)\n",
    "        \n",
    "        reward = -loss.item()  # Reward = -Perplexity (minimiser la perplexité)\n",
    "        return reward\n",
    "\n",
    "    def calculate_GRPO_advantages(self, rewards):\n",
    "        \"\"\"\n",
    "        Compute the normalized advantage for GRPO.\n",
    "        \"\"\"\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        mean_reward = rewards.mean()\n",
    "        std_reward = rewards.std() + 1e-8  # Avoid division by zero\n",
    "        advantages = (rewards - mean_reward) / std_reward\n",
    "        return advantages\n",
    "\n",
    "    def train_step(self, prompt):\n",
    "        \"\"\"\n",
    "        Perform one training step on a single prompt.\n",
    "        \"\"\"\n",
    "        # Générer des sorties\n",
    "        generated_text = self.generate(prompt)\n",
    "\n",
    "        # Calculer la récompense\n",
    "        reward = self.calculate_reward(generated_text)\n",
    "\n",
    "        # Calculer l'avantage GRPO\n",
    "        advantages = self.calculate_GRPO_advantages([reward])\n",
    "\n",
    "        # Encoder l'entrée et la sortie en tokens\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "        output_ids = self.tokenizer(generated_text, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        # Calculer la probabilité de la politique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired from HuggingFace\n",
    "import re\n",
    "import copy\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 learning_rate = 1e-5, \n",
    "                 temperature = 1.0, \n",
    "                 max_length = 100, \n",
    "                 device = \"cpu\"):\n",
    "        self.model = model.to(device) # model to optimize\n",
    "        self.ref_model = None # reference model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.max_prompt_length = max_length\n",
    "        self.eps = 1e-10\n",
    "        self.num_generations = 5 # num of generation per prompts\n",
    "        self.num_iterations = 1\n",
    "        self.beta = 0.1\n",
    "\n",
    "\n",
    "    def reward_func(self, prompts, completions, **kwargs):\n",
    "        \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "\n",
    "        pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "        completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "        matches = [re.match(pattern, content) for content in completion_contents]\n",
    "        return [1.0 if match else 0.0 for match in matches]\n",
    "    \n",
    "\n",
    "    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):\n",
    "        \"\"\"\n",
    "        Get the per-token log propabilities for the completions for the model and the reference model\n",
    "        \"\"\"\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        logits = logits[:,:-1,:] #(B, L-1, V) exclude the last logit corresponding to next token prediction\n",
    "\n",
    "        input_ids = input_ids[:, -logits_to_keep:]\n",
    "        logits = logits[:, -logits_to_keep:]\n",
    "        return torch.gather(logits.log_softmax(-1), dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) #log soft max to get probabilities\n",
    "    \n",
    "\n",
    "    def _generate_and_score_completions(self, prompts):\n",
    "        \"\"\"\n",
    "            Génère une réponse avec self.model à un batch de prompts et effectue le scoring\n",
    "        \"\"\"\n",
    "        # Tokenize les prompts et récupère le mask d'attention (pour le padding)\n",
    "        prompt_inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side='left').to(self.device) #tokenization\n",
    "        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "\n",
    "        # Troncatène les prompts si ils sont trop longs\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n",
    "\n",
    "        # Génération des réponses \n",
    "        with torch.inference_mode():\n",
    "            prompt_completion_ids = self.model.generate(prompt_ids, attention_mask=prompt_mask, num_return_sequences=self.num_generations)\n",
    "        \n",
    "        batch_size = prompt_ids.size(0)\n",
    "        prompt_completion_ids = prompt_completion_ids.view(batch_size, self.num_generations, -1)  # resizing (B, G, P+C)\n",
    "\n",
    "        # Extrait les ids des prompts et des réponses\n",
    "        prompt_length = prompt_ids.size(1)\n",
    "        prompt_ids = prompt_completion_ids[:, :, :prompt_length] #(B, G, P)\n",
    "        completion_ids = prompt_completion_ids[:, :, prompt_length:] #(B, G, C)\n",
    "\n",
    "        # Padding après le premier token EOS\n",
    "        is_eos = completion_ids == self.tokenizer.eos_token_id\n",
    "        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=self.device)\n",
    "        eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=self.device).expand(is_eos.size(0), -1)\n",
    "        completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "        \n",
    "        # Concatenation du masque du prompt et de la complétion pour le calcul des logits\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)  # (B, P+C)\n",
    "        logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens\n",
    "\n",
    "        # Calcul les log-probabilités des tokens générés\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's\n",
    "            # computation here, and use per_token_logps.detach() instead.\n",
    "            if self.num_iterations > 1:\n",
    "                old_per_token_logps = self._get_per_token_logps(\n",
    "                    self.model, prompt_completion_ids, attention_mask, logits_to_keep\n",
    "                )\n",
    "            else:\n",
    "                old_per_token_logps = None\n",
    "\n",
    "            if self.ref_model is not None:\n",
    "                ref_per_token_logps = self._get_per_token_logps(self.ref_model, prompt_completion_ids, attention_mask, logits_to_keep)\n",
    "            else: # Si il n'y a pas de ref model, on prend self.model\n",
    "                ref_per_token_logps = self._get_per_token_logps(self.model, prompt_completion_ids, attention_mask, logits_to_keep)\n",
    "        \n",
    "        # Décodage des réponses\n",
    "        completions = self.tokenizer.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calcul des rewards\n",
    "        output_rewards = self.reward_func(prompts=prompts, completions=completions)\n",
    "        rewards = torch.tensor(output_rewards, dtype=torch.float32, device=self.device)\n",
    "        assert len(output_rewards) == len(completions) # Mismatch entre le nombre de complétions et de récompenses\n",
    "\n",
    "        # Calcul des avantages\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"completion_ids\": completion_ids,\n",
    "            \"completion_mask\": completion_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps,  # Vérifier si bien calculé\n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": advantages,}\n",
    "\n",
    "\n",
    "    def _prepare_inputs(self, inputs):\n",
    "        return self._generate_and_score_completions(inputs) # Sans buffering\n",
    "    \n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Compute Loss according to GRPO paper, using advantages, per_token probabilities and KL divergence to reference model\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the per-token log probabilities for the model\n",
    "        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n",
    "        completion_ids, completion_mask = inputs[\"completion_ids\"], inputs[\"completion_mask\"]\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens\n",
    "\n",
    "        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)\n",
    "\n",
    "        # Compute the KL divergence between the model and the reference model\n",
    "        if self.beta != 0.0:\n",
    "            ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n",
    "            per_token_kl = (\n",
    "                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1\n",
    "            )\n",
    "\n",
    "        # Compute the loss\n",
    "        advantages = inputs[\"advantages\"]\n",
    "        # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's computation (see\n",
    "        # _generate_and_score_completions) and use per_token_logps.detach() instead.\n",
    "        old_per_token_logps = inputs[\"old_per_token_logps\"] if self.num_iterations > 1 else per_token_logps.detach()\n",
    "        coef_1 = torch.exp(per_token_logps - old_per_token_logps)\n",
    "        coef_2 = torch.clamp(coef_1, 1 - self.epsilon, 1 + self.epsilon) #clipped ratio\n",
    "\n",
    "        per_token_loss1 = coef_1 * advantages.unsqueeze(1)\n",
    "        per_token_loss2 = coef_2 * advantages.unsqueeze(1)\n",
    "        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)\n",
    "        if self.beta != 0.0:\n",
    "            per_token_loss = per_token_loss + self.beta * per_token_kl # on ajoute la KL\n",
    "        loss = (per_token_loss * completion_mask).sum() / completion_mask.sum() # on calcule la loss que sur la complétion\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def train(self, num_epochs, dataset, batch_size=8):\n",
    "        \"\"\"\n",
    "        Entraîne le modèle GRPO sur num_epochs en utilisant des batchs du dataset.\n",
    "        - Met à jour `self.ref_model` à chaque epoch.\n",
    "        - Effectue des `train_step()` sur chaque batch.\n",
    "        \"\"\"\n",
    "        self.ref_model = copy.deepcopy(self.model)  # Initialise le modèle de référence\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = len(dataset) // batch_size\n",
    "\n",
    "            for i in range(0, len(dataset), batch_size):\n",
    "                batch_prompts = dataset[i:i+batch_size]  # Sélectionne un batch\n",
    "                loss = self.train_step(batch_prompts)  # Effectue un train_step\n",
    "                total_loss += loss\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Met à jour le modèle de référence après chaque epoch\n",
    "            self.ref_model = copy.deepcopy(self.model)\n",
    "\n",
    "    def train_step(self, prompts):\n",
    "        \"\"\"\n",
    "            A training step with num_iterations optimization setp on given prompts mini-batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        # génère les réponses avec model = model_old, renvoie \"old_per_token_logps\" et \"ref_per_token_logps\"\n",
    "        inputs = self._prepare_inputs(prompts)\n",
    "\n",
    "        # Pour chaque GRPO iteration, on met à jour le modèle sur le même batch\n",
    "        for i in range(self.num_iterations): \n",
    "\n",
    "            # Calcul de la loss (utilise le modèle actuel qui se met à jour dans la boucle et on calcule la loss par rapport aux log_prob de old_model (dans inputs))\n",
    "            loss = self.compute_loss(self.model, inputs)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102],\n",
      "        [ 101, 2986, 1010, 4283,  999,  102,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer([\"Hello, how are you?\", \"Fine, thanks!\"], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 learning_rate = 1e-5, \n",
    "                 temperature = 1.0, \n",
    "                 max_length = 100, \n",
    "                 device = \"cpu\"):\n",
    "        self.llm = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.AdamW(self.llm.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        input = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        output = None\n",
    "        loss = None\n",
    "\n",
    "        text = self.tokenizer.decode(output[0])\n",
    "        return loss, text\n",
    "\n",
    "    def calculate_reward(self, output):\n",
    "        \"\"\"\n",
    "            Calcule the reward of a single output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculate_GRPO_advantages(self, outputs):\n",
    "        \"\"\"\n",
    "            Calculate the advantages of each output\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def train_step(self, prompt):\n",
    "        \"\"\"\n",
    "            A training step on a single prompt\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(model, tokenizer)\n",
    "prompts = [\"The best way to learn coding is\", \"The future of AI is\"]\n",
    "\n",
    "for epoch in range(3): # Train for a few epochs\n",
    "    loss = 0\n",
    "    for prompt in prompts:\n",
    "        loss += trainer.train_step(prompts)        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss / len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.generate_text(prompts)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MVA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
