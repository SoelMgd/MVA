{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Groupe Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Hugging Face libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to fill in the `GRPOTrainer` class. You have two options (and you can do both):\n",
    "* the \"normal GRPO\" with clipped surrogate objective\n",
    "* or the \"vanilla GRPO\" with original objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"A train takes 3 hours to travel from A to B at an average speed of 60 km/h. How long would the trip take if the train traveled at 80 km/h?\",\n",
    "    \"A snail climbs a 10-meter wall. It climbs 3 meters during the day and slips 2 meters at night. How many days will it take to reach the top?\",\n",
    "    'If a liar says, \"I always lie,\" is he telling the truth?',\n",
    "    'Can we say that \"this sentence is false\"? Explain why.',\n",
    "    \"Paul is twice the age Pierre was when Paul was the age Pierre is today. If Pierre is 20 years old, how old is Paul?\",\n",
    "    \"A father and his son together are 36 years old. The father is exactly three times the son's age. How old is the son?\",\n",
    "    \"All the cats I have met so far were black. Can I conclude that all cats are black? Why?\",\n",
    "    \"If all humans are mortal and Socrates is human, what can we conclude?\",\n",
    "    \"If a shirt costs twice as much as a pair of pants and the pants cost 30€, how much does the shirt cost?\",\n",
    "    'Jean says: \"All my friends are football players.\" Pierre is Jean’s friend. Can we conclude that Pierre is a football player?',\n",
    "    \"You are in a train and must choose between switching the direction of the train to avoid five people tied to one track, but in doing so, you will kill one person on the other track. What do you do and why?\",\n",
    "    \"A doctor has five patients in need of organ transplants, and a perfectly healthy patient comes in for a routine check-up. Should the doctor sacrifice this patient to save the five others?\",\n",
    "    \"What would happen if gravity on Earth were twice as strong?\",\n",
    "    \"If humans could read minds, how would that change society?\"\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "for question in questions:\n",
    "    dataset.append(f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> <\\answer> tags, respectively, i.e., <think> reasoning process here <\\think> <answer> answer here <\\answer>. User: {question}. Assistant:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOConfiguration:\n",
    "    def __init__(self,\n",
    "                 model_name=\"gpt2\",\n",
    "                 learning_rate=1e-5,\n",
    "                 temperature=0.9,\n",
    "                 max_prompt_length=200,\n",
    "                 max_output_length=200,\n",
    "                 device=\"cpu\",\n",
    "                 num_generations=3,\n",
    "                 num_iterations=2,\n",
    "                 beta=0.1,\n",
    "                 epsilon=1e-5,\n",
    "                 reward_func=None,\n",
    "                 print_outputs=False,\n",
    "                 print_advantages=False):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.temperature = temperature\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.device = device\n",
    "        self.num_generations = num_generations # number of generations per prompts\n",
    "        self.num_iterations = num_iterations    # number of iterative optimization steps per prompts\n",
    "        self.beta = beta # KL coefficient\n",
    "        self.epsilon = epsilon\n",
    "        self.reward_func = reward_func # reward function\n",
    "        self.print_outputs = print_outputs\n",
    "        self.print_advantages = print_advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on prompt 0\n",
      "GRPO Iteration 1\n",
      "advantages : tensor([0., 0., 0.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 249\u001b[0m\n\u001b[0;32m    247\u001b[0m config \u001b[38;5;241m=\u001b[39m GRPOConfiguration(num_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, print_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, print_advantages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    248\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(config)\n\u001b[1;32m--> 249\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 212\u001b[0m, in \u001b[0;36mGRPOTrainer.train\u001b[1;34m(self, num_epochs, dataset)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    215\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "Cell \u001b[1;32mIn[106], line 241\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\soelm\\Documents\\04_Code\\MVA\\MVA_venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Inspired from HuggingFace\n",
    "import re\n",
    "import copy\n",
    "\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, config: GRPOConfiguration):\n",
    "        \"\"\"\n",
    "        Initialize a GRPO Trainer\n",
    "        Args:\n",
    "            config: GRPO Configuration\n",
    "        \"\"\"\n",
    "        self.device = config.device\n",
    "        self.model_name = config.model_name\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(config.model_name).to(self.device) # model to optimize\n",
    "        self.ref_model = None # reference model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name, padding=True, padding_side=\"left\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token  #gpt2 n'a pas de padding token\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate)\n",
    "        self.temperature = config.temperature\n",
    "        self.max_prompt_length = config.max_prompt_length\n",
    "        self.max_output_length = config.max_output_length\n",
    "        self.eps = config.epsilon\n",
    "        self.num_generations = config.num_generations # num of generation per prompts\n",
    "        self.num_iterations = config.num_generations\n",
    "        self.beta = config.beta\n",
    "        self.reward_func = config.reward_func if config.reward_func else self._default_reward_func\n",
    "        self.print_outputs = config.print_outputs\n",
    "        self.print_adantages = config.print_advantages\n",
    "\n",
    "    \n",
    "    def _default_reward_func(self, prompt, outputs, **kwargs):\n",
    "        \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "        \n",
    "        pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "\n",
    "        matches = [re.match(pattern, content) for content in outputs]\n",
    "        return [1.0 if match else 0.0 for match in matches]\n",
    "        \n",
    "\n",
    "    def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):\n",
    "        \"\"\"\n",
    "        Get the per-token log propabilities for the outputs.\n",
    "        Args:\n",
    "            model: model to compute per-token log probabilities\n",
    "            input_ids: sequence of tokens #(G, L)\n",
    "            attention_mak: mapping of tokens to keep to compute attention (excluding padding)\n",
    "            logits_to_keep: number of logits to keep to compute the loss\n",
    "        \"\"\"\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        logits = logits[:,:-1,:] # (G, L-1, V) exclude the last logit corresponding to next token prediction\n",
    "\n",
    "        input_ids = input_ids[:, -logits_to_keep:]\n",
    "        logits = logits[:, -logits_to_keep:]\n",
    "        log_probs = logits.log_softmax(-1) # softmax to get probabilities\n",
    "        return torch.gather(log_probs, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1) # keeping only input_ids\n",
    "    \n",
    "\n",
    "    def _generate_and_score_outputs(self, prompt: str):\n",
    "        \"\"\"\n",
    "        Generate answers with old_model to the prompt, compute associated reward and advantages and those of ref_model.\n",
    "        Args:\n",
    "            prompt: the prompt\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        # Tokenization\n",
    "        prompt_inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=False).to(device)\n",
    "        prompt_ids, prompt_mask = prompt_inputs[\"input_ids\"], prompt_inputs[\"attention_mask\"]\n",
    "\n",
    "        # Setting to prompt lenght\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length :]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length :]\n",
    "\n",
    "        # Generate answers with old_model\n",
    "        with torch.inference_mode():\n",
    "            prompt_output_ids = self.model.generate(\n",
    "                prompt_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                num_return_sequences=self.num_generations, # number of generations\n",
    "                do_sample=True,\n",
    "                max_length=prompt_ids.size(1) + self.max_output_length,  # (P+O)\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        if self.print_outputs:\n",
    "            for i in range(prompt_output_ids.size(0)):\n",
    "                decoded_sequence = self.tokenizer.convert_ids_to_tokens(prompt_output_ids[i, :].tolist())\n",
    "                print(f\"Réponse générée (génération {i}) de len {len(decoded_sequence)}: {' '.join(decoded_sequence)}\")\n",
    "\n",
    "        # Splitting prompt and outputs tokens\n",
    "        prompt_length = prompt_ids.size(1)\n",
    "        prompt_ids = prompt_output_ids[:, :prompt_length]  # (G, P)\n",
    "        output_ids = prompt_output_ids[:, prompt_length:]  # (G, O)\n",
    "\n",
    "\n",
    "        # Padding with EOS after first EOS in outputs\n",
    "        is_eos = output_ids == self.tokenizer.eos_token_id  # (G, O)\n",
    "        eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=device)  # (G,)\n",
    "        has_eos = is_eos.any(dim=1)  # (G,)\n",
    "        first_eos_idx = is_eos.int().argmax(dim=1)  # Index of first EOS token (G,)\n",
    "        eos_idx[has_eos] = first_eos_idx[has_eos]  # (G,)\n",
    "        sequence_indices = torch.arange(is_eos.size(1), device=device).expand_as(is_eos)  # (G, O)\n",
    "        output_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()  # (G, O) Mask for padding\n",
    "\n",
    "        # Merging prompt and output  mask\n",
    "        attention_mask = torch.cat([prompt_mask.expand(self.num_generations, -1), output_mask], dim=1)  # (G, P+O)\n",
    "\n",
    "        logits_to_keep = output_ids.size(1) # Logits to keep for loss computation\n",
    "\n",
    "        # Compute log-probabilities for the prompt with model_ref and model_old\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # When using num_iterations == 1, old_per_token_logps == per_token_logps\n",
    "            if self.num_iterations > 1:\n",
    "                old_per_token_logps = self._get_per_token_logps(self.model, prompt_output_ids, attention_mask, logits_to_keep)\n",
    "            else:\n",
    "                old_per_token_logps = None\n",
    "\n",
    "            ref_per_token_logps = self._get_per_token_logps(self.ref_model, prompt_output_ids, attention_mask, logits_to_keep)\n",
    "        \n",
    "        # Decoding outputs\n",
    "        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Compute rewards\n",
    "        output_rewards = self.reward_func(prompt=prompt, outputs=outputs)\n",
    "        rewards = torch.tensor(output_rewards, dtype=torch.float32, device=self.device) #(G,)\n",
    "\n",
    "        # Compute advantages\n",
    "        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)\n",
    "\n",
    "        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4) # (G,)\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"output_ids\": output_ids,\n",
    "            \"output_mask\": output_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps, \n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": advantages,}\n",
    "\n",
    "\n",
    "    def _prepare_inputs(self, inputs):\n",
    "        return self._generate_and_score_outputs(inputs) # Without buffering\n",
    "    \n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Compute Loss according to GRPO paper, using advantages, per_token probabilities and KL divergence approximator to reference model\n",
    "        Args:\n",
    "            model: enlever\n",
    "            inputs: {\"prompt_ids\", \"prompt_mask\", \"output_ids\", \"output_mask\", \"old_per_token_logps\", \"ref_per_token_logps\", \"advantages\"}\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute the per-token log probabilities for the current model\n",
    "        prompt_ids, prompt_mask = inputs[\"prompt_ids\"], inputs[\"prompt_mask\"]\n",
    "        output_ids, output_mask = inputs[\"output_ids\"], inputs[\"output_mask\"]\n",
    "        input_ids = torch.cat([prompt_ids, output_ids], dim=1)\n",
    "        \n",
    "        prompt_mask = prompt_mask.expand(output_mask.size(0), -1)\n",
    "        attention_mask = torch.cat([prompt_mask, output_mask], dim=1)\n",
    "        logits_to_keep = output_ids.size(1)  # we only need to compute the logits for the completion tokens\n",
    "\n",
    "        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep) # policy of current model\n",
    "\n",
    "        # Compute the KL divergence between the current model and the reference model\n",
    "        if self.beta != 0.0:\n",
    "            ref_per_token_logps = inputs[\"ref_per_token_logps\"]\n",
    "            per_token_kl = (torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1 ) # (G, O) KL differentiable approximator\n",
    "\n",
    "        ### Compute the loss\n",
    "        \n",
    "        advantages = inputs[\"advantages\"]\n",
    "        if self.print_adantages:\n",
    "            print( \"advantages :\", advantages)\n",
    "\n",
    "        # When using num_iterations == 1, old_per_token_logps == per_token_logps\n",
    "        old_per_token_logps = inputs[\"old_per_token_logps\"] if self.num_iterations > 1 else per_token_logps.detach()\n",
    "        policy_ratio = torch.exp(per_token_logps - old_per_token_logps) # (G, O)\n",
    "        policy_ratio_clipped = torch.clamp(policy_ratio, 1 - self.eps, 1 + self.eps) #clipped ratio\n",
    "\n",
    "        per_token_loss1 = policy_ratio * advantages.unsqueeze(1)\n",
    "        per_token_loss2 = policy_ratio_clipped * advantages.unsqueeze(1)\n",
    "        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)\n",
    "\n",
    "        if self.beta != 0.0:\n",
    "            per_token_loss = per_token_loss + self.beta * per_token_kl # adding KL\n",
    "\n",
    "        loss = (per_token_loss * output_mask).sum() / output_mask.sum() # excluding padding in loss computation\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def train(self, num_epochs: int, dataset: list):\n",
    "        \"\"\"\n",
    "        Train the model on the dataset for specific number of epochs.\n",
    "        Args:\n",
    "            num_epochs: number of epochs \n",
    "            dataset: dataset\n",
    "        \"\"\"\n",
    "        self.ref_model = copy.deepcopy(self.model)  # Initialize reference model\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for i, prompt in enumerate(dataset):\n",
    "                print(f\"Training on prompt {i}\")\n",
    "                loss = self.train_step(prompt)\n",
    "                total_loss += loss\n",
    "            \n",
    "            avg_loss = total_loss / len(dataset)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\") \n",
    "            self.ref_model = copy.deepcopy(self.model)\n",
    "\n",
    "\n",
    "    def train_step(self, prompt: str):\n",
    "        \"\"\"\n",
    "            Do a traing step on a specific prompt with multiple iteration steps.\n",
    "            Args:\n",
    "                prompt: prompt to optimize the model on it\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "        # Generate outputs with old_model, compute log-probabilities for old_model and ref_model\n",
    "        inputs = self._generate_and_score_outputs(prompt)\n",
    "\n",
    "        # For self.num_iterations, iterativelt update the model on the same prompt\n",
    "        for i in range(1, self.num_iterations): \n",
    "            \n",
    "            print(f\"GRPO Iteration {i}\")\n",
    "\n",
    "            loss = self.compute_loss(self.model, inputs)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "config = GRPOConfiguration(num_generations=3, num_iterations=2, print_outputs=False, print_advantages=True)\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(1, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens après padding :\n",
      "0: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'Bon', 'j', 'our', ',', 'Ġcomment', 'ĠÃ', '§', 'a', 'Ġva', 'Ġ?']\n",
      "1: ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'Je', 'Ġv', 'ais', 'Ġb', 'ien', '.']\n",
      "2: ['Q', 'uel', 'Ġest', 'Ġton', 'Ġmod', 'Ã¨', 'le', 'Ġde', 'Ġlang', 'age', 'Ġpr', 'Ã©', 'f', 'Ã©', 'rÃ©', 'Ġ?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "        # On suppose que prompt_completion_ids a la forme (batch_size, num_generations, sequence_length)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.num_generations):\n",
    "                # Décoder chaque séquence de tokens (prompt + réponse)\n",
    "                decoded_sequence = self.tokenizer.convert_ids_to_tokens(prompt_completion_ids[i, j, :].tolist())\n",
    "                print(f\"Réponse générée (batch {i}, génération {j}) de len {len(decoded_sequence)}: {' '.join(decoded_sequence)}\")\n",
    "\n",
    "# Charger le tokenizer GPT-2 avec padding à gauche\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "\n",
    "# Ajouter un vrai token de padding\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Charger le modèle et adapter les embeddings (important pour qu'il reconnaisse [PAD])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Exemple de phrases\n",
    "sentences = [\n",
    "    \"Bonjour, comment ça va ?\",\n",
    "    \"Je vais bien.\",\n",
    "    \"Quel est ton modèle de langage préféré ?\"\n",
    "]\n",
    "\n",
    "# Tokenization avec padding\n",
    "encoded = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Affichage\n",
    "print(\"\\nTokens après padding :\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"{i}: {tokenizer.convert_ids_to_tokens(encoded['input_ids'][i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masques d'attention :  tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Tokens (avant padding après EOS) :\n",
      "0: ['<|endoftext|>', '<|endoftext|>', 'Bon', 'j', 'our', ',', 'Ġcomment', 'ĠÃ', '§', 'a', 'Ġva', 'Ġ?', '<|endoftext|>', 'bl', 'abl', 'aba']\n",
      "1: ['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', 'Je', 'Ġv', 'ais', 'Ġb', 'ien', '.']\n",
      "2: ['Q', 'uel', 'Ġest', 'Ġton', 'Ġmod', 'Ã¨', 'le', 'Ġde', 'Ġlang', 'age', 'Ġpr', 'Ã©', 'f', 'Ã©', 'rÃ©', 'Ġ?']\n",
      "\n",
      "Tokens (après padding après EOS) :\n",
      "0: ['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "1: ['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "2: ['Q', 'uel', 'Ġest', 'Ġton', 'Ġmod', 'Ã¨', 'le', 'Ġde', 'Ġlang', 'age', 'Ġpr', 'Ã©', 'f', 'Ã©', 'rÃ©', 'Ġ?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer GPT-2 avec padding à gauche\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 n'a pas de token PAD par défaut\n",
    "# Ajouter un vrai token de padding\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "# Charger le modèle et adapter les embeddings (important pour qu'il reconnaisse [PAD])\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Exemple de phrases\n",
    "sentences = [\n",
    "    \"Bonjour, comment ça va ?\" + tokenizer.eos_token + \"blablaba\",  # Phrase normale\n",
    "    \"Je vais bien.\",  # Phrase plus courte\n",
    "    \"Quel est ton modèle de langage préféré ?\"  # Phrase plus longue\n",
    "]\n",
    "\n",
    "# Tokenization avec padding (longueur max auto)\n",
    "encoded = tokenizer(sentences, padding=True, padding_side=\"left\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérer les tokens\n",
    "completion_ids = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n",
    "print(\"masques d'attention : \", attention_mask)\n",
    "\n",
    "# Affichage\n",
    "print(\"\\nTokens (avant padding après EOS) :\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"{i}: {tokenizer.convert_ids_to_tokens(completion_ids[i])}\")\n",
    "\n",
    "# Détection des tokens EOS\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "is_eos = completion_ids == eos_token_id  \n",
    "\n",
    "# Initialisation des indices EOS\n",
    "eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "\n",
    "# Trouver les séquences contenant au moins un <EOS>\n",
    "has_eos = is_eos.any(dim=1)\n",
    "\n",
    "# Trouver le premier <EOS>\n",
    "first_eos_idx = is_eos.int().argmax(dim=1, keepdim=False)  \n",
    "\n",
    "# Appliquer seulement aux séquences avec EOS\n",
    "if has_eos.any():\n",
    "    eos_idx[has_eos] = first_eos_idx[has_eos.nonzero(as_tuple=True)[0]]\n",
    "\n",
    "# Créer le masque\n",
    "sequence_indices = torch.arange(completion_ids.size(1), device=completion_ids.device).expand(completion_ids.size(0), -1)\n",
    "completion_mask = (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "# Appliquer le padding après EOS\n",
    "padded_sequences = completion_ids * completion_mask + (1 - completion_mask) * eos_token_id\n",
    "\n",
    "# Affichage final\n",
    "print(\"\\nTokens (après padding après EOS) :\")\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"{i}: {tokenizer.convert_ids_to_tokens(padded_sequences[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GRPOConfiguration()\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(1, dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m GRPOConfiguration()\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(config)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 185\u001b[0m, in \u001b[0;36mGRPOTrainer.train\u001b[1;34m(self, num_epochs, dataset, batch_size)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset), batch_size):\n\u001b[0;32m    184\u001b[0m     batch_prompts \u001b[38;5;241m=\u001b[39m dataset[i:i\u001b[38;5;241m+\u001b[39mbatch_size]  \u001b[38;5;66;03m# Sélectionne un batch\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Effectue un train_step\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    188\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n",
      "Cell \u001b[1;32mIn[27], line 202\u001b[0m, in \u001b[0;36mGRPOTrainer.train_step\u001b[1;34m(self, prompts)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# génère les réponses avec model = model_old, renvoie \"old_per_token_logps\" et \"ref_per_token_logps\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Pour chaque GRPO iteration, on met à jour le modèle sur le même batch\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations): \n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# Calcul de la loss (utilise le modèle actuel qui se met à jour dans la boucle et on calcule la loss par rapport aux log_prob de old_model (dans inputs))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 129\u001b[0m, in \u001b[0;36mGRPOTrainer._prepare_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 77\u001b[0m, in \u001b[0;36mGRPOTrainer._generate_and_score_completions\u001b[1;34m(self, prompts)\u001b[0m\n\u001b[0;32m     75\u001b[0m eos_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),), is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#eos_idx[is_eos.any(dim=1)] = is_eos.int().argmax(dim=1)[is_eos.any(dim=1)]\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43meos_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mis_eos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(is_eos\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), is_eos\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     79\u001b[0m sequence_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mexpand(is_eos\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m completion_mask \u001b[38;5;241m=\u001b[39m (sequence_indices \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eos_idx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mint()\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "config = GRPOConfiguration()\n",
    "trainer = GRPOTrainer(config)\n",
    "trainer.train(1, dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102],\n",
      "        [ 101, 2986, 1010, 4283,  999,  102,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer([\"Hello, how are you?\", \"Fine, thanks!\"], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(inputs)\n",
    "print(tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 learning_rate = 1e-5, \n",
    "                 temperature = 1.0, \n",
    "                 max_length = 100, \n",
    "                 device = \"cpu\"):\n",
    "        self.llm = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = torch.optim.AdamW(self.llm.parameters(), lr=learning_rate)\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        input = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        output = None\n",
    "        loss = None\n",
    "\n",
    "        text = self.tokenizer.decode(output[0])\n",
    "        return loss, text\n",
    "\n",
    "    def calculate_reward(self, output):\n",
    "        \"\"\"\n",
    "            Calcule the reward of a single output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calculate_GRPO_advantages(self, outputs):\n",
    "        \"\"\"\n",
    "            Calculate the advantages of each output\n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def train_step(self, prompt):\n",
    "        \"\"\"\n",
    "            A training step on a single prompt\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, padding_side=\"left\")\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(model, tokenizer)\n",
    "prompts = [\"The best way to learn coding is\", \"The future of AI is\"]\n",
    "\n",
    "for epoch in range(3): # Train for a few epochs\n",
    "    loss = 0\n",
    "    for prompt in prompts:\n",
    "        loss += trainer.train_step(prompts)        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss / len(prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.generate_text(prompts)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MVA_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
